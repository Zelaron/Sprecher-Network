\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,mathrsfs}
\usepackage{graphicx}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}

\title{Sprecher Networks: A generalized neural architecture \\for multivariate function approximation}
\author{Christian Hägg \and Boris Shapiro}
\date{}

\begin{document}

\maketitle

\begin{abstract}
We present \emph{Sprecher Networks}, a family of trainable neural architectures inspired by the classical Kolmogorov--Arnold--Sprecher construction for approximating multivariate continuous functions. Unlike typical deep networks, these models rely on \emph{monotonic and general splines} to implement learnable transformations with explicit shifts and summations. Our approach reproduces Sprecher's original one-layer ``sum of shifted splines'' formula and extends it to deeper, multi-layer compositions. We discuss theoretical motivations, implementation details, and potential advantages in interpretability and universality.
\end{abstract}

\section{Introduction and historical background}
Approximation of continuous functions by sums of univariate functions has been a recurring theme in mathematical analysis and neural networks. Kolmogorov's Superposition Theorem (1957), refined by Arnold, established that any multivariate continuous function $f : [0,1]^d \to \mathbb{R}$ can be represented as a finite sum of strictly increasing univariate functions applied to affine transformations of $x_1,\dots,x_d$.

\vspace{1mm}
\textbf{David Sprecher's 1965 construction.} $$f(\mathbf{x})=\sum_{q=0}^{2n}\Phi\Bigl(\,\sum_{p=1}^{n}\lambda_p\,\phi\bigl(x_p+\eta\,q\bigr)+q\Bigr)$$ In his 1965 landmark paper, David Sprecher \cite{sprecher1965} proposed a simplified version of the Kolmogorov--Arnold representation that specifically replaces families of $\phi_{q,p}$ by a single monotonic function $\phi$ with suitable argument shifts. Concretely, Sprecher showed that this single-layer construction suffices to approximate any $d$-variate continuous function when $n \ge d$ and the parameters $(\eta,\lambda_1,\dots,\lambda_n)$ and splines $(\phi,\Phi)$ are chosen carefully. His result preserves the key insight of \emph{shifting the input coordinates} by multiples of $\eta$ and summing the partial evaluations under a final univariate map $\Phi$.

\section{Motivation and overview of Sprecher Networks}
Modern deep learning practice often focuses on conventional feedforward architectures. However, Sprecher's approach offers a highly interpretable alternative:
\begin{itemize}
    \item Each layer in our network is organized around a fixed monotonic spline $\phi(\cdot)$ and a more general spline $\Phi(\cdot)$.
    \item It incorporates trainable shifts via a scalar $\eta$, and trainable weights $\lambda$ that mix each input coordinate before passing them into $\phi$.
    \item Finally, the network sums over shifted partial evaluations inside an outer spline, closely mimicking Sprecher's formula.
\end{itemize}
Our \emph{Sprecher Networks} generalize the classical shift-and-sum construction to a multi-layer architecture by stacking hidden layers. In our design, the mapping from one hidden layer (i.e. the collection of nodes in that layer) to the next is realized by a \emph{Sprecher block}: a functional operator that applies a shift-and-sum transformation using a shared pair of spline functions. Unlike conventional networks, the nodes within each hidden layer are not individually parameterized; rather, they are produced uniformly by the common transformation defined by the Sprecher block. That is, each Sprecher block applies
$$(x_i)_{i=1}^{d_\mathrm{in}} \;\mapsto\; \Bigl[\Phi\Bigl(\sum_{i}\lambda_{i,q}\,\phi(x_i+\eta\,q)+q\Bigr)\Bigr]_{q=0}^{d_{\mathrm{out}}-1},$$
and in the scalar-output case the outputs of the final Sprecher block are aggregated (via summation over $q$) to produce a single-dimensional output. In the original 1965 paper, Sprecher used only one such layer (with $2n+1$ summands). Our approach allows the network to follow the progression
$$d_0 \to d_1 \to \cdots \to d_{L-1} \to 1,$$
by stacking $L$ Sprecher blocks (each embedding the shift-and-sum transformation). In this way, we recover a deeper analog of the Kolmogorov--Arnold--Sprecher construction with enhanced expressive power while maintaining a clear theoretical connection to the classical result.

\begin{definition}[Network notation]
Throughout this paper, we denote Sprecher Network architectures using arrow notation of the form $d_{\mathrm{in}}\to[d_1,d_2,\ldots,d_L]\to d_{\mathrm{out}}$, where $d_{\mathrm{in}}$ is the input dimension, $[d_1,d_2,\ldots,d_L]$ represents the hidden layer dimensions, and $d_{\mathrm{out}}$ is the output dimension. For example, $2\to[5,3,8]\to1$ describes a network with 2-dimensional input, three hidden layers of widths 5, 3, and 8 respectively, and a scalar output. When input or output dimensions are clear from context, we may use the abbreviated notation $[d_1,d_2,\ldots,d_L]$ to focus on the hidden layer structure.
\end{definition}

\section{Core architectural details}
In our architecture, the fundamental building unit is the \emph{Sprecher block}. The network is composed of a sequence of Sprecher blocks, each of which performs a shift‐and‐sum transformation inspired by Sprecher's original construction. The following subsection details the structure of a single Sprecher block.

\subsection{Sprecher block structure}
A Sprecher block transforms an input vector from $\mathbb{R}^{d_{\mathrm{in}}}$ to an output vector in $\mathbb{R}^{d_{\mathrm{out}}}$ and consists of the following components:
\begin{itemize}
    \item \textbf{Monotonic spline} $\phi(\cdot)$: an increasing piecewise-linear function defined on a prescribed interval (typically $[0,1]$ or a slightly wider interval).
    \item \textbf{General spline} $\Phi(\cdot)$: a piecewise-linear function defined on a sufficiently large interval (e.g., $[-10,12]$) that does not require monotonicity.
    \item A matrix of \textbf{mixing weights} $\{\lambda_{i,q}\}$ of size $d_{\mathrm{in}} \times d_{\mathrm{out}}$, which linearly combines the outputs of the monotonic spline.
    \item A scalar \textbf{shift parameter} $\eta > 0$, which controls how the index $q$ shifts each coordinate.
\end{itemize}

Concretely, given an input vector $\mathbf{x} \in \mathbb{R}^{d_{\mathrm{in}}}$, a single Sprecher block (with its own parameters $\phi$, $\Phi$, $\eta$, and $\lambda$) computes its $q$-th output (with $q=0,\dots,d_{\mathrm{out}}-1$) via
$$\text{Block}_{\phi,\Phi,\eta,\lambda}(\mathbf{x})_q = \Phi\Biggl(\,\sum_{i=1}^{d_{\mathrm{in}}}\lambda_{i,q}\,\phi\Bigl(x_i+\eta\,q\Bigr) + q\Biggr).$$

Note that in a network with two or more hidden layers, each Sprecher block is endowed with its own set of parameters (which may be denoted by $\phi^{(\ell)}$, $\Phi^{(\ell)}$, $\eta^{(\ell)}$, and $\lambda^{(\ell)}$ for the $\ell$-th block). This operation implements a shift-and-sum transformation: each input coordinate is shifted by a multiple of $\eta$, passed through the monotonic spline $\phi$, linearly combined with the mixing weights, and then transformed by the general spline $\Phi$ (with an added constant term $q$ to preserve the shift). By stacking Sprecher blocks as hidden layers, our network constructs a deep, compositional representation that generalizes Sprecher's original one-layer shift-and-sum formulation.

\subsection{Layer composition and final mapping}
Let $L$ denote the number of hidden layers in the network. In our architecture, a ``hidden layer'' refers to the collection of nodes produced by a transformation, while the mapping from one hidden layer to the next is implemented by a \emph{Sprecher block}: a function that applies a shift‐and‐sum operation via a shared pair of spline functions. This means that, unlike traditional neural networks where each neuron is separately parameterized, the nodes in each hidden layer here are generated uniformly by the Sprecher block's common spline parameters, with diversity arising only through their associated mixing weights and index shifts.

Suppose that the $\ell$-th Sprecher block maps 
$$\mathbb{R}^{d_{\ell-1}} \to \mathbb{R}^{d_\ell},$$
with $d_0$ equal to the input dimension. In the $\ell$-th Sprecher block, a monotonic spline $\phi^{(\ell)}$ and a general spline $\Phi^{(\ell)}$ are applied together with the mixing weights $\{\lambda^{(\ell)}_{i,q}\}$ and a shift parameter $\eta^{(\ell)}$. If we denote the output of the $\ell$-th block by $\mathbf{h}^{(\ell)}$ (with $\mathbf{h}^{(0)}=\mathbf{x}$), then the $\ell$-th block computes its $q$-th output as
$$\mathbf{h}^{(\ell)}_q = \Phi^{(\ell)}\Biggl(\,\sum_{i=1}^{d_{\ell-1}} \lambda^{(\ell)}_{i,q}\,\phi^{(\ell)}\Bigl(\mathbf{h}^{(\ell-1)}_i+\eta^{(\ell)}\,q\Bigr) + q\Biggr),\quad q=0,\dots,d_\ell-1.$$

There are two cases for composing the blocks, depending on the desired final output dimension $m$ (i.e. the number of nodes in the output layer):

\paragraph{(a) Scalar output ($m=1$):}  
In this configuration, the final Sprecher block is designed so that its $d_L$ outputs are aggregated by summation to yield a scalar output. Formally, we define
$$f(\mathbf{x}) = \sum_{q=0}^{d_L-1} \mathbf{h}^{(L)}_q.$$
Thus, the overall function can be written in terms of the operators
$$T^{(\ell)}: \mathbb{R}^{d_{\ell-1}} \to \mathbb{R}^{d_\ell},\quad \text{with}\quad \Bigl(T^{(\ell)}(z)\Bigr)_q = \Phi^{(\ell)}\Biggl(\,\sum_{i=1}^{d_{\ell-1}} \lambda^{(\ell)}_{i,q}\,\phi^{(\ell)}\Bigl(z_i+\eta^{(\ell)}\,q\Bigr) + q\Biggr),$$
as
\begin{equation}\label{eq:compositionScalar}
f(\mathbf{x}) = \sum_{q=0}^{d_L-1} \Bigl(T^{(L)} \circ T^{(L-1)} \circ \cdots \circ T^{(1)}\Bigr)(\mathbf{x})_q.
\end{equation}

\paragraph{(b) Vector-valued output ($m>1$):}  
When the target function $f$ maps into $\mathbb{R}^m$ with $m>1$, the network first constructs $L$ hidden layers (each implemented as a Sprecher block), yielding a hidden representation $\mathbf{h}^{(L)} \in \mathbb{R}^{d_L}$. An additional output block is then appended to map $\mathbf{h}^{(L)}$ to $\mathbb{R}^m$ without performing any summation (i.e., leaving the output as a $q$-dependent vector rather than collapsing it to a scalar, to preserve the distinct contributions of each channel). In particular, define
$$\Bigl(T^{(L+1)}(z)\Bigr)_q = \Phi^{(L+1)}\Biggl(\,\sum_{r=0}^{d_L-1} \lambda^{(L+1)}_{r,q}\,\phi^{(L+1)}\Bigl(z_r+\eta^{(L+1)}\,q\Bigr)+q\Biggr),\quad q=0,\dots,m-1,$$
with $z = \mathbf{h}^{(L)}$. Then the overall network function is given by
\begin{equation}\label{eq:compositionVector}
f(\mathbf{x}) = \Bigl(T^{(L+1)} \circ T^{(L)} \circ \cdots \circ T^{(1)}\Bigr)(\mathbf{x}) \in \mathbb{R}^m.
\end{equation}
Note that this output block is constructed in the same manner as the hidden Sprecher blocks (with its own $\phi^{(L+1)}$ and $\Phi^{(L+1)}$), except that it does not include a summation over its outputs. Its role is solely to transform the final hidden representation into an $m$-dimensional output.

\medskip
In summary, when $L \ge 1$ and the final output is scalar, the network consists of exactly $L$ blocks and thus $2L$ spline functions (one pair per block). When the output is vector-valued (with $m>1$), an additional block is appended, resulting in $L+1$ blocks and $2(L+1)$ spline functions. This extra block naturally extends Sprecher's original construction to the vector-valued setting.

\medskip
We illustrate the case of a network with architecture $d_0\to[d_1,d_2,d_3]\to m$ (i.e., $L=3$ hidden layers with $m>1$ outputs) below. Let $X^{(0)}$ be the input, and $X^{(3)}$ the output of the third block. The ``extra'' final block then maps $X^{(3)}$ to $\mathbb{R}^m$:
$$\begin{array}{ccccccc}
X^{(0)} & \xrightarrow{\text{Sprecher block } 1} & X^{(1)} & 
\xrightarrow{\text{Sprecher block } 2} & X^{(2)} &
\xrightarrow{\text{Sprecher block } 3} & X^{(3)} \\
 & & & & & & \downarrow \\
 & & & & & & \text{output block} \\
 & & & & & & \downarrow \\
 & & & & & & \mathbb{R}^m \ (\text{output})
\end{array}$$

\textbf{Note:} Here, $X^{(\ell)}$ denotes the output of the $\ell$-th Sprecher block (i.e., the hidden-layer representation after $\ell$ transformations). Each Sprecher Block internally applies a pair of spline functions $(\phi^{(\ell)}, \Phi^{(\ell)})$, a weight matrix, and a shift parameter to transform its input. The diagram now emphasizes that the hidden representations $X^{(1)}, X^{(2)}, X^{(3)}$ are the complete outputs of their respective blocks, and the final (output) block transforms $X^{(3)}$ into the $m$-dimensional output without an additional summation.

This clarifies that while the classical Sprecher formula involves two splines in a single block (with summation to yield a scalar), a vector-valued extension naturally introduces an extra block and an additional pair of splines.

\medskip
We will now illustrate three examples with $L=1,2,3$ hidden layers in the common scalar-output case (i.e. $m = 1$). These examples demonstrate how the architecture—when used in its most common setting—retains the simplicity and interpretability of Sprecher's original one-layer, shift-and-sum formulation while allowing for deeper, compositional representations.

\subsubsection{Single hidden layer ($L=1$)}

In our architecture, the transformation between hidden layers is performed by a Sprecher block — a function that applies a pair of spline functions $(\phi$ and $\Phi$), a weight matrix, and a shift parameter to its input. Here, ``hidden layer'' specifically denotes the collection of nodes produced by this transformation, which are not individually parameterized as in traditional neural networks. For a network with architecture $d_{\mathrm{in}}\to[d_1]\to1$ (i.e., $d_0 = d_{\mathrm{in}}$ inputs and one hidden layer of dimension $d_1$), the network computes
$$f(\mathbf{x}) = \sum_{q=0}^{d_1-1} \Phi^{(1)}\Biggl(\sum_{i=1}^{d_0} \lambda^{(1)}_{i,q}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,q\Bigr)+q\Biggr).$$
This precisely reproduces Sprecher's 1965 construction
$$f(\mathbf{x}) = \sum_{q=0}^{2n}\Phi\Biggl(\sum_{p=1}^{n}\lambda_p\,\phi\Bigl(x_p+\eta\,q\Bigr)+q\Biggr)$$
when we set $d_1=2d_0+1$ and identify $\phi^{(1)}=\phi,\;\Phi^{(1)}=\Phi$.

\subsubsection{Two hidden layers ($L=2$)}
Let $d_0=d_{\mathrm{in}},d_1$ be the size of the first hidden layer, and $d_2$ the size of the second (final) layer, which is summed to yield a scalar output. Denoting the intermediate output as
$$\mathbf{h}^{(1)}_r=\Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i,r}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+r\Bigr),\quad r=0,\dots,d_1-1,$$
the second block becomes
$$\mathbf{h}^{(2)}_q=\Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r,q}\,\phi^{(2)}\Bigl(\mathbf{h}^{(1)}_r+\eta^{(2)}\,q\Bigr)+q\Bigr),\quad q=0,\dots,d_2-1.$$
Finally, the network output is
$$f(\mathbf{x})=\sum_{q=0}^{d_2-1}\mathbf{h}^{(2)}_q.$$

Equivalently, we can write this as
$$f(\mathbf{x})=\sum_{q=0}^{d_2-1}\Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r,q}\,\phi^{(2)}\Bigl(\Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i,r}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+r\Bigr)+\eta^{(2)}\,q\Bigr)+q\Bigr).$$

\subsubsection{Three hidden layers ($L=3$)}
Now let the layer dimensions be $d_0\to d_1\to d_2\to d_3$, with the final block summed to yield a scalar output. In recursive form, we have
$$\begin{aligned}
\mathbf{h}^{(1)}_r &= \Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i,r}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+r\Bigr),\\[1mm]
\mathbf{h}^{(2)}_s &= \Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r,s}\,\phi^{(2)}\Bigl(\mathbf{h}^{(1)}_r+\eta^{(2)}\,s\Bigr)+s\Bigr),\\[1mm]
\mathbf{h}^{(3)}_q &= \Phi^{(3)}\Bigl(\sum_{s=0}^{d_2-1}\lambda^{(3)}_{s,q}\,\phi^{(3)}\Bigl(\mathbf{h}^{(2)}_s+\eta^{(3)}\,q\Bigr)+q\Bigr).
\end{aligned}$$
Then the network output is
$$f(\mathbf{x})=\sum_{q=0}^{d_3-1}\mathbf{h}^{(3)}_q.$$

The equivalent formulation with nested sums is
$$f(\mathbf{x})=\sum_{q=0}^{d_3-1}\Phi^{(3)}\Bigl(\sum_{s=0}^{d_2-1}\lambda^{(3)}_{s,q}\,\phi^{(3)}\Bigl(\Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r,s}\,\phi^{(2)}\Bigl(\Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i,r}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+r\Bigr)+\eta^{(2)}\,s\Bigr)+s\Bigr)+\eta^{(3)}\,q\Bigr)+q\Bigr).$$

\begin{remark}
These expansions show how each layer's output is obtained by a shift-and-sum transformation using a pair of splines $\phi^{(\ell)}$ and $\Phi^{(\ell)}$. In the scalar-output case, the final layer sums over its output index $q$ to yield a single scalar. In the \emph{Vector-valued output} case (i.e. when $f:[0,1]^d\to\mathbb{R}^m$ with $m>1$), we append an extra output block that is constructed exactly as a Sprecher block but, unlike the final block in the scalar-output case, it does not perform a summation over its outputs. In other words, the extra block produces an $m$-dimensional vector directly (the formula remains the same as for a hidden block, except that the summation over the output index $q$ is omitted).
\end{remark}

\section{Relation to Sprecher (1965) and its generalization}
In his 1965 paper \cite{sprecher1965}, David Sprecher proved a version of the Kolmogorov--Arnold theorem by showing that
$$f(\mathbf{x})=\sum_{q=0}^{2n}\Phi\Bigl(\sum_{p=1}^{n}\lambda_p\,\phi\bigl(x_p+\eta\,q\bigr)+q\Bigr),$$
for suitable choices of monotonic $\phi$ and continuous $\Phi$. Our single-layer Sprecher Network with $d=n$ inputs and $d_{\mathrm{out}}=2n+1$ outputs, \emph{followed by a sum over $q$}, reproduces precisely that formula.

\vspace{1mm}
\textbf{Going deeper.} While Sprecher's original construction used one layer, we can compose multiple shift-and-sum blocks to form a deeper architecture. Each block can reduce or transform the intermediate dimension, thereby creating new function classes that remain ``Sprecher-like'' but with greater representational flexibility.

\section{Implementation Highlights}

\subsection{Trainable splines}
Each block has two spline modules:
\begin{itemize}
    \item $\phi^{(\ell)}$, a monotonic piecewise-linear spline defined on $[0,1]$ (or a slightly wider interval), initialized so that it is increasing.
    \item $\Phi^{(\ell)}$, a general piecewise-linear spline defined on, say, $[-10,12]$ (or any sufficiently wide domain). This spline is not necessarily monotonic and can be initialized near the identity function.
\end{itemize}
Both spline types typically have many knots (e.g. 100--300), which goes well beyond the requirements of the original theorem that only demanded continuity and monotonicity for $\phi$.

\subsection{Shifts and weights}
Each block has a scalar $\eta^{(\ell)}$ controlling how the block's index $q$ shifts its input coordinates, plus a matrix $\lambda^{(\ell)}_{i,q}$ mixing each input. These free parameters are updated via gradient-based optimizers to minimize a loss function of the form
$$\text{(MSE on training data)} + \alpha \times \text{(penalty on spline second derivatives)},$$
where the penalty encourages smoother spline functions.

\subsection{Parameter counting}
Since each block contains two splines (each with potentially hundreds of knot parameters), the total number of trainable parameters is typically dominated by the spline coefficients. If each $\phi^{(\ell)}$ has $K_\phi$ knots and each $\Phi^{(\ell)}$ has $K_\Phi$ knots, then each block contributes roughly $K_\phi + K_\Phi$ spline parameters, plus $d_{\ell-1}\,d_\ell$ weight parameters and one $\eta^{(\ell)}$. Summing over all blocks yields the total parameter count. Note that Sprecher Networks have a significant advantage over KANs (see \cite{liu2024kan}) in that their parameter counts don't explode for wide networks.

\subsection{Sprecher-like theorems}
If we restrict to one layer with $d_{\mathrm{out}}=2d+1$, a monotonic $\phi$, and suitable constants $(\lambda_i,\eta)$, then we reproduce the original Sprecher formula. Sprecher's 1965 argument implies that this single-layer network can approximate any continuous function $f:[0,1]^d\to\mathbb{R}$ arbitrarily well, provided $\phi$ and $\Phi$ are sufficiently flexible.

An interesting possibility arises when we consider the case of a single hidden layer with vector output. In particular, for a network approximating a function $f:[0,1]^d\to\mathbb{R}^m$ with $m>1$, one could set $d_{\mathrm{out}}=2d+1$ in the hidden layer and then append an output block that omits the final summation over $q$. This construction would then possibly generalize Sprecher's original scalar formulation to the vector-valued setting. Although the precise relationship between the number of hidden nodes and the output dimension may depend on further architectural details—and the universality of such an extension remains highly conjectural—it suggests a natural direction for future theoretical investigation.

Explicitly, we can formulate this conjecture as follows:
\begin{conjecture}[Vector-valued Sprecher representation]
Let $n\in\mathbb{N}$ and let $f:[0,1]^n \to \mathbb{R}^m$ be any continuous function with $m>1$. Then there exist monotonic splines $\phi^{(1)}:[0,1]\to\mathbb{R}$ and $\phi^{(2)}:J\to\mathbb{R}$, general splines $\Phi^{(1)}:I_1\to\mathbb{R}$ and $\Phi^{(2)}:J\to\mathbb{R}$ (defined on suitable intervals $I_1$ and $J$), mixing weight matrices $\lambda^{(1)}_{i,r}$ for $1\le i\le n$ and $0\le r\le 2n$ and $\lambda^{(2)}_{r,q}$ for $0\le r\le 2n$ and $0\le q\le m-1$, and shift parameters $\eta^{(1)},\eta^{(2)}>0$ such that for every $\mathbf{x}=(x_1,\dots,x_n)\in [0,1]^n$ one has
\begin{equation}\label{eq:SprecherVector}
f(\mathbf{x}) = \Biggl[\Phi^{(2)}\Biggl(\sum_{r=0}^{2n} \lambda^{(2)}_{r,q}\,\phi^{(2)}\Biggl(\Phi^{(1)}\Bigl(\sum_{i=1}^{n} \lambda^{(1)}_{i,r}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+r\Bigr)+\eta^{(2)}\,q\Biggr)+q\Biggr)\Biggr]_{q=0}^{m-1}.
\end{equation}
\end{conjecture}
\begin{remark}
In the case $m=1$, the representation in \eqref{eq:SprecherVector} reduces (upon summing over $q$) to Sprecher's original formulation.
\end{remark}

\subsection{Deeper extensions}
A natural conjecture is that stacking these blocks preserves universal approximation capability while potentially requiring fewer summation nodes. Intuitively, multi-layer compositions can capture complex structure more efficiently. However, a complete theoretical treatment of depth versus width for Sprecher Networks warrants further research.

\section{Empirical demonstrations}
In practice, we train Sprecher Networks on small data sets sampled from a target function $f$. Two classic examples are:
\begin{itemize}
    \item \textbf{1D case:} A polynomial or sinusoidal function on $[0,1]$. The network learns $\phi$ and $\Phi$ splines that approximate $f$ very accurately, effectively acting as a learnable spline interpolant.
    \item \textbf{2D case:} A surface such as 
    $$f(x,y)=\frac{1}{7}\Bigl(\exp\bigl(\sin(\pi x)+y^2\bigr)-1\Bigr).$$ 
    With sufficient training (typically after $O(10^5)$ gradient updates), the network achieves good approximation. Layerwise spline plots provide an interpretable view of how the network transforms inputs.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{TwoVarsThreeBlocks.png}
\caption{Visualization of a trained $2\to[5,8,5]\to1$ Sprecher Network approximating a 2D target function $z = f(x,y)$. \textbf{Top row:} The learned spline functions for each block - monotonic splines $\phi^{(\ell)}$ (cyan) and general splines $\Phi^{(\ell)}$ (magenta). \textbf{Bottom row:} Comparison between the target function (left) and the network approximation (right) showing the high-quality fit achieved after training.}
\label{fig:twovarsprecher}
\end{figure}

Figure~\ref{fig:twovarsprecher} demonstrates a trained Sprecher Network with architecture $2\to[5,8,5]\to1$, approximating a 2D surface. The top row shows the learned spline functions for each block, illustrating how they adaptively shape themselves during training. Note how the monotonic splines (cyan) maintain their increasing property while developing distinct patterns in each layer. The general splines (magenta) exhibit more complex shapes that capture the nonlinearities required for accurate function approximation. The bottom row comparison confirms that even with just three blocks, the network achieves a remarkably precise fit to the target function.

\section{Conclusion}
We have introduced and motivated the design of \emph{Sprecher Networks}, a modern, trainable extension of Sprecher's single-layer shift-and-sum formula for function approximation. By using two splines ($\phi$ and $\Phi$) per block, along with careful index shifting and summation, we retain the theoretical grounding of the Kolmogorov--Arnold approach, thereby enabling deeper networks with potentially greater expressive power.

\vspace{1mm}
\textbf{Further related work:} Recent work in neural network design has re-examined the Kolmogorov--Arnold theorem from fresh perspectives. In particular, \emph{Kolmogorov--Arnold Networks (KANs)} \cite{liu2024kan} propose learnable spline functions on edges instead of nodes, showing that such architectures can sometimes surpass MLPs in accuracy and interpretability for certain tasks. While KANs share the general KA motivation, the \emph{Sprecher Network} approach here uses one monotonic and one non-monotonic spline in each layer, with a summation over shifted inputs. It would be interesting in future work to compare these designs more thoroughly and unify them under the broader KA umbrella.

\vspace{1mm}
\textbf{Future Directions:} Potential research avenues include:
\begin{itemize}
    \item \emph{High-dimensional} spline adaptations (e.g., tensor-product or multi-dimensional $\phi,\Phi$).
    \item Incorporation of \emph{residual} or \emph{skip connections} for more powerful deep architectures.
    \item \emph{Theoretical} comparisons of depth versus width in Sprecher-style networks.
\end{itemize}

\bigskip

\begin{thebibliography}{9}
\bibitem{sprecher1965}
Sprecher, D.~A.~(1965). ``On the Structure of Continuous Functions of Several Variables,'' \emph{Transactions of the American Mathematical Society}, \textbf{115}, 340--355.

\bibitem{kolmogorov}
Kolmogorov, A.~N.~(1957). ``On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition,'' \emph{Doklady Akademii Nauk SSSR}, \textbf{114}.

\bibitem{arnold}
Arnold, V.~I.~(1963). ``On functions of three variables,'' \emph{Doklady Akademii Nauk SSSR}, \textbf{48}.

\bibitem{liu2024kan}
Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\v{c}i\'{c}, M., Hou, T.Y., Tegmark, M.~(2025).  
``KAN: Kolmogorov-Arnold Networks,'' \emph{ICLR 2025 (to appear)}.  
arXiv preprint \href{https://arxiv.org/abs/2404.19756}{arXiv:2404.19756}.
\end{thebibliography}

\end{document}