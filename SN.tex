\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb,mathrsfs}
\usepackage{graphicx}
\usepackage[pdfencoding=auto, psdextra]{hyperref}
\usepackage{cleveref}
\usepackage[figurename=Figure]{caption}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{tikz,tikz-cd}
\usetikzlibrary{positioning, arrows.meta, decorations.pathreplacing}
\usepackage{pifont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tabularx,array}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\newcommand{\mathpdf}[2]{\texorpdfstring{$#1$}{#2}}

\title{Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture}
\author{
  Christian Hägg\thanks{Department of Mathematics, Stockholm University, Stockholm, Sweden. Email: \texttt{hagg@math.su.se}} \and
  Kathlén Kohn\thanks{Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden. Email: \texttt{kathlen@kth.se}} \and
  Giovanni Luca Marchetti\thanks{Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden. Email: \texttt{glma@kth.se}} \and
  Boris Shapiro\thanks{Department of Mathematics, Stockholm University, Stockholm, Sweden. Email: \texttt{shapiro@math.su.se}}
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present \emph{Sprecher Networks} (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov--Arnold--Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (\emph{monotonic} and \emph{general}) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 ``sum of shifted splines'' formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).
\end{abstract}

\section{Introduction and historical background}
Approximation of continuous functions by sums of univariate functions has been a recurring theme in mathematical analysis and neural networks. The Kolmogorov--Arnold Representation Theorem \cite{kolmogorov, arnold} established that any multivariate continuous function $f : [0,1]^d \to \mathbb{R}$ can be represented as a finite composition of continuous functions of a single variable and the addition operation. Specifically, Kolmogorov (1957) showed that such functions can be represented as a finite sum involving univariate functions applied to sums of other univariate functions of the inputs.

\vspace{1mm}
\textbf{David Sprecher's 1965 construction.} In his 1965 landmark paper \cite{sprecher1965}, David Sprecher provided a constructive proof and a specific formula realizing the Kolmogorov--Arnold representation. He showed that any continuous function $f:[0,1]^n \to \mathbb{R}$ could be represented as:
\begin{equation}\label{eq:sprecher_original}
f(\mathbf{x})=\sum_{q=0}^{2n}\Phi\Biggl(\,\sum_{p=1}^{n}\lambda_p\,\phi\bigl(x_p+\eta\,q\bigr)+q\Biggr)
\end{equation}
for a single \emph{monotonic} inner function $\phi$, a continuous outer function $\Phi$, a constant shift parameter $\eta > 0$, and constants $\lambda_p$. This construction simplified the representation by using only one inner function $\phi$, relying on shifts of the input coordinates ($x_p + \eta q$) and an outer summation index shift ($+q$) to achieve universality. The key insight of \emph{shifting input coordinates} and summing evaluations under inner and outer univariate maps is central to Sprecher's specific result.

\vspace{1mm}
\textbf{From a shallow theorem to a deep architecture.} Sprecher's 1965 result is remarkable because it guarantees universal approximation with a single hidden layer (i.e., a shallow network). This mirrors the history of Multi-Layer Perceptrons, where the Universal Approximation Theorem also guaranteed the sufficiency of a single hidden layer. However, the entire deep learning revolution was built on the empirical discovery that composing multiple layers, while not theoretically necessary for universality, provides vast practical benefits in terms of efficiency and learnability.

This history motivates our central research question: can the components of Sprecher's shallow, highly-structured formula be used as a new kind of building block in a \emph{deep}, compositional architecture? We propose to investigate this by composing what we term \emph{Sprecher blocks}, where the vector output of one block becomes the input to the next. It is crucial to emphasize that this deep, compositional structure is our own architectural proposal, inspired by the paradigm of deep learning, and is not part of Sprecher's original construction or proof of universality. The goal of this paper is not to generalize Sprecher's theorem, but to empirically evaluate whether this theorem-inspired design is a viable and efficient alternative to existing deep learning models when extended into a deep framework.

\vspace{1mm}
\textbf{Modern context.} Recent work has revitalized interest in leveraging Kolmogorov-Arnold representations for modern deep learning. Notably, Kolmogorov-Arnold Networks (KANs) \cite{liu2024kan} were introduced, proposing an architecture with learnable activation functions (splines) placed on the \emph{edges} of the network graph, replacing traditional linear weights and fixed node activations.

\vspace{1mm}
\textbf{Architectural landscape.} Understanding how novel architectures relate to established ones is crucial. Standard Multi-Layer Perceptrons (MLPs) \cite{haykin1994neural} employ fixed nonlinear activation functions at nodes and learnable linear weights on edges, justified by the Universal Approximation Theorem \cite{cybenko1989approximation, hornik1989multilayer}. Extensions include networks with \emph{learnable activations on nodes}, sometimes called Adaptive-MLPs or Learnable Activation Networks (LANs) \cite{goyal2019learning, zhang2022neural, liu2024kan}, which retain linear edge weights but make the node non-linearity trainable. KANs \cite{liu2024kan} represent a more significant departure, moving learnable splines to edges and eliminating linear weights entirely, using simple summation at nodes. Sprecher Networks (SNs), as we detail below, propose a distinct approach derived directly from Sprecher's 1965 formula. SNs employ function blocks containing shared learnable splines ($\phi, \Phi$), learnable mixing weights ($\lambda$), explicit structural shifts ($\eta, q$), and optionally, lateral mixing connections for intra-block communication. This structure offers a different alternative within the landscape of function approximation networks.

\section{Motivation and overview of Sprecher Networks}
While MLPs are the workhorse of deep learning, architectures inspired by KAS representations offer potential benefits, particularly in interpretability and potentially parameter efficiency for certain function classes. KANs explore one direction by placing learnable functions on edges. Our \emph{Sprecher Networks} (SNs) explore a different direction, aiming to directly implement Sprecher's constructive formula within a trainable framework and extend it to deeper architectures.

SNs are built upon the following principles, directly reflecting Sprecher's formula:
\begin{itemize}
    \item Each functional block (mapping between layers) is organized around a shared \emph{monotonic} spline $\phi(\cdot)$ and a shared \emph{general} spline $\Phi(\cdot)$, both learnable.
    \item Each block incorporates a learnable scalar shift $\eta$ applied to inputs based on the output index $q$.
    \item Each block includes learnable mixing weights $\lambda_{i}$ (a vector, not a matrix) that combine contributions from different input dimensions, with the same weights shared across all output dimensions.
    \item The structure explicitly includes the additive shift $\alpha q$ inside the outer spline $\Phi$, where $\alpha$ is a scaling factor (typically $\alpha = 1$) that maintains consistency with Sprecher's formulation.
    \item Optionally, blocks can include lateral mixing connections that allow output dimensions to exchange information before the outer spline transformation, enhancing expressivity with minimal parameter overhead.
\end{itemize}
Our architecture generalizes this classical single-layer shift-and-sum construction to a multi-layer network by composing these functional units, which we term \emph{Sprecher blocks}. The mapping from one hidden layer representation to the next is realized by such a block. Unlike MLPs with fixed node activations, LANs with learnable node activations, or KANs with learnable edge activations, SNs concentrate their learnable non-linearity into the two shared splines per block, applied in a specific structure involving shifts and learnable linear weights. This imposes a strong inductive bias, trading the flexibility of independent weights/splines for extreme parameter sharing. Diversity in the transformation arises from the mixing weights ($\lambda$), the index-dependent shifts ($q$), and when enabled, the lateral mixing connections.

Concretely, each Sprecher block applies the transformation:
$$ (x_i)_{i=1}^{d_{\mathrm{in}}} \;\mapsto\; \Bigl[\Phi^{(\ell)}\Bigl(s_q^{(\ell)} \;+\; \tau^{(\ell)}\!\!\sum_{j\in\mathcal{N}(q)} \omega_{q,j}^{(\ell)}\, s_j^{(\ell)}\Bigr)\Bigr]_{q=0}^{d_{\mathrm{out}}-1},$$
where $s_q^{(\ell)}=\sum_{i=1}^{d_{\mathrm{in}}}\lambda^{(\ell)}_i\,\phi^{(\ell)}(x_i+\eta^{(\ell)} q)+\alpha q$.
Lateral mixing (if enabled) modifies the pre-activation $s_q^{(\ell)}$ \emph{before} $\Phi^{(\ell)}$, whereas residual connections (if enabled) are applied \emph{after} $\Phi^{(\ell)}$ and do not introduce lateral coupling across $q$.
For scalar outputs, the outputs of the final Sprecher block are aggregated (via summation); for vector outputs, no final summation is applied.

In Sprecher's original work, one layer (block) with $d_{\mathrm{out}} = 2n+1$ outputs (where $n=d_{\mathrm{in}}$) was sufficient for universality. Our approach stacks $L$ Sprecher blocks to create a deep network progression:
$$ d_0 \to d_1 \to \cdots \to d_{L-1} \to d_L, $$
where $d_0=d_{\mathrm{in}}$ is the input dimension, and $d_L$ is the dimension of the final hidden representation before potential aggregation or final mapping. This multi-block composition provides a deeper analog of the KAS construction, aiming for potentially enhanced expressive power or efficiency for complex compositional functions. Although the overall SN family is universal already for $L=1$ (Theorem~\ref{thm:ua_single_layer}), we do not yet have a universality characterization under fixed depth/width constraints for $L>1$ blocks (nor for our vector-output extension); we explore this empirically (see Section~\ref{sec:universality}).

\begin{definition}[Network notation]
Throughout this paper, we denote Sprecher Network architectures using arrow notation of the form $d_{\mathrm{in}}\to[d_1,d_2,\ldots,d_L]\to d_{\mathrm{out}}$, where $d_{\mathrm{in}}$ is the input dimension, $[d_1,d_2,\ldots,d_L]$ represents the hidden layer dimensions (widths), and $d_{\mathrm{out}}$ is the final output dimension of the network. For scalar output ($d_{\mathrm{out}}=1$), the final block's outputs are summed. For vector output ($d_{\mathrm{out}}>1$), an additional non-summed block maps from $d_L$ to $d_{\mathrm{out}}$. For example, $2\to[5,3,8]\to1$ describes a network with 2-dimensional input, three hidden layers of widths 5, 3, and 8 respectively, and a scalar output (implying the final block's outputs of dimension 8 are summed). $2\to[5,3]\to4$ describes a network with 2-dimensional input, two hidden layers of widths 5 and 3, and a 4-dimensional vector output (implying an additional output block maps from dimension 3 to 4 without summation). When input or output dimensions are clear from context, we may use the abbreviated notation $[d_1,d_2,\ldots,d_L]$ to focus on the hidden layer structure.
\end{definition}

\section{Core architectural details}
In our architecture, the fundamental building unit is the \emph{Sprecher block}. The network is composed of a sequence of Sprecher blocks, each performing a shift-and-sum transformation inspired by Sprecher's original construction.

\subsection{Sprecher block structure}
A Sprecher block transforms an input vector $\mathbf{x} \in \mathbb{R}^{d_{\mathrm{in}}}$ to an output vector $\mathbf{h} \in \mathbb{R}^{d_{\mathrm{out}}}$. The data flow through a single block is illustrated in Figure~\ref{fig:block_flow}. This transformation is implemented using the following shared, learnable components specific to that block:
\begin{itemize}
    \item \textbf{Monotonic spline $\phi(\cdot)$:} A non-decreasing spline mapping $\mathbb{R} \to [0,1]$ (piecewise-linear in our implementation; see \Cref{sec:implementation}) that is applied coordinate-wise. While we denote the domain as $\mathbb{R}$ for generality, in practice each $\phi^{(\ell)}$ operates on a bounded domain determined dynamically by the network's structure and current parameters, as detailed in Section~\ref{sec:theoretical_domains}. This function is shared across all input-output connections within the block and its coefficients are learnable. Monotonicity is enforced during training through appropriate parameterization (see Section \ref{sec:implementation}).
    \item \textbf{General spline $\Phi(\cdot)$:} A spline mapping $\mathbb{R} \to \mathbb{R}$ (piecewise-linear or $C^1$ piecewise-cubic Hermite/PCHIP; see \Cref{sec:implementation}, without monotonicity constraints) whose domain and codomain are determined by the network structure and can be either fixed or made trainable. If trainable codomains are used, they can be parameterized in various ways, such as by center and scale (half-width) parameters, allowing the spline to adapt its output range during training. This function is also shared across the block and its coefficients are learnable.
    \item \textbf{Mixing weights vector $\lambda$:} A vector $\{\lambda_{i}\}$ of size $d_{\mathrm{in}}$, whose entries are learnable. These weights linearly combine the contributions from different input dimensions after transformation by $\phi$. Crucially, these weights are shared across all output dimensions within the block, maintaining fidelity to Sprecher's original formulation.
    \item \textbf{Shift parameter $\eta$:} A learnable scalar $\eta$. This parameter controls the magnitude of the input shift $x_i + \eta q$, which depends on the output index $q$. While Sprecher's original construction requires $\eta > 0$, practical implementations may allow $\eta$ to take any real value during training.
    \item \textbf{Lateral mixing parameters (optional):} When enabled, learnable parameters that allow intra-block communication between output dimensions. This includes a scalar $\tau^{(\ell)}$ (lateral scale) and weight vector(s) $\omega^{(\ell)}$ that enable outputs to exchange information before passing through $\Phi^{(\ell)}$. Two variants are supported: \emph{cyclic} (each output receives from its cyclic neighbor) and \emph{bidirectional} (each output mixes with both neighbors).
\end{itemize}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        >=latex, 
        % Horizontal spacing 1.2cm (wide), Vertical 1.0cm (slightly taller for label space)
        node distance=1.0cm and 1.2cm, 
        font=\small
    ]
        % Define custom colors matching the presentation
        \definecolor{sprecherblue}{RGB}{70,130,180}
        \definecolor{sprecherred}{RGB}{220,20,60}
        \definecolor{sprechergreen}{RGB}{34,139,34}
        \definecolor{sprecherorange}{RGB}{255,140,0}
        \definecolor{sprecherpurple}{RGB}{147,51,234}

        % Define node styles
        \tikzstyle{op}=[
            draw, 
            rounded corners, 
            inner sep=6pt, 
            align=center, 
            minimum height=0.75cm
        ]
        \tikzstyle{flowarrow}=[->, thick, >=stealth, rounded corners]

        % --- Top Row (Forward Pass) ---
        \node[op, fill=blue!10] (input) {$\mathbf{x} \in \mathbb{R}^{d_{\mathrm{in}}}$};
        \node[op, fill=blue!10, right=of input] (shift) {$+\eta q$};
        \node[op, fill=sprechergreen!20, right=of shift] (phi) {$\phi$ (mono.)};
        \node[op, fill=orange!10, right=of phi] (lambda) {$\times \lambda_i$};
        \node[op, right=of lambda] (sum) {$\sum_i$};

        % --- Bottom Row (Reverse Direction) ---
        % Position 'plusq' directly below 'sum'
        \node[op, below=of sum] (plusq) {$+ \alpha q$};
        \node[op, fill=sprecherorange!10, left=of plusq] (mix) {lateral\\mix};
        \node[op, fill=sprecherpurple!20, left=of mix] (Phi) {$\Phi$};
        \node[op, fill=red!10, left=of Phi] (res) {+ resid.};
        \node[op, fill=blue!10, left=of res] (output) {$\mathbf{h} \in \mathbb{R}^{d_{\mathrm{out}}}$};

        % --- Main Flow Connections ---
        \draw[flowarrow] (input) -- (shift);
        \draw[flowarrow] (shift) -- (phi);
        \draw[flowarrow] (phi) -- (lambda);
        \draw[flowarrow] (lambda) -- (sum);
        \draw[flowarrow] (sum) -- (plusq);
        \draw[flowarrow] (plusq) -- (mix);
        \draw[flowarrow] (mix) -- (Phi);
        \draw[flowarrow] (Phi) -- (res);
        \draw[flowarrow] (res) -- (output);

        % --- Residual Connection (Fixed) ---
        % Removed fill=white. Positioned text 'right' of the curve to avoid overlap.
        \draw[->, dashed, sprecherred, thick] (input.south) 
            .. controls +(0.5,-1.5) and +(0,1.5) .. 
            node[pos=0.5, right, xshift=-16pt, yshift=-6pt, font=\scriptsize, text=sprecherred] {residual path}
            (res.north);

    \end{tikzpicture}
    \caption{Data flow through a single Sprecher block. Each input $x_i$ is shifted by $\eta q$ (where $q$ indexes outputs), passed through the shared monotonic spline $\phi$, weighted by $\lambda_i$, and summed. The pre-activation $s_q = \sum_i \lambda_i \phi(x_i + \eta q) + \alpha q$ undergoes optional lateral mixing before being transformed by the shared general spline $\Phi$. Residual connections (dashed) provide direct gradient paths.}
    \label{fig:block_flow}
\end{figure}

Concretely, we first define a single Sprecher block as an operator. Let $B^{(\ell)}: \mathbb{R}^{d_{\ell-1}} \to \mathbb{R}^{d_\ell}$ denote the $\ell$-th Sprecher block with parameters $\phi^{(\ell)}, \Phi^{(\ell)}, \eta^{(\ell)}, \lambda^{(\ell)}$, a fixed spacing constant $\alpha$ (set to $1$ in all experiments unless noted), and optionally $\tau^{(\ell)}, \omega^{(\ell)}$. Given an input vector $\mathbf{x} = (x_1, \dots, x_{d_{\mathrm{in}}}) \in \mathbb{R}^{d_{\mathrm{in}}}$, the block computes its output vector $\mathbf{h} \in \mathbb{R}^{d_{\mathrm{out}}}$ component-wise as:
$$ [B^{(\ell)}(\mathbf{x})]_q = \Phi^{(\ell)}\Biggl(\,s_q^{(\ell)} + \tau^{(\ell)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(\ell)} s_j^{(\ell)}\Biggr),$$

where $s_q^{(\ell)} = \sum_{i=1}^{d_{\mathrm{in}}} \lambda_i^{(\ell)} \phi^{(\ell)}(x_i + \eta^{(\ell)} q) + \alpha q$ represents the pre-mixing activation, $\alpha$ is a fixed channel-spacing constant (we set $\alpha=1$ in all experiments unless explicitly stated otherwise), and $\mathcal{N}(q)$ denotes the neighborhood structure:
\begin{itemize}
    \item \textbf{No mixing:} $\mathcal{N}(q) = \emptyset$ (reduces to original formulation)
    \item \textbf{Cyclic:} $\mathcal{N}(q) = \{(q+1) \bmod d_{\mathrm{out}}\}$ with scalar weights $\omega_q^{(\ell)}$
    \item \textbf{Bidirectional:} $\mathcal{N}(q) = \{(q-1) \bmod d_{\mathrm{out}}, (q+1) \bmod d_{\mathrm{out}}\}$ with weights $\omega_{q,\text{bwd}}^{(\ell)}$ for the $(q-1)\bmod d_{\mathrm{out}}$ neighbor and $\omega_{q,\text{fwd}}^{(\ell)}$ for the $(q+1)\bmod d_{\mathrm{out}}$ neighbor
\end{itemize}

While $\alpha = 1$ maintains theoretical fidelity, alternative values may be explored to improve optimization dynamics in deeper networks. Note that $q$ serves dual roles here: as an output index ($q = 0, \ldots, d_{\mathrm{out}}-1$) and as an additive shift parameter within the formula.

In a network with multiple layers, each Sprecher block (indexed by $\ell=1, \dots, L$ or $L+1$) uses its own independent set of shared parameters $(\phi^{(\ell)}, \Phi^{(\ell)}, \eta^{(\ell)}, \lambda^{(\ell)})$ and optionally $(\tau^{(\ell)}, \omega^{(\ell)})$. The block operation implements a specific form of transformation: each input coordinate $x_i$ is first shifted by an amount depending on the output index $q$ and the shared shift parameter $\eta^{(\ell)}$, then passed through the shared monotonic spline $\phi^{(\ell)}$. The results are linearly combined using the learnable mixing weights $\lambda^{(\ell)}_{i}$, shifted again by the output index scaled by $\alpha$. When lateral mixing is enabled, these pre-activation values undergo weighted mixing with neighboring outputs. Finally, the result is passed through the shared general spline $\Phi^{(\ell)}$. Figure~\ref{fig:spline_sharing} illustrates this spline-sharing structure within a single block, highlighting the key architectural difference from KANs. Stacking these blocks creates a deep, compositional representation.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
    >=Stealth,
    node distance=0.6cm,
    inputnode/.style={circle, draw=black!70, fill=blue!8, minimum size=8mm, inner sep=1pt, font=\small},
    outputnode/.style={circle, draw=black!70, fill=orange!12, minimum size=8mm, inner sep=1pt, font=\small},
    sumnode/.style={circle, draw=black!70, fill=gray!12, minimum size=7mm, inner sep=0pt, font=\scriptsize},
    phicolor/.style={draw=blue!55, thick},
    Phicolor/.style={draw=orange!65, thick},
]

% === Input nodes ===
\node[inputnode] (x1) at (0, 1.8) {$x_1$};
\node[inputnode] (x2) at (0, 0.6) {$x_2$};
\node[inputnode] (x3) at (0, -0.6) {$x_3$};

% === Summation nodes (centered horizontally) ===
\node[sumnode] (s0) at (4.2, 2.4) {$\Sigma$};
\node[sumnode] (s1) at (4.2, 0.6) {$\Sigma$};
\node[sumnode] (s2) at (4.2, -1.2) {$\Sigma$};

% === Output nodes ===
\node[outputnode] (h0) at (8.4, 2.4) {$h_0$};
\node[outputnode] (h1) at (8.4, 0.6) {$h_1$};
\node[outputnode] (h2) at (8.4, -1.2) {$h_2$};

% === Edges from inputs to sum nodes ===
% Row q=0
\draw[->, black!35] (x1) -- (s0);
\draw[->, black!35] (x2) -- (s0);
\draw[->, black!35] (x3) -- (s0);
% Row q=1
\draw[->, black!35] (x1) -- (s1);
\draw[->, black!35] (x2) -- (s1);
\draw[->, black!35] (x3) -- (s1);
% Row q=2
\draw[->, black!35] (x1) -- (s2);
\draw[->, black!35] (x2) -- (s2);
\draw[->, black!35] (x3) -- (s2);

% === Edges from sum nodes to outputs ===
\draw[->, black!50, thick] (s0) -- (h0);
\draw[->, black!50, thick] (s1) -- (h1);
\draw[->, black!50, thick] (s2) -- (h2);

% === φ spline symbols (MONOTONIC - sigmoid/S-curve shape, going UP) ===
% Centered between inputs and sums (x ≈ 2.1)
\draw[phicolor] (1.85, 2.45) 
    .. controls (1.95, 2.45) and (2.0, 2.45) .. (2.1, 2.6)
    .. controls (2.2, 2.75) and (2.25, 2.75) .. (2.35, 2.75);
\draw[phicolor] (1.85, 0.65) 
    .. controls (1.95, 0.65) and (2.0, 0.65) .. (2.1, 0.8)
    .. controls (2.2, 0.95) and (2.25, 0.95) .. (2.35, 0.95);
% Shifted down to avoid edge overlap
\draw[phicolor] (1.85, -1.35) 
    .. controls (1.95, -1.35) and (2.0, -1.35) .. (2.1, -1.2)
    .. controls (2.2, -1.05) and (2.25, -1.05) .. (2.35, -1.05);

% === Φ spline symbols (general wavy shape) ===
% Centered between sums and outputs (x ≈ 6.3)
\draw[Phicolor] (5.9, 2.65) 
    .. controls (6.05, 2.45) and (6.15, 2.8) .. (6.3, 2.6)
    .. controls (6.45, 2.4) and (6.55, 2.75) .. (6.7, 2.55);
\draw[Phicolor] (5.9, 0.85) 
    .. controls (6.05, 0.65) and (6.15, 1.0) .. (6.3, 0.8)
    .. controls (6.45, 0.6) and (6.55, 0.95) .. (6.7, 0.75);
\draw[Phicolor] (5.9, -0.95) 
    .. controls (6.05, -1.15) and (6.15, -0.8) .. (6.3, -1.0)
    .. controls (6.45, -1.2) and (6.55, -0.85) .. (6.7, -1.05);

% === Shift labels (+η·q) - plain text, positioned between φ and Σ ===
\node[font=\scriptsize, text=black!65] at (3.2, 2.7) {$+\eta \cdot 0$};
\node[font=\scriptsize, text=black!65] at (3.2, 1.2) {$+\eta \cdot 1$};
\node[font=\scriptsize, text=black!65] at (3.2, -1.35) {$+\eta \cdot 2$};

% === Alpha·q labels (above Σ nodes) ===
\node[font=\tiny, text=black!55] at (4.2, 2.95) {$+\alpha\!\cdot\!0$};
\node[font=\tiny, text=black!55] at (4.2, 1.15) {$+\alpha\!\cdot\!1$};
\node[font=\tiny, text=black!55] at (4.2, -0.65) {$+\alpha\!\cdot\!2$};

% === Annotations for shared splines (more vertical spacing) ===
\node[font=\scriptsize, text=blue!55, align=center] at (2.1, 3.85) {\textbf{Same} $\phi$};
\node[font=\footnotesize, text=blue!55] at (2.1, 3.5) {(shared)};
\draw[->, blue!40, dashed] (2.1, 3.25) -- (2.1, 2.9);

% Changed text=orange!60 to orange!100 (or just orange) for better contrast
\node[font=\scriptsize, text=orange, align=center] at (6.3, 3.85) {\textbf{Same} $\Phi$};
\node[font=\footnotesize, text=orange] at (6.3, 3.5) {(shared)};
\draw[->, orange, dashed] (6.3, 3.25) -- (6.3, 2.85);

% === Lambda annotation (single label showing weights are shared) ===
\node[font=\scriptsize, text=black!60] at (1.0, 3.0) {weights $\lambda_i$};
\draw[->, black!40, dashed] (1.0, 2.75) -- (0.6, 2.15);

% === Braces ===
\draw[decorate, decoration={brace, amplitude=5pt}, black!60] 
    (-0.6, -0.95) -- (-0.6, 2.15) node[midway, left=6pt, font=\scriptsize, text=black!70, align=right] {$d_{\mathrm{in}}$\\inputs};

\draw[decorate, decoration={brace, amplitude=5pt, mirror}, black!60]
    (8.95, -1.55) -- (8.95, 2.75) node[midway, right=6pt, font=\scriptsize, text=black!70, align=left] {$d_{\mathrm{out}}$\\outputs};

% === Block title ===
\node[font=\small\bfseries, text=black!75] at (2.0, -2.4) {Single Sprecher Block};

% === Legend box (more spacing from nodes) ===
\node[draw=black!30, rounded corners=2pt, fill=white, inner sep=5pt, font=\scriptsize, align=left] 
    at (7.2, -2.4) {
    \textcolor{blue!55}{\rule{10pt}{2pt}} $\phi$: monotonic (1 per block)\\[2pt]
    \textcolor{orange!65}{\rule{10pt}{2pt}} $\Phi$: general (1 per block)\\[2pt]
    $+\eta q$: input shift (varies with $q$)
};

\end{tikzpicture}
\caption{Internal structure of a Sprecher block showing spline sharing. Unlike KANs where each edge has a unique learnable spline, a Sprecher block uses only \textbf{two shared splines}: one monotonic $\phi$ applied to all shifted inputs, and one general $\Phi$ applied to all weighted sums. The mixing weights $\lambda_i$ are shared across all output dimensions. Diversity across outputs arises from the index-dependent shifts $+\eta q$ (applied before $\phi$) and $+\alpha q$ (added to each sum). This extreme parameter sharing yields $O(d_{\mathrm{in}} + G)$ parameters per block versus $O(d_{\mathrm{in}} \cdot d_{\mathrm{out}} \cdot G)$ for KANs.}
\label{fig:spline_sharing}
\end{figure}

\begin{remark}[Computational considerations]
While the block operation as written suggests computing all $d_{\mathrm{out}}$ outputs simultaneously, implementations may compute them sequentially to reduce memory usage from $O(B \cdot d_{\mathrm{in}} \cdot d_{\mathrm{out}})$ to $O(B \cdot \max(d_{\mathrm{in}}, d_{\mathrm{out}}))$ where $B$ is the batch size. This sequential computation produces mathematically identical results to the parallel formulation. This is particularly valuable when exploring architectures with wide layers, where memory constraints often limit feasible network configurations before parameter count becomes prohibitive. Section~\ref{sec:memory_efficient} details this sequential computation strategy.
\end{remark}

\subsection{Optional enhancements}
Several optional components can enhance the basic Sprecher block:

\begin{itemize}
    \item \textbf{Residual connections (Cyclic):} When enabled, residual connections are implemented using a dimension-adaptive modulo-based cyclic assignment that maintains architectural coherence. Like convolutional layers that achieve parameter efficiency through local connectivity patterns, cyclic residuals use periodic patterns to reduce parameters from $O(d_{\mathrm{in}} \times d_{\mathrm{out}})$ to $O(\max(d_{\mathrm{in}}, d_{\mathrm{out}}))$. This method adapts based on dimensional relationships between adjacent layers:
    \begin{itemize}
        \item \textbf{Identity} ($d_{\mathrm{in}} = d_{\mathrm{out}}$): Uses a single learnable weight $w_{\text{res}}$
        \item \textbf{Broadcasting} ($d_{\mathrm{in}} < d_{\mathrm{out}}$): Each output dimension cyclically selects an input via $1+(q \bmod d_{\mathrm{in}})$ and applies a learnable scale
        \item \textbf{Pooling} ($d_{\mathrm{in}} > d_{\mathrm{out}}$): Input dimensions are cyclically assigned to outputs via $(i-1) \bmod d_{\mathrm{out}}$ with learnable weights
    \end{itemize}
    Specifically, the block output with residual connection becomes:
    $$[B^{(\ell)}_{\text{res}}(\mathbf{x})]_q = [B^{(\ell)}(\mathbf{x})]_q + \begin{cases}
    w_{\text{res}} \cdot x_{q+1} & \text{if } d_{\mathrm{in}} = d_{\mathrm{out}}\\
    w_q^{\text{bcast}} \cdot x_{1+(q \bmod d_{\mathrm{in}})} & \text{if } d_{\mathrm{in}} < d_{\mathrm{out}}\\
    \sum_{i: (i-1) \bmod d_{\mathrm{out}} = q} w_i^{\text{pool}} \cdot x_i & \text{if } d_{\mathrm{in}} > d_{\mathrm{out}}
    \end{cases}$$
    where $w^{\text{bcast}} \in \mathbb{R}^{d_{\mathrm{out}}}$ and $w^{\text{pool}} \in \mathbb{R}^{d_{\mathrm{in}}}$ are learnable weight vectors. Empirically, these constrained connections often perform comparably to full projection residuals while using substantially fewer residual parameters.
    
    \item \textbf{Lateral mixing connections (distinct from cyclic residuals):} Inspired by the success of attention mechanisms and lateral connections in vision models, we introduce an optional intra-block communication mechanism. Before applying the outer spline $\Phi^{(\ell)}$, each output dimension can incorporate weighted contributions from neighboring outputs:
    $$\tilde{s}_q^{(\ell)} = s_q^{(\ell)} + \tau^{(\ell)} \cdot \begin{cases}
    \omega_q^{(\ell)} \cdot s_{(q+1) \bmod d_{\mathrm{out}}}^{(\ell)} & \text{(cyclic)} \\
    \omega_{q,\text{fwd}}^{(\ell)} \cdot s_{(q+1) \bmod d_{\mathrm{out}}}^{(\ell)} + \omega_{q,\text{bwd}}^{(\ell)} \cdot s_{(q-1) \bmod d_{\mathrm{out}}}^{(\ell)} & \text{(bidirectional)}
    \end{cases}$$
    This mechanism allows the network to learn correlations between output dimensions while maintaining the parameter efficiency of the architecture, adding only $O(d_{\mathrm{out}})$ parameters per block. Empirically, we find this particularly beneficial for vector-valued outputs and deeper networks.
    
    \item \textbf{Normalization:} Batch normalization can be applied to block outputs to improve training stability, particularly in deeper networks. See Section \ref{sec:normalization} for details.
    
    \item \textbf{Output affine head:} The network may include a learnable affine head applied to the final network output: $f_{\text{final}}(\mathbf{x}) = s_{\text{out}}\, f(\mathbf{x}) + b_{\text{out}}$, where $s_{\text{out}}, b_{\text{out}}\in\mathbb{R}$ (2 parameters) are shared across all output dimensions. In our experiments we initialize $b_{\text{out}}$ to the mean of the training targets (averaged over all outputs when $m>1$) and set $s_{\text{out}}=0.1$.

\end{itemize}

\begin{remark}[Lateral mixing as structured attention]
The lateral mixing mechanism can be viewed as a highly constrained form of self-attention where each output dimension ``attends'' only to its immediate neighbors in a cyclic topology (see Figure~\ref{fig:lateral_mixing}). Unlike full attention which requires $O(d_{\mathrm{out}}^2)$ parameters, our approach maintains linear scaling while still enabling cross-dimensional information flow. This design choice reflects our philosophy of extreme parameter sharing: just as the Sprecher structure shares splines across all connections within a block, lateral mixing shares the communication pattern across all samples while learning only the mixing weights.
\end{remark}

\begin{figure}[ht]
    \centering
    \definecolor{sprecherblue}{RGB}{70,130,180}
    \definecolor{sprechergreen}{RGB}{34,139,34}
    \begin{tikzpicture}[scale=1.1]
        % Cyclic mixing diagram
        \begin{scope}[xshift=0cm]
            \node[circle, draw, fill=sprecherblue!30, minimum size=12mm, font=\small] (n0) at (0:1.8) {$s_0$};
            \node[circle, draw, fill=sprecherblue!30, minimum size=12mm, font=\small] (n1) at (72:1.8) {$s_1$};
            \node[circle, draw, fill=sprecherblue!30, minimum size=12mm, font=\small] (n2) at (144:1.8) {$s_2$};
            \node[circle, draw, fill=sprecherblue!30, minimum size=12mm, font=\small] (n3) at (216:1.8) {$s_3$};
            \node[circle, draw, fill=sprecherblue!30, minimum size=12mm, font=\small] (n4) at (288:1.8) {$s_4$};
            \draw[->, thick, sprechergreen, bend left=18] (n1) to (n0);
            \node[font=\scriptsize, text=sprechergreen] at (36:2) {$\omega_0$};
            \draw[->, thick, sprechergreen, bend left=18] (n2) to (n1);
            \node[font=\scriptsize, text=sprechergreen] at (108:2) {$\omega_1$};
            \draw[->, thick, sprechergreen, bend left=18] (n3) to (n2);
            \node[font=\scriptsize, text=sprechergreen] at (180:2) {$\omega_2$};
            \draw[->, thick, sprechergreen, bend left=18] (n4) to (n3);
            \node[font=\scriptsize, text=sprechergreen] at (252:2) {$\omega_3$};
            \draw[->, thick, sprechergreen, bend left=18] (n0) to (n4);
            \node[font=\scriptsize, text=sprechergreen] at (324:2) {$\omega_4$};
            \node[font=\small\bfseries] at (0,-3.0) {Cyclic lateral mixing};
            \node[font=\footnotesize, align=center] at (0,-3.6) {$\tilde{s}_q = s_q + \tau \omega_q s_{(q+1) \bmod d_{\mathrm{out}}}$};
        \end{scope}
        
        % Equation box
        \begin{scope}[xshift=6.5cm, yshift=-0.5cm]
            \node[draw, rounded corners, fill=gray!5, inner sep=10pt, align=left, font=\small] {
                \textbf{Pre-activation:}\\[3pt]
                $s_q = \sum_{i=1}^{d_{\mathrm{in}}} \lambda_i \phi(x_i + \eta q) + \alpha q$\\[8pt]
                \textbf{After lateral mixing:}\\[3pt]
                $\tilde{s}_q = s_q + \tau \sum_{j \in \mathcal{N}(q)} \omega_{q,j} s_j$\\[8pt]
                \textbf{Output:}\\[3pt]
                $h_q = \Phi(\tilde{s}_q)$
            };
        \end{scope}
    \end{tikzpicture}
    \caption{Lateral mixing enables cross-dimensional communication before the outer spline $\Phi$.
    In the cyclic variant (shown), each pre-activation $s_q$ receives a scaled contribution from its neighbor $s_{(q+1) \bmod d_{\mathrm{out}}}$, parameterized by scale $\tau$ and per-output weights $\omega_q$.
    This adds only $O(d_{\mathrm{out}})$ parameters while breaking the symmetries inherent in the shared-weight structure.
    The bidirectional variant additionally includes contributions from $s_{(q-1) \bmod d_{\mathrm{out}}}$.}
    \label{fig:lateral_mixing}
\end{figure}

\begin{remark}[Lateral mixing resolves the shared-weight optimization plateau]
Wide single-layer Sprecher Networks exhibit a particularly severe optimization challenge beyond the typical difficulties of shallow networks. Due to the shared weight vector $\lambda$ (rather than a weight matrix), all output dimensions process identical linear combinations of inputs, 
differing only through the deterministic index-dependent shifts $x_i\mapsto x_i+\eta q$ (inside $\phi$) and the additive term $+\alpha q$ (in the pre-activation), leading to nearly identical outputs early in training. This symmetry can cause outputs to collapse into similar representations, reducing effective capacity and slowing convergence. 

Cyclic residual connections help by injecting input information directly into each output, 
but in very wide layers this may still be insufficient to break output symmetry and avoid plateaus.
As an illustrative example, a $2\to[120]\to1$ network trained on the 2D regression benchmark (\textsc{Toy-2D-Complex}) $f(x_1,x_2)=\exp(\sin(11x_1))+3x_2+4\sin(8x_2)$ over $[0,1]^2$ (with a fixed $32\times 32$ training grid) reaches an MSE of $8.55$ after $40{,}000$ epochs with cyclic residuals (broadcast weights; $120$ residual parameters). Replacing the cyclic residual with a dense projection residual ($2\times120=240$ residual parameters) provides no improvement (MSE: $8.55$), while adding cyclic lateral mixing (a scalar $\tau$ plus $120$ mixing weights) adds only $121$ parameters and drops the MSE to $2.12$. This demonstrates that the problem is not simply the general difficulty of optimizing wide shallow networks (which affects MLPs too), but 
specifically the need to break symmetries created by the shared-weight constraint. The lateral mixing mechanism provides the minimal cross-dimensional communication needed to differentiate the outputs beyond mere shifting, enabling successful optimization where traditional approaches fail.
\end{remark}

\begin{remark}[Cyclic residuals as dimensional folding]
The cyclic assignment pattern can be understood geometrically as a form of dimensional folding. When $d_{\mathrm{in}} > d_{\mathrm{out}}$, the modulo operation $(i-1) \bmod d_{\mathrm{out}}$ effectively ``wraps'' the higher-dimensional input space around the lower-dimensional output space, analogous to wrapping a string around a cylinder. This creates a regular pattern where input dimensions are distributed uniformly across outputs, ensuring complete coverage and direct gradient paths. The cyclic pattern maximally separates consecutive inputs; for instance, with $d_{\mathrm{in}}=7, d_{\mathrm{out}}=3$, consecutive inputs $\{1,2,3,4,5,6,7\}$ map to outputs $\{0,1,2,0,1,2,0\}$ respectively, preventing the local clustering that might occur with contiguous block assignments.
\end{remark}

\subsubsection{Toy-$4\to5$ synthetic regression task}
\label{sec:toy4to5}
We additionally consider a higher-dimensional vector-valued regression benchmark, \textsc{Toy-$4\to5$}, with inputs $\mathbf{x}=(x_1,x_2,x_3,x_4)\in[0,1]^4$ and outputs $f(\mathbf{x})\in\mathbb{R}^5$ defined by
\begin{align*}
f_1(\mathbf{x}) &= \sin(2\pi x_1)\cos(\pi x_2),\\
f_2(\mathbf{x}) &= \exp\!\bigl(-2(x_1^2+x_2^2)\bigr),\\
f_3(\mathbf{x}) &= x_3^3 - x_4^2 + \tfrac{1}{2}\sin(5x_3),\\
f_4(\mathbf{x}) &= \operatorname{sigmoid}\!\bigl(3(x_1+x_2-x_3+x_4)\bigr),\\
f_5(\mathbf{x}) &= \tfrac{1}{2}\sin(4\pi x_1 x_4)+\tfrac{1}{2}\cos(3\pi x_2 x_3),
\end{align*}
where $\operatorname{sigmoid}(t)=\frac{1}{1+e^{-t}}$.
Unless otherwise stated, training inputs are sampled i.i.d. uniformly from $[0,1]^4$.

\begin{table}[ht]
\centering
\caption{Cyclic residuals vs.\ linear residual projections on synthetic regression tasks. We report the best MSE on a fixed 1{,}024-point training set (measured in evaluation mode) achieved within the stated number of epochs. Toy-2D-Complex refers to the scalar 2D regression benchmark defined above; Toy-2D (vector) refers to the vector-valued 2D benchmark in Fig.~\ref{fig:twovarsprechervector_revised}; Toy-$4\to5$ refers to the synthetic regression task defined in Section~\ref{sec:toy4to5}. ``Linear'' uses a learned scalar when $d_{\mathrm{in}}=d_{\mathrm{out}}$ and a learned matrix $W\in\mathbb{R}^{d_{\mathrm{in}}\times d_{\mathrm{out}}}$ otherwise; ``cyclic'' uses the pooling/broadcast construction when $d_{\mathrm{in}}\neq d_{\mathrm{out}}$ (and a learned scalar when $d_{\mathrm{in}}=d_{\mathrm{out}}$). The last columns count residual-path parameters only. For tasks with $d_{\mathrm{out}}>1$, MSE is computed as the mean of $(\hat y-y)^2$ over all output dimensions and samples.}
\label{tab:cyclic-residual-ablation}
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llrrrrr@{}}
\toprule
Task & Architecture & Epochs & MSE (linear) & MSE (cyclic) & \#resid params (lin./cyc.) & Ratio \\
\midrule
Toy-2D-Complex & $2\rightarrow[10,10,10]\rightarrow1$ & 10{,}000 & $4.33\times10^{-2}$ & $\mathbf{1.79\times10^{-3}}$ & $22/12$ & $1.8\times$ \\
Toy-2D-Complex & $2\rightarrow[10,11,12,13,14,15,16,17]\rightarrow1$ & 50{,}000 & $\mathbf{9.77\times10^{-6}}$ & $3.88\times10^{-5}$ & $1322/108$ & $12.2\times$ \\
Toy-2D-Complex & $2\rightarrow[10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11]\rightarrow1$ & 50{,}000 & $\mathbf{1.52\times10^{-5}}$ & $3.89\times10^{-5}$ & $1670/175$ & $9.5\times$ \\
Toy-2D (vector) & $2\rightarrow[50,50,50]\rightarrow2$ & 10{,}000 & $7.50\times10^{-6}$ & $\mathbf{2.46\times10^{-6}}$ & $202/102$ & $2.0\times$ \\
Toy-2D (vector) & $2\rightarrow[10,11,12,13,14,15,16,17]\rightarrow2$ & 5{,}000 & $\mathbf{5.47\times10^{-3}}$ & $9.09\times10^{-2}$ & $1356/125$ & $10.8\times$ \\
Toy-$4\to5$ & $4\rightarrow[30,40]\rightarrow5$ & 10{,}000 & $\mathbf{8.01\times10^{-3}}$ & $1.60\times10^{-2}$ & $1520/110$ & $13.8\times$ \\
Toy-$4\to5$ & $4\rightarrow[10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11]\rightarrow5$ & 10{,}000 & $\mathbf{3.24\times10^{-4}}$ & $5.51\times10^{-3}$ & $1745/186$ & $9.4\times$ \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{table}[t]
\centering
\caption{Ablation on Toy-2D-Vector. Mean test RMSE $\pm$ std over 10 seeds (5{,}000 epochs per run). Lower is better.}
\label{tab:toy2d_addons_ablation}
\small
\begin{tabular}{lc}
\toprule
Setting & Test RMSE \\
\midrule
Base SN (no residuals/mixing/domain tracking) & $0.325 \pm 0.015$ \\
+ cyclic residuals & $0.226 \pm 0.065$ \\
+ bidirectional mixing & $0.211 \pm 0.055$ \\
+ domain tracking & $0.046 \pm 0.050$ \\
+ resampling (full SN) & $\mathbf{0.029 \pm 0.020}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht]
    \centering
    \definecolor{sprecherorange}{RGB}{255,140,0}
    % Added >={Latex[scale=0.8]} to globally shrink arrowheads
    \begin{tikzpicture}[scale=0.9, >={Latex[scale=0.8]}]
        % Identity case
        \begin{scope}[xshift=0cm]
            \node[font=\small\bfseries, color=blue!70!black] at (1.25,0.5) {Identity};
            \node[font=\footnotesize, color=gray] at (1.25,0.1) {$d_{\mathrm{in}} = d_{\mathrm{out}}$};
            
            \foreach \i in {1,2,3} {
                \node[circle, draw, fill=blue!20, minimum size=6mm, font=\footnotesize] (x\i) at (0, -\i*0.9) {\i};
                \node[circle, draw, fill=red!20, minimum size=6mm, font=\footnotesize] (y\i) at (2.5, -\i*0.9) {\i};
                \draw[->, dashed, sprecherorange, thick] (x\i) -- (y\i);
            }
            
            \node[font=\footnotesize, align=center] at (1.25,-4.5) {$h_q \mathrel{+}= w \cdot x_{q}$\\[2pt]\footnotesize 1 parameter};
        \end{scope}
        
        % Pooling case
        \begin{scope}[xshift=5.5cm]
            \node[font=\small\bfseries, color=green!50!black] at (1.25,0.5) {Pooling};
            \node[font=\footnotesize, color=gray] at (1.25,0.1) {$d_{\mathrm{in}} > d_{\mathrm{out}}$};
            
            \foreach \i in {1,...,5} {
                \node[circle, draw, fill=blue!20, minimum size=5mm, font=\scriptsize] (px\i) at (0, -\i*0.7) {\i};
            }
            \node[circle, draw, fill=red!20, minimum size=6mm, font=\footnotesize] (py1) at (2.5, -1.4) {1};
            \node[circle, draw, fill=red!20, minimum size=6mm, font=\footnotesize] (py2) at (2.5, -2.8) {2};

            % Cyclic assignment
            \draw[->, dashed, sprecherorange, thick] (px1) -- (py1);
            \draw[->, dashed, sprecherorange, thick] (px2) -- (py2);
            \draw[->, dashed, sprecherorange, thick] (px3) -- (py1);
            \draw[->, dashed, sprecherorange, thick] (px4) -- (py2);
            \draw[->, dashed, sprecherorange, thick] (px5) -- (py1);

            \node[font=\footnotesize, align=center] at (1.25,-4.5) {$h_q \mathrel{+}= \sum_{i:\ (i-1)\bmod d_{\mathrm{out}} = q-1} w_i x_i$\\[2pt]\footnotesize $d_{\mathrm{in}}$ parameters};
        \end{scope}
        
        % Broadcast case
        \begin{scope}[xshift=11cm]
            \node[font=\small\bfseries, color=red!70!black] at (1.25,0.5) {Broadcast};
            \node[font=\footnotesize, color=gray] at (1.25,0.1) {$d_{\mathrm{in}} < d_{\mathrm{out}}$};
            
            \node[circle, draw, fill=blue!20, minimum size=6mm, font=\footnotesize] (bx1) at (0, -1.4) {1};
            \node[circle, draw, fill=blue!20, minimum size=6mm, font=\footnotesize] (bx2) at (0, -2.8) {2};

            \foreach \j in {1,...,5} {
                \node[circle, draw, fill=red!20, minimum size=5mm, font=\scriptsize] (by\j) at (2.5, -\j*0.7) {\j};
            }
            
            % Cyclic source
            \draw[->, dashed, sprecherorange, thick] (bx1) -- (by1);
            \draw[->, dashed, sprecherorange, thick] (bx2) -- (by2);
            \draw[->, dashed, sprecherorange, thick] (bx1) -- (by3);
            \draw[->, dashed, sprecherorange, thick] (bx2) -- (by4);
            \draw[->, dashed, sprecherorange, thick] (bx1) -- (by5);

            \node[font=\footnotesize, align=center] at (1.25,-4.5) {$h_q \mathrel{+}= w_q \cdot x_{((q-1) \bmod d_{\mathrm{in}})+1}$\\[2pt]\footnotesize $d_{\mathrm{out}}$ parameters};
        \end{scope}
    \end{tikzpicture}
    \caption{Dimension-adaptive cyclic residual connections (indices shown 1-based for readability). \textbf{Left:} When dimensions match, a single scalar weight applies element-wise. \textbf{Center:} When $d_{\mathrm{in}} > d_{\mathrm{out}}$ (pooling), multiple inputs are cyclically assigned to each output via modular indexing. \textbf{Right:} When $d_{\mathrm{in}} < d_{\mathrm{out}}$ (broadcast), inputs are cyclically reused across outputs. All cases maintain $O(\max(d_{\mathrm{in}}, d_{\mathrm{out}}))$ parameters, preserving the linear scaling of the architecture.}
    \label{fig:cyclic_residuals}
\end{figure}

\begin{remark}[Structured sparsity and architectural coherence]
The cyclic residual design can be viewed as implementing a structured sparse projection where each output connects to approximately $d_{\mathrm{in}}/d_{\mathrm{out}}$ inputs (pooling case) or each input connects to approximately $d_{\mathrm{out}}/d_{\mathrm{in}}$ outputs (broadcasting case). This represents another form of weight sharing that complements the main Sprecher architecture: while Sprecher blocks share splines across output dimensions with diversity through shifts, residual connections share structure across connections with diversity through cyclic assignment. The unexpected effectiveness of this severe constraint suggests that for gradient flow and information propagation, the specific connection pattern may matter less than ensuring uniform coverage and balanced paths, a hypothesis that warrants further theoretical investigation.
\end{remark}

\begin{figure}[ht]
    \centering
    \definecolor{sprechergreen}{RGB}{34,139,34}
    \definecolor{sprecherpurple}{RGB}{147,51,234}
    \begin{tikzpicture}
        % === Inner spline phi (Left) ===
        \begin{scope}[xshift=0cm]
            % Subfigure Title (Moved up to 3.8 for better clearance)
            \node[font=\small\bfseries] at (2.1, 3.8) {Inner spline $\phi$ (monotonic)};

            % Axes
            \draw[->] (0,0) -- (4.2,0) node[right] {$x$};
            % Label 'above' the arrow tip
            \draw[->] (0,0) -- (0,2.8) node[above] {$\phi(x)$};
            
            % Draw monotonic spline
            \draw[thick, sprechergreen] 
                (0,0) -- (0.6,0.14) -- (1.2,0.32) -- (1.8,0.56) -- 
                (2.4,0.74) -- (3.0,0.88) -- (3.6,1.0);
            
            % Mark knots
            \foreach \x/\y in {0/0, 0.6/0.14, 1.2/0.32, 1.8/0.56, 2.4/0.74, 3.0/0.88, 3.6/1.0} {
                \fill[sprechergreen] (\x,\y) circle (2pt);
            }
            
            % Codomain annotation
            \draw[<->, gray] (-0.4,0) -- (-0.4,1.0);
            \node[left, gray, font=\footnotesize] at (-0.4,0.5) {$[0,1]$};
            
            % Label: Fixed offset from x-axis
            \node[font=\footnotesize] at (2.1,-0.7) {dynamic domain};
        \end{scope}
        
        % === Outer spline Phi (Right) ===
        \begin{scope}[xshift=7cm]
            % Subfigure Title (Moved up to 3.8)
            \node[font=\small\bfseries] at (2.1, 3.8) {Outer spline $\Phi$ (general)};

            % Axes (x-axis shifted down to y=-0.5)
            \draw[->] (0,-0.5) -- (4.2,-0.5) node[right] {$s$};
            % Label 'above'
            \draw[->] (0,-0.5) -- (0,2.8) node[above] {$\Phi(s)$};
            
            % Draw general (non-monotonic) spline
            \draw[thick, sprecherpurple] 
                (0,0.6) -- (0.6,1.8) -- (1.2,0.4) -- (1.8,2.1) -- 
                (2.4,-0.1) -- (3.0,1.4) -- (3.6,0.8);
            
            % Mark knots
            \foreach \x/\y in {0/0.6, 0.6/1.8, 1.2/0.4, 1.8/2.1, 2.4/-0.1, 3.0/1.4, 3.6/0.8} {
                \fill[sprecherpurple] (\x,\y) circle (2pt);
            }
            
            % Codomain annotation
            \draw[<->, gray] (-0.4,-0.1) -- (-0.4,2.1);
            \node[left, gray, font=\footnotesize, align=right] at (-0.5,1.0) {$[c_c{-}c_r,$\\$c_c{+}c_r]$};
            
            % Label: Fixed offset from x-axis (y=-0.5 -> pos -1.2)
            \node[font=\footnotesize] at (2.1,-1.2) {dynamic domain};
        \end{scope}
    \end{tikzpicture}
    \caption{The dual spline system in Sprecher Networks. Left: The inner spline $\phi$ is monotonic (non-decreasing) with fixed codomain $[0, 1]$, parameterized via cumulative sums of softplus-transformed increments; it is strictly increasing on its spline domain and uses constant extension outside that domain ($0$ for inputs below the leftmost knot, $1$ above the rightmost knot). Right: The outer spline $\Phi$ is a general (non-monotonic) univariate spline (piecewise-linear or cubic PCHIP) with optional learnable codomain parameters $(c_c, c_r)$ defining center and radius. It uses linear extrapolation outside its domain. Both domains can be updated during training as $\lambda$ and $\eta$ evolve; in some experiments we freeze domain updates after a warm-up period (and in our PINN experiments we disable domain updates altogether).}
    \label{fig:dual_splines}
\end{figure}

\subsection{Normalization considerations}\label{sec:normalization}

The unique structure of Sprecher Networks requires careful consideration when incorporating normalization techniques. While standard neural networks typically normalize individual neuron activations, the shared-spline architecture of SNs suggests a different approach: normalizing the entire vector output $\mathbf{h}^{(\ell)} \in \mathbb{R}^{d_\ell}$ of each Sprecher block as a cohesive unit.

Batch normalization can be placed either \emph{before} or \emph{after} a Sprecher block. In both placements it acts on the vector features $\mathbf{h}$, not on the internal scalars $s^{(\ell)}_q$, so the additive $+\alpha q$ and any lateral mixing remain entirely inside $\Phi^{(\ell)}$.
\begin{itemize}
    \item \textbf{After block (default).} Apply BN to the block output $\mathbf{h}^{(\ell)} \in \mathbb{R}^{d_\ell}$.
    \item \textbf{Before block.} Apply BN to the input $\mathbf{h}^{(\ell-1)} \in \mathbb{R}^{d_{\ell-1}}$ before it enters the block.
\end{itemize}

The transformation takes the familiar form:
$$\tilde{\mathbf{h}} = \gamma \odot \frac{\mathbf{h} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$
where $\gamma, \beta \in \mathbb{R}^{d}$ are learnable affine parameters, and the statistics $\mu$ and $\sigma^2$ are computed per-dimension across the batch.

This approach maintains the parameter efficiency central to SNs: each normalization layer adds only $2d$ parameters, preserving the $O(LN)$ scaling. Moreover, by treating the block output as a unified representation rather than normalizing individual components, the method respects the architectural philosophy that all output dimensions arise from shared transformations through $\phi^{(\ell)}$ and $\Phi^{(\ell)}$.

We often find it beneficial to skip normalization for the first block (default setting), allowing the network to directly process the input features while still stabilizing deeper layers. This selective application is particularly important when theoretical domain computation assumes inputs in $[0,1]^n$, as normalization could shift the data outside expected ranges.

When computing theoretical domains (Section~\ref{sec:theoretical_domains}), the bounds must account for normalization:
\begin{itemize}
    \item \textbf{Training mode}: Conservative bounds assume standardization to approximately $[-4, 4]$ (covering ~99.99\% of a standard normal distribution), with affine transformation applied afterward.
    \item \textbf{Evaluation mode}: Uses the actual per-channel running statistics (and affine parameters, if enabled) to compute tighter bounds.
\end{itemize}

Our empirical findings suggest that the combination of normalization with residual connections and lateral mixing proves particularly effective for deeper SNs, enabling successful training of networks with many blocks while maintaining the characteristic parameter efficiency of the architecture.

\subsection{Layer composition and final mapping}
Let $L$ be the number of hidden layers specified by the architecture $[d_1, \dots, d_L]$. In our framework, a ``hidden layer'' corresponds to the vector output of a Sprecher block. The mapping from the representation at layer $\ell-1$ to layer $\ell$ is implemented by the $\ell$-th Sprecher block.

Let the input to the network be $\mathbf{h}^{(0)} = \mathbf{x} \in \mathbb{R}^{d_0}$ (where $d_0 = d_{\mathrm{in}}$). The spline core of the $\ell$-th Sprecher block ($\ell = 1, \dots, L$) produces a vector $\tilde{\mathbf{h}}^{(\ell)} \in \mathbb{R}^{d_\ell}$, computed component-wise as:
\begin{align}
    \label{eq:SN}\tilde{\mathbf{h}}^{(\ell)}_q = \Phi^{(\ell)}\Biggl(\,\sum_{i=1}^{d_{\ell-1}} \lambda^{(\ell)}_{i}\,\phi^{(\ell)}\Bigl(\mathbf{h}^{(\ell-1)}_i+\eta^{(\ell)}\,q\Bigr) + \alpha q + \tau^{(\ell)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(\ell)} s_j^{(\ell)}\Biggr) + R^{(\ell)}\!\bigl(\mathbf{h}^{(\ell-1)}\bigr)_q,\quad q=0,\dots,d_\ell-1.
\end{align}
where the lateral mixing term is included when enabled, and $s_j^{(\ell)}$ denotes the pre-mixing activation for output $j$. The term $R^{(\ell)}(\mathbf{h}^{(\ell-1)})_q$ denotes an (optional) residual connection (e.g., the cyclic residual of Fig.~\ref{fig:cyclic_residuals} or a standard learned linear projection); set $R^{(\ell)}\equiv 0$ when residuals are disabled. When BatchNorm is used, it is applied before or after each block as described in Section~\ref{sec:normalization}. Throughout, $\tilde{\mathbf{h}}^{(\ell)}$ denotes the pre-normalization output of the Sprecher block; if BatchNorm-after-block is enabled then $\mathbf{h}^{(\ell)}$ denotes the post-normalization vector passed to the next layer (otherwise $\mathbf{h}^{(\ell)}=\tilde{\mathbf{h}}^{(\ell)}$).

\begin{remark}[On the nature of composition]
Note that in this $L>1$ composition (Eq. \ref{eq:SN}), the argument to the inner spline $\phi^{(\ell)}$ is $\mathbf{h}^{(\ell-1)}_i$, the output of the previous layer, not the original input coordinate $x_i$. This is the fundamental departure from Sprecher's construction and is the defining feature of our proposed deep architecture. The motivation for this compositional structure comes not from Sprecher's work, but from the empirical success of the deep learning paradigm. Each layer processes the complex, transformed output of the layer before it, enabling the network to learn hierarchical representations.
\end{remark}

The composition of these blocks and the final output generation depend on the desired final output dimension $m=d_{\mathrm{out}}$:

\paragraph{(a) Scalar output ($m=1$):}
The network consists of exactly $L$ Sprecher blocks. The output of the final block, $\mathbf{h}^{(L)} \in \mathbb{R}^{d_L}$, is aggregated by summation to yield the scalar output:
$$ f(\mathbf{x}) = \sum_{q=0}^{d_L-1} \mathbf{h}^{(L)}_q. $$
If we define the operator for the $\ell$-th block (including residuals when enabled) as $T^{(\ell)}: \mathbb{R}^{d_{\ell-1}} \to \mathbb{R}^{d_\ell}$, where
$$ \Bigl(T^{(\ell)}(z)\Bigr)_q = \Phi^{(\ell)}\Biggl(\,\sum_{i=1}^{d_{\ell-1}} \lambda^{(\ell)}_{i}\,\phi^{(\ell)}\Bigl(z_i+\eta^{(\ell)}\,q\Bigr) + \alpha q + \tau^{(\ell)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(\ell)} s_j^{(\ell)}\Biggr) + R^{(\ell)}(z)_q, $$
then the overall function is
$$ f(\mathbf{x}) = \sum_{q=0}^{d_L-1} \Bigl(T^{(L)} \circ T^{(L-1)} \circ \cdots \circ T^{(1)}\Bigr)(\mathbf{x})_q. $$
This network uses $L$ blocks and $2L$ shared spline functions in total (one pair $(\phi^{(\ell)}, \Phi^{(\ell)})$ per block).

\paragraph{(b) Vector-valued output ($m>1$):}
When the target function $f$ maps to $\mathbb{R}^m$ with $m>1$, the network first constructs the $L$ hidden layers as above, yielding a final hidden representation $\mathbf{h}^{(L)} \in \mathbb{R}^{d_L}$. An \emph{additional} output block (block $L+1$) is then appended to map this representation $\mathbf{h}^{(L)}$ to the final output space $\mathbb{R}^m$. This $(L+1)$-th block operates \emph{without} a final summation over its output index. It computes the final output vector $\mathbf{y} \in \mathbb{R}^m$ as:
$$ y_q = \Bigl(T^{(L+1)}(\mathbf{h}^{(L)})\Bigr)_q = \Phi^{(L+1)}\Biggl(\,\sum_{r=0}^{d_L-1} \lambda^{(L+1)}_{r}\,\phi^{(L+1)}\Bigl(\mathbf{h}^{(L)}_r+\eta^{(L+1)}\,q\Bigr)+\alpha q + \tau^{(L+1)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(L+1)} s_j^{(L+1)}\Biggr) + R^{(L+1)}(\mathbf{h}^{(L)})_q, $$
for $q=0,\dots,m-1$. The network output function is then:
\begin{equation}\label{eq:defsn}
f(\mathbf{x}) = \mathbf{y} = \Bigl(T^{(L+1)} \circ T^{(L)} \circ \cdots \circ T^{(1)}\Bigr)(\mathbf{x}) \in \mathbb{R}^m. 
\end{equation}
In this configuration, the network uses $L+1$ blocks and involves $2(L+1)$ shared spline functions. The extra block serves as a trainable output mapping layer, transforming the final hidden representation $\mathbf{h}^{(L)}$ into the desired $m$-dimensional output vector.

\medskip
In summary: for an architecture with $L$ hidden layers, a scalar-output SN uses $L$ blocks and $2L$ shared splines. A vector-output SN (with $m>1$) uses $L+1$ blocks and $2(L+1)$ shared splines. This structure provides a natural extension of Sprecher's original scalar formula to the vector-valued setting. In our supervised regression experiments we additionally apply a learned affine head to the final output,
$$f_{\mathrm{final}}(\mathbf{x}) \;=\; s_{\mathrm{out}}\, f(\mathbf{x}) + b_{\mathrm{out}},$$
with $s_{\mathrm{out}}, b_{\mathrm{out}}\in\mathbb{R}$ (2 parameters, shared across output dimensions). We initialize $b_{\mathrm{out}}$ to the mean of the training targets (averaged over all outputs when $m>1$) and set $s_{\mathrm{out}}=0.1$. In PINN experiments (where no full supervised target set is available), we keep the default initialization $s_{\mathrm{out}}=1$ and $b_{\mathrm{out}}=0$.
\medskip

We illustrate the vector-output case ($m>1$) for a network architecture $d_0\to[d_1,d_2,d_3]\to m$ (i.e., $L=3$ hidden layers). Let $X^{(0)}$ be the input $\mathbf{x}$.
\begin{center}
\begin{tikzpicture}[
    node distance=0.8cm and 2.2cm, % Vertical and horizontal node distances
    block/.style={font=\small}, % Style for block labels
    dim/.style={font=\footnotesize} % Style for dimension labels
  ]
    % Define nodes
    \node (X0)                                      {$X^{(0)}$};
    \node (L0) [below=0.07cm of X0, dim]             {$(\in \mathbb{R}^{d_0})$};
    \node (X1) [right=of X0]                        {$X^{(1)}$};
    \node (L1) [below=0.07cm of X1, dim]             {$(\in \mathbb{R}^{d_1})$};
    \node (X2) [right=of X1]                        {$X^{(2)}$};
    \node (L2) [below=0.07cm of X2, dim]             {$(\in \mathbb{R}^{d_2})$};
    \node (X3) [right=of X2]                        {$X^{(3)}$};
    \node (L3) [right=0.07cm of X3, dim]             {$(\in \mathbb{R}^{d_3})$};
    \node (OB) [below=of X3]                        {Output block 4 $(T^{(4)})$};
    \node (Y)  [below=of OB]                        {$\mathbf{y} \in \mathbb{R}^m$ (Network output)};

    % Draw arrows
    \draw[-{Stealth[length=2mm]}] (X0) -- node[above, block] {Block 1} (X1);
    \draw[-{Stealth[length=2mm]}] (X1) -- node[above, block] {Block 2} (X2);
    \draw[-{Stealth[length=2mm]}] (X2) -- node[above, block] {Block 3} (X3);
    \draw[-{Stealth[length=2mm]}] (X3) -- (OB); % Vertical arrow
    \draw[-{Stealth[length=2mm]}] (OB) -- (Y);  % Vertical arrow
\end{tikzpicture}
\end{center}
Here, $X^{(\ell)} = \mathbf{h}^{(\ell)}$ denotes the output vector of the $\ell$-th Sprecher block (we use both notations interchangeably for clarity in different contexts). Each block $T^{(\ell)}$ internally uses its own pair of shared splines $(\phi^{(\ell)}, \Phi^{(\ell)})$, mixing weights $\lambda^{(\ell)}$, shift $\eta^{(\ell)}$, and optionally lateral mixing parameters $(\tau^{(\ell)}, \omega^{(\ell)})$. The final output block $T^{(4)}$ maps the representation $X^{(3)}$ to the final $m$-dimensional output $\mathbf{y}$ without subsequent summation.

\subsection{Illustrative expansions (scalar output)}
To further clarify the compositional structure for the scalar output case ($m=1$), we write out the full expansions for networks with $L=1, 2, 3$ hidden layers, omitting optional residual connections and normalization operators for readability. For readability, we omit optional add-ons such as cyclic residual terms and normalization; when enabled, these appear additively (e.g., each block output includes an extra $R^{(\ell)}(\cdot)_q$ term as in \eqref{eq:SN}).

\subsubsection{Single hidden layer ($L=1$)}\label{sec:single_layer}
For a network with architecture $d_{\mathrm{in}}\to[d_1]\to1$ (i.e., $d_0=d_{\mathrm{in}}$), the network computes:
$$f(\mathbf{x}) = \sum_{q=0}^{d_1-1} \mathbf{h}^{(1)}_q = \sum_{q=0}^{d_1-1} \Phi^{(1)}\Biggl(\sum_{i=1}^{d_0} \lambda^{(1)}_{i}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,q\Bigr)+\alpha q + \tau^{(1)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(1)} s_j^{(1)}\Biggr).$$
When lateral mixing is disabled ($\tau^{(1)} = 0$ or $\mathcal{N}(q) = \emptyset$), this reduces to Sprecher's 1965 construction if we choose $d_1=2d_0+1$, identify $\phi^{(1)}=\phi$, $\Phi^{(1)}=\Phi$, $\lambda^{(1)}_{i} = \lambda_i$, and set $\alpha = 1$.

\subsubsection{Two hidden layers ($L=2$)}
Let the architecture be $d_0\to[d_1, d_2]\to1$. The intermediate output $\mathbf{h}^{(1)} \in \mathbb{R}^{d_1}$ is computed as:
$$\mathbf{h}^{(1)}_r=\Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+\alpha r + \tau^{(1)} \sum_{j \in \mathcal{N}(r)} \omega_{r,j}^{(1)} s_j^{(1)}\Bigr),\quad r=0,\dots,d_1-1.$$
The second block computes $\mathbf{h}^{(2)} \in \mathbb{R}^{d_2}$ using $\mathbf{h}^{(1)}$ as input:
$$\mathbf{h}^{(2)}_q=\Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r}\,\phi^{(2)}\Bigl(\mathbf{h}^{(1)}_r+\eta^{(2)}\,q\Bigr)+\alpha q + \tau^{(2)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(2)} s_j^{(2)}\Bigr),\quad q=0,\dots,d_2-1.$$
The final network output is the sum over the components of $\mathbf{h}^{(2)}$: $f(\mathbf{x})=\sum_{q=0}^{d_2-1}\mathbf{h}^{(2)}_q$. 

For the base architecture without optional enhancements, the fully expanded form reveals the nested compositional structure:
\begin{equation}\label{eq:two-layer-expanded}
f(\mathbf{x})=\sum_{q=0}^{d_2-1}\Phi^{(2)}\Biggl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r}\,\phi^{(2)}\Biggl(\underbrace{\Phi^{(1)}\Biggl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+\alpha r\Biggr)}_{\text{Output of first block}}+\eta^{(2)}\,q\Biggr)+\alpha q\Biggr).
\end{equation}
This nested structure, where the output of $\Phi^{(1)}$ becomes the input to $\phi^{(2)}$, fundamentally departs from Sprecher's original construction and represents our empirically-motivated deep architecture.

\begin{remark}[Lateral mixing in expanded form]
When lateral mixing is enabled, the expanded forms become more complex. For instance, in the two-layer case, each $s_r^{(1)}$ term undergoes mixing before the $\Phi^{(1)}$ transformation, and similarly for $s_q^{(2)}$ before $\Phi^{(2)}$. We omit the full expansion for clarity, but note that lateral mixing represents an additional source of inter-dimensional coupling beyond the shifts, potentially enhancing the network's ability to capture correlations between output dimensions.
\end{remark}

\subsubsection{Three hidden layers ($L=3$)}
Let the architecture be $d_0\to [d_1, d_2, d_3]\to1$. The recursive definition involves:
$$\begin{aligned}
\mathbf{h}^{(1)}_r &= \Phi^{(1)}\Bigl(\sum_{i=1}^{d_0}\lambda^{(1)}_{i}\,\phi^{(1)}\Bigl(x_i+\eta^{(1)}\,r\Bigr)+\alpha r + \tau^{(1)} \sum_{j \in \mathcal{N}(r)} \omega_{r,j}^{(1)} s_j^{(1)}\Bigr),\quad r=0,\dots,d_1-1,\\[1mm]
\mathbf{h}^{(2)}_s &= \Phi^{(2)}\Bigl(\sum_{r=0}^{d_1-1}\lambda^{(2)}_{r}\,\phi^{(2)}\Bigl(\mathbf{h}^{(1)}_r+\eta^{(2)}\,s\Bigr)+\alpha s + \tau^{(2)} \sum_{j \in \mathcal{N}(s)} \omega_{s,j}^{(2)} s_j^{(2)}\Bigr),\quad s=0,\dots,d_2-1,\\[1mm]
\mathbf{h}^{(3)}_q &= \Phi^{(3)}\Bigl(\sum_{s=0}^{d_2-1}\lambda^{(3)}_{s}\,\phi^{(3)}\Bigl(\mathbf{h}^{(2)}_s+\eta^{(3)}\,q\Bigr)+\alpha q + \tau^{(3)} \sum_{j \in \mathcal{N}(q)} \omega_{q,j}^{(3)} s_j^{(3)}\Bigr),\quad q=0,\dots,d_3-1.
\end{aligned}$$
The network output is $f(\mathbf{x})=\sum_{q=0}^{d_3-1}\mathbf{h}^{(3)}_q$. 

\begin{remark}
These expansions highlight the compositional nature where the output of one Sprecher block, which is a vector of transformed values, serves as the input to the next. Each transformation layer involves its own pair of shared splines, learnable parameters, and optionally lateral mixing connections.
\end{remark}

\begin{remark}[Necessity of internal shifts]\label{rem:shiftneeded}
It is tempting to simplify the nested structures, for instance by removing the inner shift terms like $\eta^{(2)}q$ inside $\phi^{(2)}$. One might hypothesize that the outer splines $\Phi^{(\ell)}$ could absorb this shifting effect (yielding a single composite spline per Sprecher block). However, experiments suggest that these internal shifts $\eta^{(\ell)}q$ applied to the inputs of the $\phi^{(\ell)}$ splines are crucial for the effective functioning of deeper Sprecher Networks. Removing them significantly degrades performance. The precise theoretical reason for their necessity in the multi-layer case, beyond their presence in Sprecher's original single-layer formula, warrants further investigation.
\end{remark}

\section{Comparison with related architectures}
To position Sprecher Networks accurately, we compare their core architectural features with Multi-Layer Perceptrons (MLPs), networks with learnable node activations (LANs/Adaptive-MLPs), and Kolmogorov-Arnold Networks (KANs).

\begin{table}[ht]
\centering
\caption{Architectural comparison of neural network families.}
\label{tab:arch_comparison}
\small % Use smaller font size if needed
\begin{tabular}{@{}lllll@{}}
\toprule
Feature                   & MLP                 & LAN / Adaptive-MLP & KAN                   & Sprecher Network (SN) \\ \midrule
\textbf{Learnable}        & Linear Weights      & Linear Weights     & Edge Splines          & Block Splines ($\phi, \Phi$) \\
\textbf{Components}       & (on edges)          & + Node Activations &                       & + Mixing Weights ($\lambda$) \\
                          &                     &                    &                       & + Shift Parameter ($\eta$) \\
                          &                     &                    &                       & + Lateral Mixing ($\tau, \omega$) \\
\textbf{Fixed components}   & Node Activations    & ---                & Node Summation        & Node Summation (implicit) \\
                          &                     &                    &                       & + Fixed Shifts ($+\alpha q$) \\
\textbf{Location of}      & Nodes               & Nodes              & Edges                 & Blocks \\
\textbf{Non-linearity}    & (Fixed)             & (Learnable)        & (Learnable)           & (Shared, Learnable) \\
\textbf{Node operation}     & Apply $\sigma(\cdot)$ & Apply $\sigma_{\text{learn}}(\cdot)$ & $\sum (\text{inputs})$ & Implicit in Block Formula \\
\textbf{Parameter sharing}  & None (typically)    & Activations? (Maybe) & None (typically)      & Splines ($\phi, \Phi$) per block \\
\textbf{Intra-layer mixing} & None & None & None & Lateral (cyclic/bidir.) \\
                           &      &      &      & $O(N)$ params \\
\textbf{Residual design}    & Matrix projection   & Matrix projection  & None (typically)      & Cyclic \\
                          & $O(N^2)$ per skip   & $O(N^2)$ per skip  &                       & $O(N)$ per skip \\
\textbf{Theoretical basis}  & UAT                 & UAT                & KAT (inspired)        & KAS (Sprecher, direct) \\
\textbf{Param scaling}    & $O(L N^2)$          & $O(L N^2 + L N G)$ & $O(L N^2 G)$          & $O(L N + L G)$ \\
                          &                     & (Approx.)          &                       & (Approx.) \\
\textbf{Memory complexity} & $O(L N^2)$          & $O(L N^2)$         & $O(L N^2 G)$          & $O(L N^2)$ parallel \\
                          &                     &                    &                       & $\mathbf{O(L N + L G)}$ sequential \\
\bottomrule
\end{tabular}
\vspace{2mm}
\parbox{\textwidth}{\footnotesize \textit{Notes:} $L$=depth, $N$=average width, $G$=spline grid size/complexity. UAT=Universal Approx. Theorem, KAT=Kolmogorov-Arnold Theorem, KAS=Kolmogorov-Arnold-Sprecher. LAN details often follow KAN Appendix B \cite{liu2024kan}. Forward working memory refers to peak \emph{forward-pass} intermediate storage plus parameters (excluding optimizer state), treating batch size as a constant; during training, reverse-mode autodiff may store additional activations unless checkpointing is used. The ``sequential'' mode for SNs processes output dimensions iteratively rather than in parallel, maintaining mathematical equivalence while reducing memory usage. All architectures may optionally include normalization layers; SNs apply normalization to entire block outputs rather than individual activations. For SNs, the $O(LN+LG)$ parameter scaling assumes the default cyclic/node residual; using a full linear residual projection adds an additional $O(\sum_{\ell} d_{\ell-1}d_{\ell})$ parameters.
\textit{The parameter scaling notation uses $N$ to denote a typical or average layer width for simplicity, following \cite{liu2024kan}. For architectures with varying widths $d_\ell$, the $LN^2$ terms should be understood as $\sum_{\ell} d_{\ell-1}d_{\ell}$ (MLP, LAN), the $LN^2G$ term for KAN as $(\sum_{\ell} d_{\ell-1}d_{\ell})G$, and the $LN$ term for SN as $\sum_{\ell} (d_{\ell-1}+d_{\ell})$ (since SN uses weight vectors, not matrices), where the sum is over the relevant blocks/layers, for precise counts.}}
\end{table}

Table \ref{tab:arch_comparison} summarizes the key distinctions between these architectures. MLPs learn edge weights with fixed node activations, LANs add learnable node activations to this structure, KANs move learnability entirely to edge splines while eliminating linear weights, and SNs concentrate learnability in shared block-level splines, block-level shifts, mixing weights, and optionally lateral mixing connections. The critical difference for SNs is their use of weight vectors rather than matrices, which fundamentally reduces the dependence on width from quadratic to linear. This architectural choice can be understood through an analogy with convolutional neural networks: just as CNNs achieve parameter efficiency and improved generalization by sharing weights across spatial locations, SNs share weights across output dimensions within each block. In CNNs, spatial shifts provide the necessary diversity despite weight sharing; in SNs, the shifts $\eta q$ and the additive term $+\alpha q$, along with lateral mixing when enabled, play this diversifying role. This perspective reframes our architectural constraint not as a limitation, but as a principled form of weight sharing motivated by Sprecher's theorem, suggesting that SNs might be viewed as a ``convolutional'' approach to function approximation networks. While this weight sharing is theoretically justified for single-layer networks, its effectiveness in deep compositions remains an empirical finding that warrants further theoretical investigation. This combination of choices leads to SNs' distinctive parameter scaling of $O(L N + L G)$ compared to KANs' $O(L N^2 G)$ and, for fixed spline resolution $G$, parameter-memory scaling of $O(LN)$.\footnote{The $O(LN)$ term includes contributions from input weights ($\lambda$), normalization parameters when used, lateral mixing parameters, and (when used) \emph{cyclic} residual connections. Cyclic residuals contribute at most $\max(d_{\mathrm{in}}, d_{\mathrm{out}})$ parameters per block; a learned linear residual projection instead adds $d_{\mathrm{in}}d_{\mathrm{out}}$ parameters when $d_{\mathrm{in}}\neq d_{\mathrm{out}}$ (and only a single scalar gate when $d_{\mathrm{in}}=d_{\mathrm{out}}$), so it is quadratic in width only on dimension-changing blocks.}

Here, we provide a precise comparison between LANs and SNs. While the following proposition shows that SNs can be expressed as special cases of LANs with specific structural constraints, it is important to note that Sprecher's construction guarantees that this constrained form retains full expressivity in the single-layer case. This suggests that the extreme parameter sharing and structural constraints in SNs may serve as a beneficial inductive bias rather than a limitation.

\begin{definition}
A LAN is an MLP with learnable activation. More precisely, the model is defined as: 
$$
f(\mathbf{x}) = A^{(L)} \circ \sigma^{(L-1)} \circ A^{(L-1)} \circ \sigma^{(L-2)} \circ \cdots \circ \sigma^{(1)} \circ A^{(1)} (\mathbf{x}),
$$
where $A^{(k)} \colon \mathbb{R}^{d_{k-1}} \rightarrow \mathbb{R}^{d_k}$ is an affine map, and $\sigma^{(k)} \colon \mathbb{R} \rightarrow \mathbb{R}  $ is the activation function (applied coordinate-wise). In an MLP, the trainable parameters are the weights $W^{(k)}$ and biases $b^{(k)}$ of $A^{(k)}(\mathbf{x}) = W^{(k)}\mathbf{x} + b^{(k)}$ for $k = 1, \ldots, L$. In a LAN, $\sigma$ contains additional trainable parameters, e.g., the coefficients of a spline.
\end{definition}

\begin{remark}[Note on weight structure]
The following proposition uses matrix weights $\lambda_{i,q}^{(\ell)}$ to establish the connection between SNs and LANs. However, the architecture we propose and analyze throughout this paper uses vector weights $\lambda_i^{(\ell)}$ (following Sprecher's original formulation), where the same weights are shared across all output dimensions. This vector version represents an even more constrained special case of the LAN formulation shown below. The practical SN architecture with vector weights corresponds to the constraint that these matrix weights are constant across the output index, i.e., $\lambda_{i,q}^{(\ell)} = \lambda_i^{(\ell)}$ for all $q$. This constraint is fundamental to maintaining the "true Sprecher" structure and achieving the characteristic $O(LN)$ parameter scaling. Additionally, when lateral mixing is enabled, the structure becomes more complex, requiring additional terms in the LAN representation to capture the intra-block communication.
\end{remark}

\begin{prop} \label{prop:SNareLAN}
    A matrix-weighted variant of a Sprecher Network (i.e., with per-output mixing weights $\lambda^{(\ell)}_{i,q}$ in each block) and with lateral mixing disabled is a LAN, where:
    \begin{itemize}
        \item in odd layers $k = 2\ell - 1$, the weight matrix $W^{(k)} \in \mathbb{R}^{d_\ell d_{\ell-1} \times d_{\ell-1}}$ is fixed to  $[I | \cdots | I]^\top$, where $I$ is the $d_{\ell-1} \times d_{\ell-1}$ identity matrix, the bias vector has only one learnable parameter $\eta^{(\ell)}$ and is structured as $b^{(k)} = \eta^{(\ell)}(0,\ldots,0, 1, \ldots, 1, \ldots, d_{\ell}-1, \ldots, d_{\ell}-1)^\top \in \mathbb{R}^{d_\ell d_{\ell-1}}$, and the activation is $\sigma^{(k)} = \phi^{(\ell)}$, 
        \item in even layers $k = 2\ell $, the learnable weight matrix $W^{(k)} \in \mathbb{R}^{d_\ell \times d_\ell d_{\ell-1}}$ is structured as
        \begin{align*}
            \begin{bmatrix}
                \lambda_{1,0}^{(\ell)} & \cdots & \lambda_{d_{\ell-1},0}^{(\ell)} & 0 &&& \cdots &&&0 \\
               0  &\cdots & 0 & \lambda_{1,1}^{(\ell)} & \cdots & \lambda_{d_{\ell-1},1}^{(\ell)} & 0 && \cdots & 0 \\
               &&&&&& \ddots \\
               0 &&&\cdots &&&0&\lambda_{1,d_\ell-1}^{(\ell)} & \cdots & \lambda_{d_{\ell-1},d_\ell-1}^{(\ell)}
            \end{bmatrix},
        \end{align*} 
        the bias is fixed to $b^{(k)} = \alpha(0,\ldots, d_\ell-1)^\top \in \mathbb{R}^{d_\ell}$, and the activation is $\sigma^{(k)} = \Phi^{(\ell)}$. 
    \end{itemize}
\end{prop}
\begin{proof}
    Follows immediately by inspecting \eqref{eq:SN} with $\tau^{(\ell)} = 0$ and $R^{(\ell)}\equiv 0$ (no lateral mixing or residual path).
\end{proof}

\begin{remark}[Understanding the LAN representation]
The representation of SNs as LANs in Proposition~\ref{prop:SNareLAN} uses an expanded 
intermediate state space. Each Sprecher block is decomposed into two LAN layers:
\begin{itemize}
\item The first layer expands the input $\mathbf{h}^{(\ell-1)} \in \mathbb{R}^{d_{\ell-1}}$ 
to $\mathbb{R}^{d_\ell d_{\ell-1}}$ by creating $d_\ell$ shifted copies, where the 
$(q \cdot d_{\ell-1} + i)$-th component contains $\phi^{(\ell)}(h^{(\ell-1)}_i + \eta^{(\ell)} q)$.
\item The second layer applies the (tied) mixing weights $\lambda^{(\ell)}_{i}$ (shared across all $q$) to select and sum 
the appropriate components for each output $q$, adds the shift $\alpha q$, and applies $\Phi^{(\ell)}$.
\end{itemize}
This construction shows that while SNs can be expressed within the LAN framework, they 
represent a highly structured special case with specific weight patterns and an expanded 
intermediate dimension of $O(d_{\ell-1} d_\ell)$ between each pair of SN layers. The inclusion of lateral mixing would further complicate this representation, requiring additional structure to capture the intra-block communication.
\end{remark}

While this proposition shows that SNs are special cases of LANs with specific structural constraints, Sprecher's theorem guarantees that \emph{in the single-layer setting} this highly structured shift-and-sum form is still universal on $[0,1]^n$. In Sprecher's original construction, $\eta$ can be chosen as a universal constant rather than a learnable parameter. Thus, universality can already hold in the presence of strongly structured, index-dependent bias patterns; Proposition \ref{prop:SNareLAN} makes this connection explicit by exhibiting one concrete structured realization.

\begin{figure}[h!]
    \centering  
\begin{tikzcd}[ampersand replacement=\&]
    \& \textnormal{LAN} \\
    \textnormal{MLP} \&\& \textnormal{SN}
    \arrow[hook, from=2-1, to=1-2]
    \arrow[hook', from=2-3, to=1-2]
\end{tikzcd}
    \caption{Diagram illustrating the dependencies between the models, in terms of learnable parameters. MLPs are LANs with fixed activation function, while SNs are LANs with a particular parameter structure (Proposition \ref{prop:SNareLAN}).}
    \label{fig:model_dependencies}
\end{figure}

\begin{remark}[Domain considerations]
The above proposition assumes that the spline functions $\phi^{(\ell)}$ can handle inputs outside their original domain $[0,1]$, which may arise due to the shifts $\eta^{(\ell)}q$. While Sprecher's theoretical construction extends $\phi$ appropriately, practical implementations typically compute exact theoretical bounds for all spline domains based on the network structure, and use these bounds to update spline knot ranges; any remaining out-of-range queries are handled by the splines' explicit extension/extrapolation rules (constant extension for $\phi^{(\ell)}$ and linear extrapolation for $\Phi^{(\ell)}$).
\end{remark}

\section{Parameter counting and efficiency as a trade-off}
A key consequence of adhering to the vector-based weighting scheme inspired by Sprecher's formula is a dramatic reduction in parameters compared to standard architectures. This represents a strong architectural constraint that may serve as either a beneficial inductive bias or a limitation, depending on the target function class. The specific design of SNs, particularly the sharing of splines and use of weight vectors rather than matrices, leads to a distinctive parameter scaling that warrants careful analysis.

Let's assume a network of depth $L$ (meaning $L$ hidden layers, thus $L$ blocks for scalar output or $L+1$ for vector output), with an average layer width $N$. We denote the input dimension as $N_{in}$ when it differs significantly from the hidden layer widths. Let $G$ be the number of knot locations (equivalently, the number of learnable coefficients) used to parameterize each one-dimensional spline; a piecewise-linear spline then has $G-1$ intervals. For simplicity, we approximate the number of parameters per spline as $O(G)$.

The parameter counts for different architectures scale as follows. MLPs primarily consist of linear weight matrices, leading to a total parameter count dominated by these weights, scaling as $O(L N^2)$. LANs (Adaptive-MLPs) have both linear weights ($O(L N^2)$) and learnable activations; if each of the $N$ nodes per layer has a learnable spline, this adds $O(L N G)$ parameters, for a total of $O(L N^2 + L N G)$. KANs replace linear weights with learnable edge splines, with $O(N^2)$ edges between layers. If each edge has a spline with $O(G)$ parameters, the total count per layer is $O(N^2 G)$, leading to an overall scaling of $O(L N^2 G)$.

Sprecher Networks have a fundamentally different structure. Each block $\ell$ contains mixing weights $\lambda^{(\ell)}$ with $O(d_{\ell-1})$ parameters, where $d_{\ell-1}$ is the input dimension to that block (crucially, these are vectors, not matrices), shared splines $\phi^{(\ell)}, \Phi^{(\ell)}$ with $2 \times O(G) = O(G)$ parameters per block independent of $N$, and a shift parameter $\eta^{(\ell)}$ with $O(1)$ parameter per block. When lateral mixing is enabled, each block additionally contains a lateral scale parameter $\tau^{(\ell)}$ with $O(1)$ parameter and mixing weights $\omega^{(\ell)}$ with $O(d_\ell)$ parameters (specifically $d_\ell$ for cyclic, $2d_\ell$ for bidirectional). When normalization is used, each block may have an associated normalization layer adding $O(N)$ parameters (specifically $2N$ for the affine transformation). When residual connections are used, cyclic residuals add at most $O(N)$ parameters per block (specifically $\max(d_{\ell-1}, d_\ell)$ for block $\ell$, or just 1 when dimensions match); a learned linear residual projection instead adds $O(d_{\ell-1}d_\ell)$ parameters when $d_{\ell-1}\neq d_\ell$ (and just 1 when $d_{\ell-1}=d_\ell$). Summing over $L$ (or $L+1$) blocks, the total parameter count scales approximately as $O(LN + LG + L)$, where the linear dependence on $N$ now includes mixing weights, optional normalization parameters, lateral mixing weights, and residual connections.

Beyond parameter efficiency, SNs also admit memory-efficient computation strategies. While standard vectorized implementations materialize a $(B \cdot d_{\mathrm{in}} \cdot d_{\mathrm{out}})$ tensor of shifted inputs, a sequential evaluation strategy avoids this materialization and reduces the largest temporary forward tensor to $O(B \cdot \max(d_{\mathrm{in}}, d_{\mathrm{out}}))$ (Section~\ref{sec:memory_efficient}). In wide settings where $d_{\mathrm{in}}\approx d_{\mathrm{out}}\approx N$, this yields a peak temporary forward footprint that scales linearly in $N$ rather than quadratically. (As usual, achieving this scaling during training may require checkpointing/recomputation, since the backward pass can retain additional intermediates.)

This scaling reveals the crucial trade-off: SNs achieve a reduction from $O(LN^2)$ to $O(LN)$ in parameter count (and corresponding parameter memory) by using weight vectors rather than matrices; with sequential evaluation (Section~\ref{sec:memory_efficient}) they also avoid explicitly materializing the $B\times d_{\mathrm{in}}\times d_{\mathrm{out}}$ shifted-input tensor in the forward pass. Additionally, in KANs, the spline complexity $G$ multiplies the $N^2$ term ($O(L N^2 G)$), while in SNs, due to spline sharing within blocks, it appears as an additive term ($O(L G)$). This suggests potential for significant parameter and memory savings, particularly when high spline resolution ($G$) is required for accuracy or when the layer width ($N$) is moderate to large.

This extreme parameter sharing represents a fundamental architectural bet: that the structure imposed by Sprecher's theorem, using only weight vectors with output diversity through shifts and lateral mixing, provides a beneficial inductive bias that outweighs the reduction in parameter flexibility. The empirical observation that training remains feasible (albeit sometimes requiring more iterations) with vector weights suggests this bet may be justified for certain function classes. Moreover, viewing this constraint through the lens of weight sharing in CNNs provides a new perspective: both architectures sacrifice parameter flexibility for a structured representation that has proven effective in practice, though for SNs the theoretical justification comes from Sprecher's theorem rather than domain-specific intuitions about spatial invariance. The addition of lateral mixing provides a middle ground, allowing some cross-dimensional communication while maintaining the overall parameter efficiency. Whether this constraint serves as beneficial regularization or harmful limitation likely depends on the specific problem domain and the alignment between the target function's structure and the inductive bias imposed by the SN architecture.

\subsection{Illustrative parameter comparisons (hypothetical examples)}
We present parameter count comparisons based on architectures reported in \cite{liu2024kan} to illustrate the potential parameter efficiency of SNs. Note that these examples are hypothetical; while the KAN architectures are from their paper, the SN results are theoretical projections that require empirical validation. Optimal architectures for SNs may differ from those of KANs due to the shared spline structure.

\paragraph{PDE solving example (KAN Ref § 3.4):}
KAN architecture $[2, 10, 1]$, reported effective. $N_{in}=2, N_{hid}=10, N_{out}=1$.
\begin{itemize}
    \item \textbf{KAN (est.):} $(2\times10 + 10\times1) = 30$ edges/splines. KANs use B-splines with $G_{\mathrm{KAN}}=20$ intervals, $k=3$ ($G_{\mathrm{KAN}}+k \approx 23$ params/spline). Total KAN params $\approx 30 \times 23 = 690$.
    \item \textbf{SN (hypothetical):} Architecture $2 \to [10] \to 1$. $L=1$ hidden layer, scalar output means $L=1$ block total. Using piecewise linear splines with $G=21$ knots (20 intervals): $G=21$ params/spline. Shared splines: $2L \times G = 2 \times 21 = 42$. Mixing weights: $d_0 = 2$ (vector weights). Shift $\eta^{(1)}$: $1$. If lateral mixing enabled (cyclic): $\tau^{(1)}$: $1$, $\omega^{(1)}$: $10$. Total SN params $\approx 42 + 2 + 1 + 1 + 10 = 56$ (with lateral mixing) or $45$ (without).
    \item \textbf{Potential advantage:} For equivalent structure, theoretical reduction factor $\approx 12-15$x. Actual performance requires empirical validation.
\end{itemize}

\paragraph{Knot theory example (KAN Ref § 4.3):}
Pruned KAN $[17, 1, 14]$, with B-splines using $G_{\mathrm{KAN}}=3$ intervals, $k=3$ ($G_{\mathrm{KAN}}+k \approx 6$ params/spline).
\begin{itemize}
    \item \textbf{KAN (est.):} $(17\times1 + 1\times14) = 31$ edges/splines. Total KAN params $\approx 31 \times 6 = 186$.
    \item \textbf{SN (hypothetical):} Architecture $17 \to [1] \to 14$. $L=1$ hidden layer, vector output means $L+1=2$ blocks total. Using piecewise linear splines with $G=4$ knots (3 intervals): $2(L+1) \times G = 4 \times 4 = 16$. Mixing weights: $d_0 + d_1 = 17 + 1 = 18$ (vector weights). Shifts $\eta^{(1)}, \eta^{(2)}$: $2$. If lateral mixing enabled (cyclic) for both blocks: $\tau^{(1)}, \tau^{(2)}$: $2$, $\omega^{(1)}$: $1$, $\omega^{(2)}$: $14$. Total SN params $\approx 16 + 18 + 2 + 2 + 1 + 14 = 53$ (with lateral mixing) or $36$ (without).
    \item \textbf{Potential advantage:} Theoretical reduction factor $\approx 3.5-5$x. The narrow intermediate dimension $d_1=1$ might pose challenges for SN training.
\end{itemize}

These calculations illustrate the dramatic parameter reduction possible with SNs, but they also highlight a crucial practical consideration: the optimal architecture for SNs likely differs substantially from that of MLPs or KANs. With vector weights providing only linear scaling in width, SNs may require wider or deeper architectures to achieve comparable expressivity. The art of architecture selection for SNs involves balancing the parameter efficiency against the need for sufficient expressivity: a trade-off that remains poorly understood and likely depends strongly on the problem domain. Early empirical evidence suggests that SNs excel when the target function aligns well with their compositional, shift-based structure, but may require more iterations when forced to approximate functions that require truly independent processing of different output dimensions. The addition of lateral mixing partially addresses this limitation by enabling limited cross-dimensional communication.

\section{Theoretical aspects and open questions}\label{sec:universality}

\subsection{Relation to Sprecher (1965) and universality}
As shown in Section \ref{sec:single_layer}, a single-layer ($L=1$) Sprecher Network (SN) reduces exactly to Sprecher's constructive representation \eqref{eq:sprecher_original} when lateral mixing and residuals are disabled (i.e., $\tau=0$ and no residual connection), with the additive shift implemented by $\alpha q$ (set $\alpha=1$ to match \eqref{eq:sprecher_original}). In his 1965 paper, Sprecher proved that for any continuous function $f:[0,1]^n\to\mathbb{R}$ there exist a \emph{single} nondecreasing univariate function $\phi$, a continuous univariate function $\Phi$, real constants $\lambda_1,\ldots,\lambda_n$ and $\eta$, and an integer upper index $2n$ such that the representation \eqref{eq:sprecher_original} holds \cite{sprecher1965}. This immediately implies:

\begin{theorem}[Universality of single-layer SNs]\label{thm:ua_single_layer}
For any $n\ge 1$ and any $f\in C([0,1]^n)$, there exists a single-layer SN with $\tau=0$ and no residual connection that represents $f$ exactly in the form \eqref{eq:sprecher_original} (taking $d_{\mathrm{out}}=2n+1$ and $\alpha=1$). In particular, for every $\epsilon>0$ there is a single-layer SN (with $\tau=0$ and no residual) such that $\sup_{\mathbf{x}\in[0,1]^n}|f(\mathbf{x})-\hat f(\mathbf{x})|<\epsilon$.
\end{theorem}

Thus, single-layer SNs inherit (indeed, contain) the family given by Sprecher's representation. In practice, however, we do not fit arbitrary $C^2$ univariate functions $\phi$ and $\Phi$ exactly; we parameterize them as splines. This motivates the following finite-parameter approximation analysis, which also clarifies how optional \emph{lateral mixing} (controlled by $\tau$ and $\omega$) and cyclic residuals affect constants but not rates.

\medskip
\noindent\textbf{Notation for a single block.}
A (possibly mixed) Sprecher block $T:\mathbb{R}^{d_{\mathrm{in}}}\to\mathbb{R}^{d_{\mathrm{out}}}$ acts componentwise as
$$[T(\mathbf{z})]_q \;=\; \Phi\!\Biggl(\,\underbrace{\sum_{i=1}^{d_{\mathrm{in}}}\lambda_i\,\phi(z_i+\eta q)+\alpha q}_{=:s_q}\;+\;\tau\sum_{j\in\mathcal{N}(q)}\omega_{qj}\,s_j\Biggr),
\quad q=0,\ldots,d_{\mathrm{out}}-1,$$
where $\lambda\in\mathbb{R}^{d_{\mathrm{in}}}$, $\eta,\alpha,\tau\in\mathbb{R}$, $\mathcal{N}(q)$ specifies the (optional) neighborhood used for lateral mixing, and $\omega=(\omega_{qj})$ are the mixing weights. When a cyclic residual connection is present, the block output is $T_{\mathrm{res}}(\mathbf{z})=T(\mathbf{z})+R(\mathbf{z})$. The approximation analysis below applies to $T$; when the residual map $R$ is shared between the target block and its spline approximation, it cancels in the blockwise difference. However, Lipschitz/composition constants for the full block include an additive contribution from $R$ (e.g.\ $L_{T_{\mathrm{res}}}\le L_T+L_R$).

\begin{lemma}[Single block approximation with piecewise-linear splines]\label{lem:single_block}
Fix $B_{\mathrm{in}}>0$ and assume $\mathbf{z}\in\mathcal{B}_{\mathrm{in}}:=[-B_{\mathrm{in}},B_{\mathrm{in}}]^{d_{\mathrm{in}}}$. Let
$$I_\phi := \bigl[-B_{\mathrm{in}}-|\eta|(d_{\mathrm{out}}-1),\; B_{\mathrm{in}}+|\eta|(d_{\mathrm{out}}-1)\bigr],
\quad
M_\phi := \|\phi\|_{L^\infty(I_\phi)}.$$
Define
$$B_\omega:=\sup_{q}\sum_{j\in\mathcal{N}(q)}|\omega_{qj}|,
\qquad
R_{\mathrm{mix}}:=1+|\tau|\,B_\omega,
\qquad
M_s := \|\lambda\|_{1}\,M_\phi + |\alpha|\,(d_{\mathrm{out}}-1),$$
and set $I_\Phi := [-R_{\mathrm{mix}}M_s,\; R_{\mathrm{mix}}M_s]$. Assume $\phi,\Phi\in C^2$ on neighborhoods of $I_\phi$ and $I_\Phi$, respectively. Let $\hat\phi$ and $\hat\Phi$ be the (shape-preserving) piecewise-linear interpolants of $\phi$ on a uniform grid of $G_\phi\ge 2$ knots over $I_\phi$ and of $\Phi$ on a uniform grid of $G_\Phi\ge 2$ knots over $I_\Phi$; write
$$h_\phi:=\frac{|I_\phi|}{G_\phi-1},\quad h_\Phi:=\frac{|I_\Phi|}{G_\Phi-1},\quad
M_{\phi''}:=\|\phi''\|_{L^\infty(I_\phi)},\quad M_{\Phi''}:=\|\Phi''\|_{L^\infty(I_\Phi)}.$$
Define $\hat T$ by replacing $(\phi,\Phi)$ with $(\hat\phi,\hat\Phi)$ in $T$ (keeping the same $\lambda,\eta,\alpha,\tau,\omega,\mathcal{N}$). Then, for all $\mathbf{z}\in\mathcal{B}_{\mathrm{in}}$,
\begin{equation}\label{eq:single_block_sup}
\|T(\mathbf{z})-\hat T(\mathbf{z})\|_\infty
\;\le\;
L_\Phi\,R_{\mathrm{mix}}\,\|\lambda\|_{1}\,\delta_\phi\;+\;\delta_\Phi,
\qquad
\delta_\phi:=\frac{M_{\phi''}}{8}h_\phi^2,\ \ \delta_\Phi:=\frac{M_{\Phi''}}{8}h_\Phi^2,
\end{equation}
where $L_\Phi:=\|\Phi'\|_{L^\infty(I_\Phi)}$. Equivalently,
$$\|T(\mathbf{z})-\hat T(\mathbf{z})\|_\infty
\;\le\;
K_T\cdot \max\{h_\phi^2,h_\Phi^2\}
\quad\text{with}\quad
K_T:=\frac{1}{8}\Bigl(L_\Phi\,R_{\mathrm{mix}}\,\|\lambda\|_{1}\,M_{\phi''}+M_{\Phi''}\Bigr).$$
The same bounds hold for a block with a (shared) cyclic residual $T_{\mathrm{res}}=T+R$, since $R$ cancels when $T$ and $\hat T$ share the same residual path.
\end{lemma}

\begin{proof}
On each subinterval of the uniform grid for $I_\phi$ (resp.\ $I_\Phi$) of length $h_\phi$ (resp.\ $h_\Phi$), the classical bound for piecewise-linear interpolation of a $C^2$ function gives
$$\|\phi-\hat\phi\|_{L^\infty(I_\phi)}\le \frac{M_{\phi''}}{8}h_\phi^2=:\delta_\phi,
\qquad
\|\Phi-\hat\Phi\|_{L^\infty(I_\Phi)}\le \frac{M_{\Phi''}}{8}h_\Phi^2=:\delta_\Phi.$$
For fixed $\mathbf{z}$ and $q$, write $s_q=\sum_i\lambda_i\,\phi(z_i+\eta q)+\alpha q$ and $\hat s_q$ the analogous quantity with $\hat\phi$. Then
$$|s_q-\hat s_q|\;\le\;\sum_{i=1}^{d_{\mathrm{in}}}|\lambda_i|\,\|\phi-\hat\phi\|_{L^\infty(I_\phi)}
\;\le\;\|\lambda\|_{1}\,\delta_\phi.$$
Define the post-mixing scalars $\tilde s_q:=s_q+\tau\sum_{j\in\mathcal{N}(q)}\omega_{qj}s_j$ and $\hat{\tilde s}_q:=\hat s_q+\tau\sum_{j\in\mathcal{N}(q)}\omega_{qj}\hat s_j$. Using the triangle inequality and the definition of $B_\omega$,
$$|\tilde s_q-\hat{\tilde s}_q|
\;\le\;
|s_q-\hat s_q| + |\tau| \sum_{j\in\mathcal{N}(q)}|\omega_{qj}|\,|s_j-\hat s_j|
\;\le\; R_{\mathrm{mix}}\cdot \sup_{j}|s_j-\hat s_j|
\;\le\; R_{\mathrm{mix}}\,\|\lambda\|_{1}\,\delta_\phi.$$
Next, since a linear interpolant lies in the convex hull of the function values at the grid endpoints, we have
$$\|\hat\phi\|_{L^\infty(I_\phi)} \;\le\; \|\phi\|_{L^\infty(I_\phi)}=M_\phi.$$
Hence $|s_j|\le M_s$ and $|\hat s_j|\le M_s$, which implies $|\tilde s_q|\le R_{\mathrm{mix}}M_s$ and $|\hat{\tilde s}_q|\le R_{\mathrm{mix}}M_s$. Therefore $\tilde s_q,\hat{\tilde s}_q\in I_\Phi$, and
$$|T(\mathbf{z})_q-\hat T(\mathbf{z})_q|
=\bigl|\Phi(\tilde s_q)-\hat\Phi(\hat{\tilde s}_q)\bigr|
\le \underbrace{|\Phi(\tilde s_q)-\Phi(\hat{\tilde s}_q)|}_{\le L_\Phi |\tilde s_q-\hat{\tilde s}_q|}
+ \underbrace{|\Phi(\hat{\tilde s}_q)-\hat\Phi(\hat{\tilde s}_q)|}_{\le \delta_\Phi}.$$
Combining the bounds and taking the maximum over $q$ yields \eqref{eq:single_block_sup}. If a residual path $R$ is present and shared between $T$ and $\hat T$, then $T_{\mathrm{res}}-\hat T_{\mathrm{res}}=(T-\hat T)+(R-R)=(T-\hat T)$, so the same inequalities apply.
\end{proof}

\begin{lemma}[Lipschitz constant of a Sprecher block]\label{lem:block_lip}
Assume $\phi$ and $\Phi$ are Lipschitz on $I_\phi$ and $I_\Phi$ with constants $L_\phi:=\|\phi'\|_{L^\infty(I_\phi)}$ and $L_\Phi:=\|\Phi'\|_{L^\infty(I_\Phi)}$, respectively. Then $T$ is $L_T$-Lipschitz on $\mathcal{B}_{\mathrm{in}}$ in the $\ell_\infty$ norm with
$$L_T \;\le\; L_\Phi\,R_{\mathrm{mix}}\,\|\lambda\|_1\,L_\phi,$$
where $R_{\mathrm{mix}}=1+|\tau|\,B_\omega$ as in Lemma~\ref{lem:single_block}. If a residual path $R$ (shared between the exact and approximated networks) is present with Lipschitz constant $L_R$ on $\mathcal{B}_{\mathrm{in}}$, then $T_{\mathrm{res}}=T+R$ is $(L_T+L_R)$-Lipschitz.
\end{lemma}

\begin{proof}
For $\mathbf{z},\mathbf{z}'\in\mathcal{B}_{\mathrm{in}}$ and each $q$,
$$\bigl|s_q(\mathbf{z})-s_q(\mathbf{z}')\bigr|
\le \sum_i |\lambda_i|\,\bigl|\phi(z_i+\eta q)-\phi(z_i'+\eta q)\bigr|
\le \|\lambda\|_1\,L_\phi\,\|\mathbf{z}-\mathbf{z}'\|_\infty.$$
Therefore,
$$\bigl|\tilde s_q(\mathbf{z})-\tilde s_q(\mathbf{z}')\bigr|
\le \Bigl(1+|\tau|\,\sum_{j\in\mathcal{N}(q)}|\omega_{qj}|\Bigr)\,\|\lambda\|_1\,L_\phi\,\|\mathbf{z}-\mathbf{z}'\|_\infty
\le R_{\mathrm{mix}}\|\lambda\|_1 L_\phi\,\|\mathbf{z}-\mathbf{z}'\|_\infty,$$
and hence
$$|T(\mathbf{z})_q-T(\mathbf{z}')_q|
\le L_\Phi\,|\tilde s_q(\mathbf{z})-\tilde s_q(\mathbf{z}')|
\le L_\Phi R_{\mathrm{mix}}\|\lambda\|_1 L_\phi\,\|\mathbf{z}-\mathbf{z}'\|_\infty.$$
Taking the maximum over $q$ gives the claimed bound for $T$, and adding $L_R$ handles $T_{\mathrm{res}}$ by the triangle inequality.
\end{proof}

\begin{lemma}[Error composition]\label{lem:error_composition}
Consider an $L$-block SN (with optional lateral mixing and cyclic residuals at each block). Let $T^{(\ell)}:\mathbb{R}^{d_{\ell-1}}\to\mathbb{R}^{d_{\ell}}$ denote the $\ell$-th block map on the exact network and $\hat T^{(\ell)}$ its spline-approximated counterpart (same $\lambda,\eta,\alpha,\tau,\omega$ but $(\phi,\Phi)$ replaced by $(\hat\phi,\hat\Phi)$). Assume there exist bounded sets $\mathcal{B}_{\ell}\subset\mathbb{R}^{d_{\ell}}$ such that for all inputs $\mathbf{x}\in[0,1]^n$,
$$\mathbf{h}^{(\ell)}(\mathbf{x}) := T^{(\ell)}\!\circ\cdots\circ T^{(1)}(\mathbf{x}) \in \mathcal{B}_{\ell}
\quad\text{and}\quad
\hat{\mathbf{h}}^{(\ell)}(\mathbf{x}) := \hat T^{(\ell)}\!\circ\cdots\circ \hat T^{(1)}(\mathbf{x}) \in \mathcal{B}_{\ell},$$
and suppose each $T^{(\ell)}$ is $L_{T^{(\ell)}}$-Lipschitz on $\mathcal{B}_{\ell-1}$. If $\varepsilon_\ell:=\sup_{\mathbf{z}\in\mathcal{B}_{\ell-1}}\|T^{(\ell)}(\mathbf{z})-\hat T^{(\ell)}(\mathbf{z})\|_\infty$, then for $E_\ell:=\sup_{\mathbf{x}\in[0,1]^n}\|\mathbf{h}^{(\ell)}(\mathbf{x})-\hat{\mathbf{h}}^{(\ell)}(\mathbf{x})\|_\infty$ we have
\begin{equation}\label{eq:error_composition_result}
E_\ell \;\le\; \sum_{j=1}^{\ell}\left(\prod_{m=j+1}^{\ell} L_{T^{(m)}}\right)\varepsilon_j,
\end{equation}
with the empty product equal to $1$.
\end{lemma}

\begin{proof}
The case $\ell=1$ is $E_1\le \varepsilon_1$. Assume the claim for $\ell-1$. Then, for any $\mathbf{x}$,
\begin{align*}
\|\mathbf{h}^{(\ell)}(\mathbf{x})-\hat{\mathbf{h}}^{(\ell)}(\mathbf{x})\|_\infty
&= \|T^{(\ell)}(\mathbf{h}^{(\ell-1)}(\mathbf{x})) - \hat T^{(\ell)}(\hat{\mathbf{h}}^{(\ell-1)}(\mathbf{x}))\|_\infty\\
&\le \underbrace{\|T^{(\ell)}(\mathbf{h}^{(\ell-1)}(\mathbf{x})) - T^{(\ell)}(\hat{\mathbf{h}}^{(\ell-1)}(\mathbf{x}))\|_\infty}_{\le L_{T^{(\ell)}}\,\|\mathbf{h}^{(\ell-1)}(\mathbf{x})-\hat{\mathbf{h}}^{(\ell-1)}(\mathbf{x})\|_\infty}
+ \underbrace{\|T^{(\ell)}-\hat T^{(\ell)}\|_{\infty,\mathcal{B}_{\ell-1}}}_{\le \varepsilon_\ell}\\
&\le L_{T^{(\ell)}} E_{\ell-1} + \varepsilon_\ell,
\end{align*}
and the induction hypothesis gives \eqref{eq:error_composition_result}.
\end{proof}

\begin{corollary}[Global spline approximation rate for SNs with piecewise-linear splines]\label{cor:sprecher_rate}
Let $f:[0,1]^n\to\mathbb{R}$ be realized by an ideal $L$-block SN (possibly with lateral mixing and cyclic residuals) with scalar output. For each block $\ell=1,\dots,L$, let $B_{\mathrm{in}}^{(\ell)}:=\sup\{\|\mathbf{z}\|_\infty:\mathbf{z}\in\mathcal{B}_{\ell-1}\}$. Assume:
\begin{enumerate}[label=\textup{(\roman*)}]
\item $\phi^{(\ell)},\Phi^{(\ell)}\in C^2$ on neighborhoods of the intervals
$$I_\phi^{(\ell)} := \bigl[-B_{\mathrm{in}}^{(\ell)}-|\eta^{(\ell)}|(d_{\ell}-1),\; B_{\mathrm{in}}^{(\ell)}+|\eta^{(\ell)}|(d_{\ell}-1)\bigr],
\quad
I_\Phi^{(\ell)} := \bigl[-R_{\mathrm{mix}}^{(\ell)}M_s^{(\ell)},\; R_{\mathrm{mix}}^{(\ell)}M_s^{(\ell)}\bigr],$$
with $M_s^{(\ell)}:=\|\lambda^{(\ell)}\|_{1}\|\phi^{(\ell)}\|_{L^\infty(I_\phi^{(\ell)})}+|\alpha|(d_\ell-1)$ and $R_{\mathrm{mix}}^{(\ell)}:=1+|\tau^{(\ell)}|\,B_\omega^{(\ell)}$, $B_\omega^{(\ell)}:=\sup_q\sum_{j\in\mathcal{N}^{(\ell)}(q)}|\omega^{(\ell)}_{qj}|$;
\item the structural parameters are bounded: $\|\lambda^{(\ell)}\|_{1}\le \Lambda_1$, $|\eta^{(\ell)}|\le H$, $|\alpha|\le A$, $|\tau^{(\ell)}|\le T$, and $B_\omega^{(\ell)}\le \Omega$;
\item there exist bounded sets $\mathcal{B}_\ell$ such that the exact and approximated forward passes both remain in $\mathcal{B}_\ell$ for all inputs in $[0,1]^n$ (bounded propagation).
\end{enumerate}
Construct $\hat\phi^{(\ell)}$ and $\hat\Phi^{(\ell)}$ as the piecewise-linear interpolants on uniform grids with $G_\phi^{(\ell)}\ge2$ and $G_\Phi^{(\ell)}\ge2$ knots on $I_\phi^{(\ell)}$ and $I_\Phi^{(\ell)}$, respectively, and write $h_\phi^{(\ell)}:=|I_\phi^{(\ell)}|/(G_\phi^{(\ell)}-1)$ and $h_\Phi^{(\ell)}:=|I_\Phi^{(\ell)}|/(G_\Phi^{(\ell)}-1)$. Then, with
$$\delta_\phi^{(\ell)}:=\frac{\|\phi^{(\ell)''}\|_{L^\infty(I_\phi^{(\ell)})}}{8}\,\bigl(h_\phi^{(\ell)}\bigr)^2, \qquad
\delta_\Phi^{(\ell)}:=\frac{\|\Phi^{(\ell)''}\|_{L^\infty(I_\Phi^{(\ell)})}}{8}\,\bigl(h_\Phi^{(\ell)}\bigr)^2,$$
the blockwise error satisfies
$$\varepsilon_\ell
:=\sup_{\mathbf{z}\in\mathcal{B}_{\ell-1}}\|T^{(\ell)}(\mathbf{z})-\hat T^{(\ell)}(\mathbf{z})\|_\infty
\ \le\ L_{\Phi}^{(\ell)}\,R_{\mathrm{mix}}^{(\ell)}\,\|\lambda^{(\ell)}\|_{1}\,\delta_\phi^{(\ell)}\;+\;\delta_\Phi^{(\ell)},$$
where $L_{\Phi}^{(\ell)}:=\|\Phi^{(\ell)'}\|_{L^\infty(I_\Phi^{(\ell)})}$. Consequently, by Lemma~\ref{lem:error_composition},
$$\sup_{\mathbf{x}\in[0,1]^n}|f(\mathbf{x})-\hat f(\mathbf{x})|
\;\le\;
\sum_{j=1}^{L}\left(\prod_{m=j+1}^{L}L_{T^{(m)}}\right)\Bigl(L_{\Phi}^{(j)}\,R_{\mathrm{mix}}^{(j)}\,\|\lambda^{(j)}\|_{1}\,\delta_\phi^{(j)}+\delta_\Phi^{(j)}\Bigr),$$
with $L_{T^{(m)}}$ any Lipschitz constant of the $m$-th block on $\mathcal{B}_{m-1}$ (e.g.\ from Lemma~\ref{lem:block_lip}, plus the residual-path constant when present). In particular, if $G_\phi^{(\ell)}=G_\Phi^{(\ell)}=G$ for all $\ell$, then with $h:=\max_\ell\{|I_\phi^{(\ell)}|,|I_\Phi^{(\ell)}|\}/(G-1)$ we obtain
$$\sup_{\mathbf{x}\in[0,1]^n}|f(\mathbf{x})-\hat f(\mathbf{x})|
\;=\; \mathcal{O}\!\left(h^2\right)
\;=\; \mathcal{O}\!\left(G^{-2}\right),$$
with constants depending on $\{\|\lambda^{(\ell)}\|_{1},L_{\phi^{(\ell)}},L_{\Phi}^{(\ell)},R_{\mathrm{mix}}^{(\ell)},\|\phi^{(\ell)''}\|_\infty,\|\Phi^{(\ell)''}\|_\infty,L_{T^{(\ell)}}\}_{\ell=1}^L$ and on the bounded-propagation sets $\{\mathcal{B}_\ell\}$.
\end{corollary}

\begin{remark}[Impact of lateral mixing and cyclic residuals]
Lateral mixing appears only inside the argument of $\Phi$ and inflates constants via $R_{\mathrm{mix}}^{(\ell)}=1+|\tau^{(\ell)}|\,B_\omega^{(\ell)}$ (with $B_\omega^{(\ell)}\le \|\omega^{(\ell)}\|_\infty N_{\max}^{(\ell)}$); it does not change the $G^{-2}$ rate. If a cyclic residual path $R$ is present and shared between $T^{(\ell)}$ and $\hat T^{(\ell)}$, it cancels in the blockwise difference and thus does not affect $\varepsilon_\ell$, but it \emph{does} contribute additively to $L_{T^{(\ell)}}$ in \eqref{eq:error_composition_result}. For example, when $d_{\mathrm{in}}=d_{\mathrm{out}}$ and $R(\mathbf{z})=w_{\mathrm{res}}\mathbf{z}$, we may take $L_{T^{(\ell)}}\le L_{\Phi}^{(\ell)}\,R_{\mathrm{mix}}^{(\ell)}\,\|\lambda^{(\ell)}\|_{1}\,L_{\phi^{(\ell)}} + |w_{\mathrm{res}}|$, with $L_{\phi^{(\ell)}}:=\|\phi^{(\ell)'}\|_{L^\infty(I_\phi^{(\ell)})}$. Analogous bounds hold for broadcast/pooling residuals using the operator norm of the corresponding linear map.
\end{remark}

\begin{remark}[Monotone spline parameterization]
Sprecher's construction requires the inner map to be nondecreasing. The piecewise-linear interpolant of a $C^2$ nondecreasing function on a uniform grid is itself nondecreasing (no oscillatory overshoot), and the $O(h^2)$ error bound used above holds unchanged. Hence the analysis is compatible with the monotone-spline parameterizations of $\phi$ used in this work.
\end{remark}

\begin{remark}[Depth dependence]
The $\mathcal{O}(G^{-2})$ \emph{rate} is dimension-free, while constants accumulate with depth through $\prod_{m=j+1}^{L}L_{T^{(m)}}$ as in \eqref{eq:error_composition_result}. This mirrors standard error-propagation phenomena in deep models and highlights the practical value of regularizing $\|\lambda^{(\ell)}\|_{1}$, $L_{\phi^{(\ell)}}$, and $L_{\Phi^{(\ell)}}$, and of controlling lateral mixing ($\tau^{(\ell)},\omega^{(\ell)}$).
\end{remark}

\subsection{Vector-valued functions and deeper extensions}
For vector-valued functions $f: [0,1]^n \to \mathbb{R}^m$ with $m>1$, our construction appends an $(L+1)$-th block without final summation. While intuitively extending the representation, the universality of this specific construction is not directly covered by Sprecher's original theorem. The composition of multiple Sprecher blocks to create deep networks represents a natural but theoretically uncharted extension of Sprecher's construction. While single-layer universality is guaranteed, the expressive power of deep SNs remains an open question with several competing hypotheses. Depth might provide benefits analogous to those in standard neural networks: enabling more efficient representation of compositional functions, creating a more favorable optimization landscape despite the constrained parameter space, or allowing the network to gradually transform inputs into representations that are progressively easier to process. The addition of lateral mixing connections may further enhance these benefits by enabling richer transformations at each layer. Alternatively, the specific constraints of the SN architecture might interact with depth in unexpected ways, either amplifying the benefits of the structured representation or creating new challenges not present in single-layer networks.

\begin{conjecture}[Vector-valued Sprecher Representation]\label{conj:vector}
Let $n, m \in \mathbb{N}$ with $m > 1$, and let $f:[0,1]^n \to \mathbb{R}^m$ be any continuous function. Then for any $\epsilon > 0$, there exists a Sprecher Network with architecture $n \to [d_1] \to m$ (using $L=1$ hidden block of width $d_1 \ge 2n+1$ and one output block), with sufficiently flexible continuous splines $\phi^{(1)}, \Phi^{(1)}, \phi^{(2)}, \Phi^{(2)}$ ($\phi^{(1)}, \phi^{(2)}$ monotonic), appropriate parameters $\lambda^{(1)}, \eta^{(1)}, \lambda^{(2)}, \eta^{(2)}$, and optionally lateral mixing parameters, such that the network output $\hat{f}(\mathbf{x})$ satisfies $\sup_{\mathbf{x} \in [0,1]^n} \|f(\mathbf{x}) - \hat{f}(\mathbf{x})\|_{\mathbb{R}^m} < \epsilon$.
\end{conjecture}

Furthermore, stacking multiple Sprecher blocks ($L > 1$) creates deeper networks. It is natural to hypothesize that these deeper networks also possess universal approximation capabilities, potentially offering advantages in efficiency or learning dynamics for certain function classes, similar to depth advantages observed in MLPs. The role of lateral mixing in enhancing or modifying these universality properties remains unexplored.

\begin{conjecture}[Deep universality]\label{conj:deep_universal}
For any input dimension $n \ge 1$, any number of hidden blocks $L \ge 1$, and any continuous function $f: [0,1]^n \to \mathbb{R}$ (or $f: [0,1]^n \to \mathbb{R}^m$), and any $\epsilon > 0$, there exists a Sprecher Network with architecture $n \to [d_1, \dots, d_L] \to 1$ (or $\to m$), provided the hidden widths $d_1, \dots, d_L$ are sufficiently large (e.g., perhaps $d_\ell \ge 2 d_{\ell-1} + 1$ is sufficient, although likely not necessary), with sufficiently flexible continuous splines $\phi^{(\ell)}, \Phi^{(\ell)}$, appropriate parameters $\lambda^{(\ell)}, \eta^{(\ell)}$, and optionally lateral mixing parameters $\tau^{(\ell)}, \omega^{(\ell)}$, such that the network output $\hat{f}(\mathbf{x})$ satisfies $\sup_{\mathbf{x} \in [0,1]^n} |f(\mathbf{x}) - \hat{f}(\mathbf{x})| < \epsilon$ (or the vector norm equivalent).
\end{conjecture}

Proving Conjectures \ref{conj:vector} and \ref{conj:deep_universal} rigorously would require analyzing the compositional properties and ensuring that the range of intermediate representations covers the domain needed by subsequent blocks, potentially involving careful control over the spline ranges, the effect of the shifts $\eta^{(\ell)}$, and the impact of lateral mixing on the network's expressive power.

\section{Implementation considerations}\label{sec:implementation}

\subsection{Trainable splines}
For practical implementations, we support both piecewise-linear splines and $C^1$ cubic Hermite splines with \textsc{PCHIP} (shape-preserving) slopes for both $\phi^{(\ell)}$ and $\Phi^{(\ell)}$. Piecewise-linear splines are the default when only function values are required (high efficiency, simple evaluation); the cubic option is used when higher-order derivatives must flow through the spline (e.g., the Poisson PINN benchmark, which requires second derivatives). In both cases, a spline with $G$ uniformly spaced knot locations (thus $G-1$ intervals) is parameterized by the values at these $G$ knot locations (thus $O(G)$ parameters per spline); for the cubic case, the \textsc{PCHIP} endpoint slopes are computed deterministically from these knot values. For range propagation, piecewise-linear extrema on an interval occur at the interval endpoints and/or knot points, whereas for cubic splines we additionally consider interior stationary points when computing tight bounds (see Lemma~\ref{lem:domain_prop} and Algorithm~\ref{alg:domain_prop}). Each spline can be defined by a set of knots (x-coordinates) and corresponding coefficients (y-coordinates). A common approach is to use fixed, uniformly spaced knots over the spline's domain, with learnable coefficients. The number of knots is a hyperparameter that affects both expressivity and computational cost.

For the inner spline $\phi^{(\ell)}$, we fix the codomain to $[0,1]$ and enforce \emph{strict} monotonicity of the knot values by parameterizing positive differences. Concretely, let $(v_k)_k$ be unconstrained parameters and set $\Delta_k=\operatorname{softplus}(v_k)>0$; then the coefficients are
$$u_k \;=\; \sum_{i\le k}\Delta_i,\qquad c_k \;=\; \frac{u_k}{u_{G-1}+\varepsilon},$$
with a tiny $\varepsilon>0$ (we use $\varepsilon=10^{-8}$) which yields strictly increasing knot values. Interpolating these knots with either piecewise--linear segments or a shape--preserving monotone $C^1$ cubic Hermite (\textsc{PCHIP}) scheme yields a monotone non-decreasing spline on its domain. Outside the domain we use constant extension, so $\phi^{(\ell)}(x)=0$ for inputs below the leftmost knot and $\phi^{(\ell)}(x)=1$ above the rightmost knot. The increments are initialized nearly uniform so that $\phi^{(\ell)}$ starts close to linear.

The outer spline $\Phi^{(\ell)}$ operates on a domain determined by the block’s bounds, and its codomain is parameterized as an interval $[\,c_c^{(\ell)}-c_r^{(\ell)},\,c_c^{(\ell)}+c_r^{(\ell)}\,]$ with trainable center $c_c^{(\ell)}$ and scale (initialized positive) $c_r^{(\ell)}$. We initialize $(c_c^{(\ell)},c_r^{(\ell)})$ from the computed input domain so that $\Phi^{(\ell)}$ is near identity at initialization, and the parameters then adapt the output range during training. Monotonicity is not required for $\Phi^{(\ell)}$.

\subsection{Shifts, weights, and optimization}
Each block includes the learnable scalar shift $\eta^{(\ell)}$ and the learnable mixing weight vector $\lambda^{(\ell)} \in \mathbb{R}^{d_{\ell-1}}$. While Sprecher's original construction requires the shift parameter to be positive ($\eta > 0$), practical implementations can relax this constraint. As shown in Lemma~\ref{lem:domain_prop}, the theoretical domain computation handles both positive and negative values of $\eta^{(\ell)}$, allowing it to be trained as an unconstrained parameter initialized to a small positive value.

When lateral mixing is enabled, each block additionally includes a lateral scale parameter $\tau^{(\ell)}$ (typically initialized to a small value like 0.1) and lateral mixing weights $\omega^{(\ell)}$. For cyclic mixing, this involves $d_\ell$ weights (one per output), while bidirectional mixing requires $2d_\ell$ weights (forward and backward for each output). These weights are typically initialized to small values to ensure training begins with minimal lateral communication, allowing the network to gradually learn the optimal mixing patterns.

All learnable parameters (spline coefficients, $\eta^{(\ell)}$, $\lambda^{(\ell)}$, lateral mixing parameters, and potentially range parameters for $\Phi^{(\ell)}$) are trained jointly using gradient-based optimization methods like Adam \cite{kingma2014adam} or LBFGS. The loss function is typically Mean Squared Error (MSE) for regression tasks. Due to the constrained parameter space and shared spline structure, SNs may require more training iterations than equivalent MLPs or KANs to converge, though the per-iteration computational cost is typically lower due to fewer parameters.

\subsection{Memory-efficient forward computation}
\label{sec:memory_efficient}

A key advantage of SNs' vector-based weight structure extends beyond parameter count to memory footprint, in particular to peak forward-intermediate memory. While dense MLP layers require $O(N^2)$ parameters (and thus memory) for their weight matrices alone, SN blocks have only $O(N)$ vector parameters (plus $O(G)$ spline parameters) and can be evaluated without materializing the full $d_{\mathrm{in}}\times d_{\mathrm{out}}$ outer-product intermediate.

The standard forward pass for a Sprecher block naively computes:
$$\text{shifted}_{b,i,q} = x_{b,i} + \eta q \quad \forall b \in [B], i \in [d_{\mathrm{in}}], q \in [d_{\mathrm{out}}]$$
storing the full tensor before applying $\phi$, requiring $O(B \cdot d_{\mathrm{in}} \cdot d_{\mathrm{out}})$ memory. However, we can compute the \emph{unmixed} pre-activations
$$s_{b,q} = \sum_{i=1}^{d_{\mathrm{in}}} \lambda_i \phi(x_{b,i} + \eta q) + \alpha q$$
sequentially for $q = 0, \ldots, d_{\mathrm{out}}-1$ without materializing $\text{shifted}_{b,i,q}$. When lateral mixing is enabled, we then apply
$$\tilde{s}_{b,q} = s_{b,q} + \tau \sum_{j\in \mathcal{N}(q)} \omega_{q,j}\, s_{b,j}$$
to the assembled vector $(s_{b,q})_{q=0}^{d_{\mathrm{out}}-1}$ before evaluating $\Phi(\tilde{s}_{b,q})$ (and adding the residual path).

This reformulation reduces peak \emph{forward-intermediate} memory from $O(B \cdot N^2)$ to $O\!\bigl(B \cdot \max(d_{\mathrm{in}}, d_{\mathrm{out}})\bigr)$ during computation, while producing \emph{mathematically identical} results. Combined with SNs' $O(N+G)$ parameter memory per block, the total (parameters + peak forward intermediates) scales as $O\!\bigl(N+G + B \cdot \max(d_{\mathrm{in}}, d_{\mathrm{out}})\bigr)$; for fixed $B$ and $G$, this is $O(N)$, compared to dense MLPs' $O(N^2)$ parameter memory per block. (In standard reverse-mode autodiff frameworks, training may require additional saved state for backpropagation; the bound above refers to forward intermediates/inference, or to training with recomputation/checkpointing.)

In practice, this enables training wider architectures under fixed memory budgets by avoiding materialization of the full $(B\times d_{\mathrm{in}}\times d_{\mathrm{out}})$ tensor of shifted inputs. The runtime impact is workload-dependent: sequential evaluation reduces intra-layer parallelism, but can also improve cache behavior and reduce allocator pressure for very large layers.

\begin{remark}[Preservation of mathematical structure]
The sequential computation is a pure implementation optimization that is mathematically exact: it produces identical outputs to the naive parallel formulation for any choice of parameters and inputs. It only reduces peak memory by avoiding materialization of the full $(B\times d_{\mathrm{in}}\times d_{\mathrm{out}})$ tensor of shifted inputs; it does not change the function being computed.
\end{remark}

\begin{remark}[Parallelism vs.\ memory trade-off]
Sequential (per-$q$) evaluation reduces peak activation memory from $O(B\,d_{\mathrm{in}} d_{\mathrm{out}})$ to $O\bigl(B\,\max\{d_{\mathrm{in}}, d_{\mathrm{out}}\}\bigr)$, at the cost of less intra-layer parallelism. This is often advantageous on accelerators with ample compute but constrained memory. 
\end{remark}

\subsection{Theoretical domain computation}
\label{sec:theoretical_domains}
One advantage of the structured Sprecher architecture is the ability to compute \emph{provably safe} interval bounds for spline inputs and outputs, which we use to adapt spline domains during training. For a fixed input interval to a spline, the exact range of a piecewise-linear spline on that interval is attained at the endpoints and any interior knots; for shape-preserving cubic (PCHIP) splines, stationary points within knot intervals must also be checked. When these per-spline range computations are propagated through multiple blocks using interval arithmetic, the resulting network-level bounds are generally conservative (since correlations between coordinates are ignored), but are typically much tighter than naive global bounds. Throughout this analysis, we assume inputs lie in $[0, 1]^n$, though the methodology extends naturally to any bounded domain. This assumption enables principled initialization and can aid in debugging and analysis.

\begin{lemma}[Domain propagation through Sprecher blocks]\label{lem:domain_prop}
Consider a Sprecher block where inputs lie in a box $\prod_{i=1}^{d_{\mathrm{in}}}[a_i,b_i]$ (and define $a:=\min_i a_i$ and $b:=\max_i b_i$), with shift parameter $\eta$, weights $\lambda_i$, lateral mixing parameters $\tau, \omega$, and output dimension $d_{\mathrm{out}}$. Let us distinguish between a spline's \emph{domain} (the interval of valid inputs) and its \emph{codomain} (the interval of possible outputs). Then:
\begin{enumerate}
\item The domain required for $\phi$ to handle all possible inputs is:
$$\mathcal{D}_\phi = \begin{cases}
[a, b + \eta(d_{\mathrm{out}}-1)] & \text{if } \eta \geq 0 \\
[a + \eta(d_{\mathrm{out}}-1), b] & \text{if } \eta < 0
\end{cases}$$

\item Without lateral mixing, monotonicity of $\phi$ implies $\phi(x_i+\eta q)\in[\phi(a+\eta q),\,\phi(b+\eta q)]$ for all $i$ and $q$. Let
$\lambda^{+}:=\sum_{i:\lambda_i\ge 0}\lambda_i$ and $\lambda^{-}:=\sum_{i:\lambda_i<0}\lambda_i$.
Then for each head index $q$,
$$
s_{q}^{\min,\mathrm{unmixed}}=\lambda^{+}\,\phi(a+\eta q)+\lambda^{-}\,\phi(b+\eta q)+\alpha q,\qquad
s_{q}^{\max,\mathrm{unmixed}}=\lambda^{+}\,\phi(b+\eta q)+\lambda^{-}\,\phi(a+\eta q)+\alpha q.
$$
A valid domain for $\Phi$ is therefore $\mathcal{D}_{\Phi}=\bigl[\min_{q}s_{q}^{\min,\mathrm{unmixed}},\;\max_{q}s_{q}^{\max,\mathrm{unmixed}}\bigr]$.
(As a coarse bound, one may use only that $\phi(\cdot)\in[0,1]$ and thus replace the unknown values $\phi(a+\eta q)$ and $\phi(b+\eta q)$ by $0$ and $1$, respectively.)

\item With lateral mixing, for cyclic mixing with scale $\tau$ and weights $\omega$, the domain for $\Phi$ must account for sign-aware neighbor contributions. Using the \emph{unmixed} bounds from the previous item, for each output $q$ the mixed pre-activation bounds are:
$$\begin{aligned}
s^{\min}_q &= s^{\min,\text{unmixed}}_q + (\tau\omega_q)^+ s^{\min,\text{unmixed}}_{(q+1) \bmod d_{\mathrm{out}}} + (\tau\omega_q)^- s^{\max,\text{unmixed}}_{(q+1) \bmod d_{\mathrm{out}}}\\
s^{\max}_q &= s^{\max,\text{unmixed}}_q + (\tau\omega_q)^+ s^{\max,\text{unmixed}}_{(q+1) \bmod d_{\mathrm{out}}} + (\tau\omega_q)^- s^{\min,\text{unmixed}}_{(q+1) \bmod d_{\mathrm{out}}}
\end{aligned}$$
where $a^+ = \max(a,0)$ and $a^- = \min(a,0)$. For bidirectional mixing, apply the analogous sign-split bound to each neighbor contribution $(\tau\omega_{q,\text{fwd}})\,s_{(q+1)\bmod d_{\mathrm{out}}}$ and $(\tau\omega_{q,\text{bwd}})\,s_{(q-1)\bmod d_{\mathrm{out}}}$, and sum the contributions.

\item Tight output bounds require checking all candidate extrema of $\Phi$ over the interval. For piecewise linear $\Phi$, extrema occur at endpoints and knot points, so it suffices to evaluate $\Phi$ at interval endpoints and all knots within the interval. For cubic $\Phi$ (PCHIP), extrema may also occur at internal critical points where $\Phi'$ is zero, requiring evaluation at those points. (If an affine codomain reparameterization $(c_c,c_r)$ is used, it is an affine map of the underlying spline values; it does not change the set of candidate points for extrema, only rescales and shifts the resulting values.) This yields correct range propagation even for oscillatory $\Phi$.

\item When residual connections are present, the output range must be adjusted by the residual contribution. For a scalar residual weight $w$ (the $d_{\mathrm{in}}=d_{\mathrm{out}}$ case), the residual term in coordinate
$q$ is $r_q=w x_{q+1}$ and lies in $[\min\{w a_{q+1},w b_{q+1}\},\max\{w a_{q+1},w b_{q+1}\}]$. For a linear residual projection matrix $W\in\mathbb{R}^{d_{\mathrm{in}}\times d_{\mathrm{out}}}$ (the $d_{\mathrm{in}}\neq d_{\mathrm{out}}$ case), the residual term in coordinate $q$ is $r_q=\sum_{i=1}^{d_{\mathrm{in}}}W_{i,q}x_i$ and lies in:
$$r_q^{\min}=\sum_{i=1}^{d_{\mathrm{in}}}\bigl(W^{+}_{i,q}a_i+W^{-}_{i,q}b_i\bigr),\qquad
r_q^{\max}=\sum_{i=1}^{d_{\mathrm{in}}}\bigl(W^{+}_{i,q}b_i+W^{-}_{i,q}a_i\bigr),$$
where $W^{+}=\max(W,0)$ and $W^{-}=\min(W,0)$ elementwise. These adjustments can be applied elementwise. For Cyclic residuals:

   \begin{itemize}
   \item
 \textbf{Broadcast}: Each output $q$ receives $w_q^{\text{bcast}} \cdot x_{1+(q \bmod d_{\mathrm{in}})}$
   \item 
\textbf{Pooling}: With assignment $i \mapsto q(i)$ and weights $w_i$, the contribution to output $q$ is:
   $$
r^{\min}_q = \sum_{i: q(i)=q} \min\{w_i a_i, w_i b_i\}, \quad r^{\max}_q = \sum_{i: q(i)=q} \max\{w_i a_i, w_i b_i\}$$
   \end{itemize}

\item \textbf{Out-of-domain extension:} When inputs fall outside the spline's domain:
   \begin{itemize}
   \item $\phi$ (monotonic): Uses \emph{constant} extension (zero slope) outside its domain: for $x<\min\mathcal{D}_\phi$, set $\phi(x)=0$; for $x>\max\mathcal{D}_\phi$, set $\phi(x)=1$. This preserves the codomain $[0,1]$ and aligns with the monotone-increments parameterization.
   \item $\Phi$ (general): Uses \emph{linear} extrapolation based on the boundary \emph{derivative} of the spline interpolation (piecewise-linear: slope of the first/last segment; cubic \textsc{PCHIP}: endpoint derivative from the \textsc{PCHIP} slope rule). If $\Phi$'s codomain is parameterized by $(c_c,c_r)$ with $\operatorname{cod}(\Phi)=[c_c-c_r,\,c_c+c_r]$, we first evaluate the \emph{unnormalized} spline $\tilde{\Phi}$ (including its linear extrapolation) in coefficient space, then map its value $t=\tilde{\Phi}(x)$ by
   $$t \;\mapsto\; c_c - c_r \;+\; 2c_r \cdot \frac{t - t_{\min}}{t_{\max}-t_{\min}+\epsilon}\,,$$
   where $t_{\min}=\min_k \tilde{\Phi}(u_k)$ and $t_{\max}=\max_k \tilde{\Phi}(u_k)$ are the minimum/maximum unnormalized knot values (with $\{u_k\}$ the knot locations) and $\epsilon>0$ is a small constant. The boundary derivatives are scaled by the same factor $2c_r/(t_{\max}-t_{\min}+\epsilon)$.
   \end{itemize}
\end{enumerate}
\end{lemma}

\begin{corollary}[Per-dimension input intervals]
When per-dimension intervals $x_i \in [a_i, b_i]$ are available, tighter bounds can be computed:
$$\begin{aligned}
s_q^{\min} &= \sum_i \bigl(\lambda_i^+ \phi(a_i + \eta q) + \lambda_i^- \phi(b_i + \eta q)\bigr) + \alpha q\\
s_q^{\max} &= \sum_i \bigl(\lambda_i^+ \phi(b_i + \eta q) + \lambda_i^- \phi(a_i + \eta q)\bigr) + \alpha q
\end{aligned}$$
where $\lambda_i^+ = \max(\lambda_i, 0)$ and $\lambda_i^- = \min(\lambda_i, 0)$. These per-$q$ bounds then undergo sign-aware lateral mixing as in Lemma \ref{lem:domain_prop}.
\end{corollary}

This lemma enables a forward propagation algorithm for computing all spline domains throughout the network. The algorithm can optionally apply a small relative domain safety margin (e.g., 1\%) to reduce edge hits during training; setting the margin to 0 yields the tightest domains. Crucially, the algorithm's efficiency relies on the fact that spline extrema over compact intervals can be found by evaluating a small, explicit candidate set: for piecewise-linear splines, the interval endpoints and any knots inside the interval; for piecewise-cubic Hermite/PCHIP splines, the same candidates plus any interior critical points where the derivative vanishes (one quadratic solve per segment).

\begin{figure}[ht]
    \centering
    % Custom colors
    \definecolor{sprecherblue}{RGB}{70,130,180}
    \definecolor{sprecherred}{RGB}{220,20,60}
    \definecolor{sprechergreen}{RGB}{34,139,34}
    
    \begin{tikzpicture}[scale=0.95, >=latex]
        % --- Left Subfigure: Naive Approach (Stretching & Shifting) ---
        \begin{scope}[xshift=0cm]
            % Header
            \node[font=\small\bfseries, anchor=south] at (2.4, 3.6) {Naive: horizontal rescaling};
            
            % Axes
            \draw[->] (0,0) -- (4.8,0) node[right, font=\footnotesize] {$x$};
            \draw[->] (0,0) -- (0,3.0) node[above, font=\footnotesize] {$\Phi$};
            
            % Old function (Gray)
            \draw[thick, gray!50] (0.2,0.5) -- (0.8,1.5) -- (1.4,0.8) -- (2.0,1.8) -- (2.6,1.2);
            \foreach \p in {(0.2,0.5), (0.8,1.5), (1.4,0.8), (2.0,1.8), (2.6,1.2)}
                \fill[gray!50] \p circle (2pt);
            \node[font=\scriptsize, gray, anchor=south] at (1.4, 1.9) {old};
            
            % Transition Arrow
            \draw[->, thick, sprecherred] (2.7, 1.5) -- (3.1, 1.5);
            
            % New function (Red Dashed) - Stretched & Shifted
            \draw[thick, sprecherred, dashed] (0.8,0.5) -- (1.7,1.5) -- (2.6,0.8) -- (3.5,1.8) -- (4.4,1.2);
            \foreach \p in {(0.8,0.5), (1.7,1.5), (2.6,0.8), (3.5,1.8), (4.4,1.2)}
                \fill[sprecherred] \p circle (2pt);
            
            \node[font=\scriptsize, sprecherred, anchor=south] at (3.5, 1.9) {stretched \& shifted};
            
            % Failure Text: Centered
            \node[anchor=north, align=left, font=\footnotesize, text=sprecherred] at (2.4, -0.6) {
                \ding{55} Derivatives changed\\
                \ding{55} Function shape destroyed
            };
        \end{scope}
        
        % --- Right Subfigure: Proper Approach (Resampling) ---
        \begin{scope}[xshift=7.5cm]
            % Header
            \node[font=\small\bfseries, anchor=south] at (2.4, 3.6) {Proper: function-preserving resampling};
            
            % Axes
            \draw[->] (0,0) -- (4.8,0) node[right, font=\footnotesize] {$x$};
            \draw[->] (0,0) -- (0,3.0) node[above, font=\footnotesize] {$\Phi$};
            
            % Old function (Gray)
            \draw[thick, gray!40] (0.2,0.5) -- (0.8,1.5) -- (1.4,0.8) -- (2.0,1.8) -- (2.6,1.2);
            \foreach \p in {(0.2,0.5), (0.8,1.5), (1.4,0.8), (2.0,1.8), (2.6,1.2)}
                \fill[gray!40] \p circle (2pt);
            
            % Transition Arrow
            \draw[->, thick, sprechergreen] (2.7, 1.5) -- (3.1, 1.5);

            % New function (Green)
            % Domain shifted by +0.4. New Domain: [0.6, 3.0].
            % Exactly 5 knots (same count as old).
            % Points evaluated on old geometry:
            % 1. x=0.6 (Seg 1): y ~ 1.17
            % 2. x=1.2 (Seg 2): y ~ 1.03
            % 3. x=1.8 (Seg 3): y ~ 1.47
            % 4. x=2.4 (Seg 4): y ~ 1.40
            % 5. x=3.0 (Clamped): y ~ 1.20
            
            \draw[thick, sprechergreen] (0.6,1.17) -- (1.2,1.03) -- (1.8,1.47) -- (2.4,1.40) -- (3.0,1.20);
            
            % New Knots (5 count)
            \foreach \p in {(0.6,1.17), (1.2,1.03), (1.8,1.47), (2.4,1.40), (3.0,1.20)}
                \fill[sprechergreen] \p circle (2pt);
                
            % Label moved safely below the tail
            \node[font=\scriptsize, sprechergreen, anchor=north] at (3.0, 1.1) {resampled};
            
            % Success Text: Centered
            \node[anchor=north, align=left, font=\footnotesize, text=sprechergreen] at (2.4, -0.6) {
                \ding{51} Function shape approximately preserved\\
                \ding{51} $C^0$ continuity maintained
            };
        \end{scope}
    \end{tikzpicture}
    \caption{Spline domain resampling during training (applied to the outer spline $\Phi$). As parameters $\lambda$ and $\eta$ evolve, induced spline input ranges shift dynamically. \textbf{Left:} Naive horizontal rescaling distorts the learned function by stretching and shifting the shape to fit the new domain. \textbf{Right:} Our resampling evaluates the previous spline at the new knot positions (for piecewise-linear $\Phi$, query points are clamped to the previous domain when a new knot lies outside it; for cubic/\textsc{PCHIP} $\Phi$, the previous spline's linear extrapolation is used), approximately preserving the function values at the new knots that lie within the previous domain; the updated $\Phi$ still uses linear extrapolation outside its \emph{new} domain.}
    \label{fig:resampling}
\end{figure}

\begin{algorithm}
\caption{Forward domain propagation with lateral mixing and per-dimension tracking}
\label{alg:domain_prop}
\begin{algorithmic}[1]
    \State \textbf{Input:} Network parameters $\{\lambda^{(\ell)}, \eta^{(\ell)}, \phi^{(\ell)}, \Phi^{(\ell)}, \tau^{(\ell)}, \omega^{(\ell)}, R^{(\ell)}\}_{\ell=1}^{L_{\text{blocks}}}$, input domain $[0,1]^n$
    \State \textbf{(Here $L_{\text{blocks}}$ is the number of Sprecher blocks: $L_{\text{blocks}}=L$ for scalar outputs, and $L_{\text{blocks}}=L{+}1$ for vector outputs.)}
    \State \textbf{Output:} Domains and ranges for all splines
    \State Initialize current range $\mathcal{R}_0 \leftarrow [0,1]^n$ (per-dimension when available)
    \For{each block $\ell = 1, \ldots, L_{\text{blocks}}$}
        \If{normalization is applied \emph{before} block $\ell$}
            \State Apply normalization bounds to $\mathcal{R}_{\ell-1}$ (training: conservative shared bound derived from standardized $[-4,4]$; eval: use running stats and affine)
        \EndIf
        \State $\mathcal{D}_\phi^{(\ell)} \leftarrow$ apply Lemma \ref{lem:domain_prop} part (1) to the current input range
        \If{per-dimension intervals available}
            \State Compute per-$q$ bounds using the corollary (sign-splitting via $\lambda_i^\pm$ and $\phi(a_i+\eta q),\phi(b_i+\eta q)$)
        \Else
            \State Compute coarse per-$q$ bounds $s^{\min,\mathrm{unmixed}}_q,s^{\max,\mathrm{unmixed}}_q$ via Lemma~\ref{lem:domain_prop} part (2)
        \EndIf
        \State Apply sign-aware lateral mixing (part 3) to get final $s^{\min}_q, s^{\max}_q$
        \State $\mathcal{D}_\Phi^{(\ell)} \leftarrow [\min_q s^{\min}_q, \max_q s^{\max}_q]$
        \State Evaluate $\Phi^{(\ell)}$ on each interval $[s^{\min}_q, s^{\max}_q]$ at (i) the endpoints, (ii) all $\Phi$-knots lying inside the interval, and (iii) for cubic/PCHIP $\Phi$, all interior critical points where $\frac{d}{dx}\Phi(x)=0$
        \State Let $[y_q^{\min},y_q^{\max}]$ denote the range of $\Phi^{(\ell)}$ on $[s_q^{\min},s_q^{\max}]$
        \If{block $\ell$ has residual connections}
            \State Adjust each $[y_q^{\min},y_q^{\max}]$ according to Lemma \ref{lem:domain_prop} part (5)
        \EndIf
        \If{$\ell=L_{\text{blocks}}$ and the network output is scalar (sum over $q$)}
            \State $\mathcal{R}_\ell \leftarrow \Bigl[\sum_{q=0}^{d_\ell-1} y_q^{\min},\,\sum_{q=0}^{d_\ell-1} y_q^{\max}\Bigr]$
        \Else
            \State $\mathcal{R}_\ell \leftarrow \prod_{q=0}^{d_\ell-1} [y_q^{\min},y_q^{\max}]$
        \EndIf
        \If{normalization is applied \emph{after} block $\ell$}
            \State Apply normalization bounds (training: conservative shared bound derived from standardized $[-4,4]$; eval: use running stats and affine)
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}

\begin{remark}[Dynamic spline updates and function preservation]
A critical challenge in training Sprecher Networks is that the domains of the splines $\phi^{(\ell)}$ and $\Phi^{(\ell)}$ evolve as the parameters $\eta^{(\ell)}$, $\lambda^{(\ell)}$, and lateral mixing parameters are updated. To maintain theoretical fidelity to Sprecher's formula while adapting to evolving domains, we employ a selective function-preserving resampling strategy:

\paragraph{General Splines ($\Phi^{(\ell)}$):} When the domain of a $\Phi^{(\ell)}$ spline changes, new knot locations are established (typically uniformly spaced across the new computed domain). The original spline is treated as a continuous function and evaluated at these new knot locations to yield updated knot coefficients (for piecewise-linear $\Phi^{(\ell)}$, query points are clamped to the previous domain so any new knot outside the old domain uses the nearest boundary value; for cubic/\textsc{PCHIP} $\Phi^{(\ell)}$, the previous spline's linear extrapolation is used outside the old domain). This effectively ``resamples'' the learned shape onto the new domain: it preserves the \emph{values} of the old spline at the new knot locations that lie within the previous domain (and therefore preserves the function exactly on the overlap whenever the old spline is linear on each new knot interval, e.g.\ when the new knot set contains the old knots). For piecewise-linear $\Phi^{(\ell)}$, this procedure is exact whenever the new knot set contains the old knots; for cubic (PCHIP) $\Phi^{(\ell)}$, it matches the old spline at the resampled knots but can (slightly) change derivatives between knots, while still avoiding abrupt resets. Optionally, one may additionally apply a cheap post-update refresh that adjusts knot \emph{locations} without resampling coefficients; this is not function-preserving (it amounts to a mild horizontal rescaling), so it is best reserved for small domain changes where the primary goal is to keep bound propagation synchronized after parameter updates.

\paragraph{Monotonic Splines ($\phi^{(\ell)}$):} For the monotonic splines, whose purpose is to provide an increasing map to $[0,1]$, complex resampling is not required. Their learnable parameters define relative increments between knots, not an arbitrary shape. Therefore, a straightforward update of the knot positions to the new theoretical domain is sufficient and computationally efficient.

This targeted approach avoids the most direct sources of information loss and instability from domain changes. In engineering ablations on Toy-2D (Table~\ref{tab:component_ablations}), disabling $\Phi$-resampling typically led to oscillatory or unstable training as domains shifted, while disabling dynamic domain updates caused spline inputs to drift far outside their learned ranges (forcing $\phi$ into saturation and $\Phi$ into extrapolation). We therefore treat dynamic domains and $\Phi$-resampling as default components unless explicitly stated otherwise (e.g., the PINN setting in Sec.~\ref{sec:pinn_poisson}). The fundamental challenge of optimizing splines within dynamically shifting domains remains, but this mitigation strategy has proven effective in practice.
\end{remark}

One particularly useful application of domain computation is the initialization of $\Phi^{(\ell)}$ splines. When $\Phi$ codomain scaling $(c_c^{(\ell)},c_r^{(\ell)})$ is enabled, we compute each $\Phi^{(\ell)}$'s theoretical domain before training and (i) set its knot domain to this interval, (ii) initialize its knot coefficients to a linear ramp over that same interval so that the underlying spline represents the identity map on the interval (for both linear and cubic/\textsc{PCHIP} interpolation), and (iii) initialize $(c_c^{(\ell)},c_r^{(\ell)})$ to the domain center and radius so that the affine codomain reparameterization preserves this identity map. This provides a principled initialization strategy that ensures the initial network performs a series of near-identity transformations.

\begin{remark}[Practical benefits]
The ability to compute \emph{provably safe} (typically conservative) domain and range bounds through interval arithmetic provides several practical benefits: (i) it enables theoretically grounded initialization without arbitrary hyperparameters, (ii) it can help diagnose training issues by detecting when values fall outside expected ranges, (iii) it promotes numerical stability by encouraging spline evaluations to remain within their well-resolved knot regions (with explicit extension/extrapolation behavior outside the knot domain), and (iv) it allows for adaptive domain adjustments that account for lateral mixing dynamics. These benefits distinguish Sprecher Networks from architectures with less structured internal representations.
\end{remark}

\begin{table}[t]
\centering
\small
\begin{tabular}{l p{0.68\linewidth}}
\toprule
Component ablated & Typical observed effect \\
\midrule
Disable domain resampling & Training can become unstable when pre-activations drift beyond initial spline domains, increasing extrapolation error. \\
Disable domain updates & Intermediate pre-activations may drift outside predicted bounds, reducing effective capacity via saturation/clipping effects. \\
Remove normalization & Pre-activations become poorly scaled, making spline-domain management harder and slowing optimization. \\
Remove $\Phi$ codomain scaling $(c_c,c_r)$ & Forces $\Phi$ to learn both scale and shape, often slowing convergence and degrading final accuracy. \\
Remove cyclic residuals & Deep networks become harder to train, with degraded accuracy and occasional divergence. \\
Remove lateral mixing & Wide shallow networks can plateau due to shared-weight symmetries (cf.\ Remark above). \\
\bottomrule
\end{tabular}
\caption{Qualitative summary of component ablations observed across our experiments. These effects align with each component's motivation: domain management for numerical stability, normalization and $\Phi$ codomain scaling for conditioning, residual connections for depth, and lateral mixing for breaking shared-weight symmetries.}
\label{tab:component_ablations}
\end{table}

\section{Empirical demonstrations and case studies}
While comprehensive benchmarking remains future work, we provide initial empirical demonstrations to illustrate the feasibility and characteristics of SNs, including the impact of lateral mixing connections. \\

\paragraph{Benchmarks used in this work.}

We evaluate Sprecher Networks on synthetic supervised regression tasks and on a physics-informed Poisson problem.

\paragraph{Poisson PINN benchmark (manufactured solution).}\label{sec:pinn_poisson}
We solve the Dirichlet problem
$$\Delta f(\mathbf{x}) = g(\mathbf{x}) \quad \text{for } \mathbf{x}\in \Omega,\qquad
f(\mathbf{x})=0 \quad \text{for } \mathbf{x}\in \partial\Omega,$$
with $\Omega=[-1,1]^2$, using the manufactured solution
$$u(x,y) = \sin(\pi x)\,\sin(\pi y^2),$$
so that $g=\Delta u$ is known analytically.
We train with collocation sets $S_{\mathrm{int}}\subset\Omega$ and $S_{\partial}\subset\partial\Omega$ using

$$\mathcal{L} \;=\;
\frac{1}{|S_{\mathrm{int}}|}\sum_{\mathbf{x}\in S_{\mathrm{int}}}
\frac{\bigl(\Delta f(\mathbf{x})-g(\mathbf{x})\bigr)^2}{\mathbb{E}_{\mathbf{x}\in S_{\mathrm{int}}}[g(\mathbf{x})^2]}
\;+\;
\frac{1}{|S_{\partial}|}\sum_{\mathbf{x}\in S_{\partial}} f(\mathbf{x})^2.$$
Unless stated otherwise, we draw a fixed set of $|S_{\mathrm{int}}|=2048$ interior points i.i.d.\ uniformly at random in $\Omega$ (sampled once at the start of training), and
$|S_{\partial}|=4\cdot 257$ boundary points are taken on a uniform grid along each edge (including the corners on each edge, hence corners are duplicated).

\emph{Model variants for the PINN benchmark.}
To isolate the spline parameterization itself, we compare a \textbf{barebones Sprecher Network} (no lateral mixing, no residuals, no normalization, and \textbf{fixed} spline domains; i.e.\ no domain updates) against a \textbf{barebones KAN} baseline with spline-only edge activations (no SiLU/base activation and no grid updates). Because the Poisson residual involves second derivatives, both models use shape-preserving cubic (PCHIP) splines in this benchmark (piecewise-linear splines yield $\Delta f_\theta=0$ almost everywhere).
For the Sprecher Network, inputs are linearly rescaled from $[-1,1]^2$ to $[0,1]^2$ before the first spline evaluation, matching the $[0,1]$ spline-domain convention used for SNs in this work; the KAN baseline operates directly on $[-1,1]^2$. In both cases, derivatives are taken with respect to the original coordinates via automatic differentiation (for SN this is through the rescaling map).

\emph{Optimization and evaluation.}
We train in \texttt{torch.float64} using Adam with $10^{-3}$ learning rate, cosine learning rate decay to $10^{-5}$, and gradient-norm clipping 1.0. We report two parameter budgets: an $\approx$1200-parameter model trained for 5{,}000 epochs (mean $\pm$ std over 5 seeds), and an $\approx$3000-parameter model trained for 20{,}000 epochs (1 seed). We evaluate both models on a uniform $101\times101$ grid over $\Omega=[-1,1]^2$ and report: (i) PDE residual MSE (interior grid points), (ii) boundary MSE (boundary grid points), and (iii) $L^2$ MSE against the manufactured solution (full grid).

\begin{table}[t]
  \centering
  \caption{\textbf{Poisson PINN results (manufactured solution).} Metrics are evaluated on a uniform $101\times101$ grid over $\Omega=[-1,1]^2$: PDE residual MSE is computed on the interior grid points, boundary MSE on the boundary grid points, and $L^2$ MSE against the manufactured solution on the full grid (all lower is better). For the $\approx$1200-parameter setting we report mean $\pm$ std over 5 seeds; the $\approx$3000-parameter setting is reported for 1 seed due to cost.}
  \label{tab:poisson-pinn}
  \begin{tabular}{lrrr}
    \toprule
    Model & PDE MSE & Bndry MSE & $L^2$ MSE \\
    \midrule
    \multicolumn{4}{l}{\textbf{$\approx$1200 params, 5{,}000 epochs (5 seeds)}} \\
    SN  & $\mathbf{1.36\times 10^{2} \pm 3.69}$ & $7.85\times 10^{-2} \pm 7.22\times 10^{-3}$ & $\mathbf{6.55\times 10^{-2} \pm 1.75\times 10^{-3}}$ \\
    KAN & $2.41\times 10^{3} \pm 1.78\times 10^3$ & $\mathbf{2.06\times 10^{-4} \pm 4.53\times 10^{-5}}$ & $1.70\times 10^{-1} \pm 4.33\times 10^{-3}$ \\
    \midrule
    \multicolumn{4}{l}{\textbf{$\approx$3000 params, 20{,}000 epochs (1 seed)}} \\
    SN  & $\mathbf{1.38\times 10^2}$ & $4.76\times 10^{-2}$ & $\mathbf{1.05\times 10^{-1}}$ \\
    KAN & $1.09\times 10^4$ & $\mathbf{8.42\times 10^{-4}}$ & $1.32\times 10^{-1}$ \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Basic function approximation}
We train SNs on datasets sampled from known target functions $f$. The network learns the parameters ($\eta^{(\ell)}$, $\lambda^{(\ell)}$), spline coefficients ($\phi^{(\ell)}$, $\Phi^{(\ell)}$), and when enabled, lateral mixing parameters ($\tau^{(\ell)}$, $\omega^{(\ell)}$) via gradient descent (Adam optimizer) using an MSE objective.

For 1D functions $f(x)$ on $[0,1]$, an SN like $1 \to [W] \to 1$ (one block) learns $\phi^{(1)}$ and $\Phi^{(1)}$ that accurately approximate $f$, effectively acting as a learnable spline
interpolant structured according to Sprecher's formula. Figure~\ref{fig:onevar_example} provides an example where an SN is trained on data generated from a known Sprecher-style superposition. While the network achieves a very accurate approximation of the overall function $f(x)$, the learned components (splines $\hat{\phi}, \hat{\Phi}$, weights $\hat{\lambda}$, shift $\hat{\eta}$) only partially resemble the ground truth functions and parameters used to generate the data. This is expected: the internal decomposition is generally not identifiable, and in particular the learned outer spline $\hat{\Phi}$ can become strongly oscillatory/jagged even when the ground-truth outer function $\Phi$ is smooth.

Moving to multivariate functions, consider the 2D scalar case $f(x,y) = \exp(\sin(11x)) + 3y + 4\sin(8y)$. A network like $2\to[5,8,5]\to1$ (3 blocks) can achieve high accuracy. Figure
\ref{fig:twovarsprecher_revised} shows the interpretable layerwise spline plots and the final fit quality. When lateral mixing is enabled for this architecture, we often observe a different allocation of complexity across channels and layers; however, the learned $\Phi^{(\ell)}$ splines can still be highly oscillatory, so we treat the spline plots as qualitative rather than expecting smoothness.

For 2D vector-valued functions $f(x,y)=(f_1(x,y), f_2(x,y))$, we use an SN of the form $2\to[20,20]\to2$ (2 hidden layers plus a final block for the vector-valued output; 3 blocks total). Figure \ref{fig:twovarsprechervector_revised} illustrates the learned splines and the approximation of both output surfaces. Lateral mixing can be beneficial here by breaking output symmetry across channels, so we enable it in this example.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figures/Toy1DComplex-OneBlock.png}
\caption{Visualization of a trained Sprecher Network with architecture \texorpdfstring{$1\to[5]\to1$}{1 -> [5] -> 1} (one block) trained on data sampled from $f(x) = \sum_{q=0}^{4} \Phi(a_q \phi(x+\eta q)+q)$, where the ground truth functions were $\phi(x) = (e^x-1)/(e-1)$ and $\Phi(x)=\sin{x}$, with shift $\eta=1/10$ and channel coefficients $\{a_q\}_{q=0}^{4} = \{1/2, -4/5, 1, 1/5, -6/5\}$. Top row: Network structure, learned monotonic spline $\phi^{(1)}$ (cyan), learned general spline $\Phi^{(1)}$ (magenta). Bottom row: Comparison between the target function $f(x)$ (dashed black) and the network output (solid red). Even though the ground-truth outer function $\Phi$ here is smooth, the learned outer spline $\Phi^{(1)}$ can become highly oscillatory/jagged while still yielding an accurate composite approximation, consistent with the role of $\Phi$ in Sprecher-style constructions. (This run uses the default cyclic lateral mixing; for 1D targets it typically has only a minor effect.)}
\label{fig:onevar_example}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figures/Toy2DComplex-ThreeBlocks.png}
\caption{Example of SN approximating the scalar 2D target function $z = f(x,y) = \exp(\sin(11x)) + 3y + 4\sin(8y)$ on $(x,y)\in[0,1]^2$ using architecture \texorpdfstring{$2\to[5,8,5]\to1$}{2 -> [5,8,5] -> 1} (3 blocks). Top row: Learned spline functions for each block --- monotonic splines $\phi^{(\ell)}$ (cyan) and general splines $\Phi^{(\ell)}$ (magenta). Bottom row: Comparison between the target function surface (left) and the network approximation (right). This run uses cyclic lateral mixing (default); the learned outer splines $\Phi^{(\ell)}$ can be highly oscillatory/jagged even when the overall fit is accurate, so the per-layer spline plots should be interpreted qualitatively.}
\label{fig:twovarsprecher_revised}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figures/Toy2DVector-ThreeBlocks.png}
\caption{Example of SN approximating a 2D vector-valued function $f:[0,1]^2\to\mathbb{R}^2$ with components $f_1(x,y) = \frac{\exp(\sin(\pi x) + y^2)-1}{7}$ and $f_2(x,y) = \frac{1}{4}y + \frac{1}{5}y^2 - x^3 + \frac{1}{5}\sin(7x)$. The network architecture is \texorpdfstring{$2\to[20,20]\to2$}{2 -> [20,20] -> 2} (2 hidden layers plus an additional output block; 3 blocks total). The first row shows the learned monotonic splines $\phi^{(\ell)}$ (cyan) and general splines $\Phi^{(\ell)}$ (magenta) for each block. The second row shows, for each output component (left: $f_1$, right: $f_2$), the target surface overlaid with the corresponding network output surface. Lateral mixing was enabled (cyclic in this run); as in other examples, the learned $\Phi^{(\ell)}$ splines can be highly oscillatory/jagged even when the overall fit is accurate.}
\label{fig:twovarsprechervector_revised}
\end{figure}

These examples demonstrate the feasibility of training SNs and the potential interpretability offered by visualizing the learned shared splines $\phi^{(\ell)}$ and $\Phi^{(\ell)}$ for each block, as well as the impact of lateral mixing on spline smoothness and approximation quality.

\subsection{Impact of lateral mixing}
To evaluate the contribution of lateral mixing, we conducted ablation studies comparing networks with and without this mechanism across various tasks:

\paragraph{Scalar outputs:} For many scalar-output regression tasks with moderate widths, lateral mixing yields only modest gains. However, scalar-output SNs can still benefit substantially in regimes where the shared-weight constraint induces strong symmetry across channels (e.g., very wide shallow networks), where lateral mixing provides lightweight cross-channel interaction that can break optimization plateaus (see the $2\to[120]\to1$ example above). Note that the final summation is a fixed linear readout \emph{after} applying $\Phi$ channelwise; it does not provide pre-$\Phi$ cross-channel communication, so lateral mixing remains a distinct (and sometimes crucial) mechanism even when the network output is scalar.

\paragraph{Vector outputs:} On vector-valued regression tasks, networks with lateral mixing (cyclic variant) often achieved lower RMSE with only a marginal increase in parameters (one additional per-channel weight plus a shared scale). The improvement was most pronounced for functions where output dimensions exhibit strong correlations, suggesting that lateral mixing can help the network capture cross-output dependencies despite the constraint of shared splines.

\paragraph{Convergence speed:} Networks with lateral mixing often converged faster during training in our ablations, reaching the same loss threshold in fewer iterations. This suggests that lateral connections can provide beneficial gradient pathways that accelerate optimization, although the effect is task-dependent.

\paragraph{Spline behavior:} Visual inspection of learned splines suggests that lateral mixing can change how complexity is distributed across output channels. In particular, the learned outer splines $\Phi^{(\ell)}$ can remain highly oscillatory even when the overall fit is accurate, which is consistent with Sprecher's original construction. KANs are often motivated by interpretability because each edge carries an explicit learned univariate function \cite{liu2024kan}; however, as Figure~\ref{fig:sn_vs_kan_splines} illustrates, these learned edge functions can also be jagged/noisy under parameter parity, so visual interpretability does not automatically imply smoothness.

\subsection{Baseline apples-to-apples comparisons: SNs vs.\ KANs}\label{sec:sn-kan-baselines}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Figures/SNvsKAN-Splines.png}
\caption{Side-by-side comparison of learned functions and internal univariate splines for an SN and a KAN on the Toy-2D benchmark ($f(x,y)=\exp(\sin(\pi x)+y^2)$) under parameter parity ($\approx 1200$ parameters). For this figure we use a spline-isolation setting (no residual connections, no normalization, no lateral mixing, fixed spline domains) to make the learned univariate functions directly comparable. In the representative run shown, the SN (left; $\text{arch}=[3,3,3]$, $1201$ params) attains MSE $4.334\times 10^{-2}$ and the KAN (right; width=3, layers=4, $1198$ params) attains MSE $5.279\times 10^{-2}$. KANs are often motivated by interpretability because each edge carries an explicit learned univariate function \cite{liu2024kan}, yet the learned edge functions (27 total here, one per edge) can still be jagged/noisy in practice. In contrast, the SN achieves a comparable fit using one shared pair $(\phi^{(\ell)},\Phi^{(\ell)})$ per block ($6$ shared splines total across three blocks) with similarly oscillatory outer splines $\Phi^{(\ell)}$, consistent with the role of $\Phi$ in Sprecher-style superpositions.}

\label{fig:sn_vs_kan_splines}
\end{figure}

\paragraph{Rationale.}
The goal of this subsection is to provide simple, controlled, apples-to-apples tests where Sprecher Networks (SNs) and Kolmogorov–Arnold Networks (KANs) are trained under the same budget and with matched modeling capacity, so that we can isolate architectural inductive biases rather than hyperparameter tuning. We do \emph{not} claim that SNs dominate KANs on every task; rather, we highlight representative settings where SNs perform as well or better, sometimes substantially so, while keeping the comparison fair.

\paragraph{Protocol (fairness constraints).}
Unless otherwise stated (e.g., Figure~\ref{fig:sn_vs_kan_splines}), each head-to-head uses exactly $4000$ training epochs for both architectures. Both models use the same \emph{linear} residual style: a learned scalar gate when $d_{\mathrm{in}}=d_{\mathrm{out}}$, and a learned projection matrix $W\in\mathbb{R}^{d_{\mathrm{in}}\times d_{\mathrm{out}}}$ when $d_{\mathrm{in}}\neq d_{\mathrm{out}}$. KANs use shape-preserving cubic Hermite (PCHIP; $\deg=3$) edge splines. SNs use domain warmup early in training (300–400 epochs depending on the task; relative domain safety margin $0.01$) and then freeze the learned domains; lateral mixing is disabled (i.e., $\tau^{(\ell)}=0$) in these parity runs. We match parameter counts by choosing the KAN spline size $K$ (the number of learnable knot values / spline coefficients per univariate edge spline) to be as close as possible (preferring $\leq$) to the SN parameter count, and we keep batch-normalization (BN) semantics aligned at test time with the training semantics. Seeds, datasets, and train/test splits are identical across models. Training is full-batch (no mini-batching): each epoch uses the entire training set. Primary metrics are test set RMSE or MSE (reported per task below); for multi-head monotone problems we also report a monotonicity-violation rate and a correlation-structure error.

\paragraph{Reporting conventions (and what we \emph{do not} report).}
To avoid confounds from implementation-level vectorization (e.g., a highly vectorized KAN forward vs.\ a less vectorized SN implementation), we \emph{do not} treat wall-clock time as a primary figure of merit here. We record and report it for completeness, but treat it as secondary and highly implementation-dependent; we therefore do not draw algorithmic conclusions from wall-clock time.

\vspace{0.5ex}
\noindent\textbf{Structure of this subsection.} We present each comparison task as its own \emph{subsubsection}, with (i) a brief problem description, (ii) a one-paragraph explanation for why the task might reward SN inductive biases (if applicable), and (iii) a compact result table for that task.

\subsubsection{Monotone single-index with quantiles (MQSI; 20D, $m{=}9$ heads)}
\label{sssec:mqsi_20d_9h_parity}

\paragraph{Setup.}
We sample inputs $\mathbf{x}\sim \mathrm{Unif}([0,1]^D)$ with $D=20$ and define $m=9$ heads at quantile levels
$\tau_j=\mathrm{linspace}(0.1,0.9)$, with $z_j=\Phi_{\mathcal{N}}^{-1}(\tau_j)=\sqrt{2}\,\mathrm{erf}^{-1}(2\tau_j-1)$, where $\Phi_{\mathcal{N}}$ is the standard normal CDF (to avoid confusion with the outer spline $\Phi$).
For each coordinate, define a monotone sigmoid feature
$$h_i(x_i)=\operatorname{sigmoid}\!\bigl(a_i(x_i-c_i)\bigr),$$
where $\operatorname{sigmoid}(z)=\frac{1}{1+e^{-z}}$, $a_i\sim \mathrm{Unif}[2,8]$, and $c_i\sim \mathrm{Unif}[0.2,0.8]$.
Draw raw positive weights $w_i^{\mathrm{raw}},v_i^{\mathrm{raw}}\sim \mathrm{Unif}[0,1]$ and normalize
$w_i=w_i^{\mathrm{raw}}/\sum_k w_k^{\mathrm{raw}}$ and
$v_i=v_{\mathrm{scale}}\,v_i^{\mathrm{raw}}/\sum_k v_k^{\mathrm{raw}}$ with $v_{\mathrm{scale}}=0.25$.
Let $s_0=0.15$ and define
$$\sigma(\mathbf{x})=s_0+\max\!\Bigl(10^{-5},\,\sum_{i=1}^{D} v_i\,h_i(x_i)\Bigr).$$
To introduce controlled head correlations while preserving monotonicity, we add positive pairwise terms:
choose $n_{\mathrm{pairs}}=\mathrm{round}(0.15D)$ disjoint index pairs $(i,j)$ (implemented by sampling $2n_{\mathrm{pairs}}$ distinct indices without replacement and grouping them into pairs) and set
$$\mu(\mathbf{x})=\sum_{i=1}^{D} w_i\,h_i(x_i)\;+\;0.08\sum_{(i,j)} h_i(x_i)\,h_j(x_j).$$
Finally, outputs are
$$y_j=\tanh\,\!\bigl(\beta(\mu(\mathbf{x})+\sigma(\mathbf{x})z_j)\bigr),\qquad \beta=0.8,$$
with no additional observation noise. We train on a fixed i.i.d.\ training set of size $n_{\mathrm{train}}=1024$ and evaluate on a held-out i.i.d.\ test set of size $n_{\mathrm{test}}=50{,}000$.

\paragraph{Metrics.}
Primary: \emph{mean RMSE across heads} (lower is better). Secondary: (i) correlation-structure error
$\;\|\mathrm{Corr}(\widehat{\mathbf{Y}})-\mathrm{Corr}(\mathbf{Y})\|_{\mathrm{F}}\,$
where $\mathrm{Corr}(\cdot)$ denotes the Pearson correlation matrix across head dimensions computed over the test set, and (ii) the monotonicity-violation rate
$$\mathrm{viol}=\frac{1}{n_{\mathrm{test}}}\sum_{n=1}^{n_{\mathrm{test}}}\mathbf{1}\!\left[\exists\, j\in\{0,\dots,m-2\}:\ \widehat{y}_{n,j+1}-\widehat{y}_{n,j}<0\right],$$
i.e.\ the fraction of test inputs for which the predicted head sequence is not monotone (lower is better).

\paragraph{Results (10 seeds).}
Best seed (minimum over seeds of mean RMSE): SN $=4.47\times 10^{-3}$, KAN $=6.58\times 10^{-3}$ (ratio $1.47\times$ in favor of SN). Averaged over the 10 seeds: SN $=8.31\times 10^{-3}$ vs.\ KAN $=1.280\times 10^{-2}$ ($1.54\times$), with SN winning in $8/10$ seeds. Correlation–structure fidelity strongly favored SN: mean Frobenius error $\approx 0.0124$ (SN) vs.\ $0.124$ (KAN). Monotonicity violations were negligible for both (SN had two seeds with $O(10^{-5})$ rates; KAN was $0$). Wall clock (train, CPU; $4{,}000$ epochs): KAN $393$\,s (avg; min $88.6$\,s, max $469.1$\,s) vs.\ SN $589$\,s (avg; min $231.5$\,s, max $732.4$\,s); SN timing includes warm‑up $+$ domain‑freeze.

\paragraph{Discussion.}
This MQSI family is structurally aligned with an SN block $s_q=\sum_i \lambda_i\,\phi(x_i+\eta q)+\cdots$ followed by $\Phi$, so the learned $q$‑shift and monotone $\phi$ produce heads that are smooth shifts of a shared latent index; $\Phi$ handles squashing. KANs can approximate this but lack that inductive bias and rely on fixed grids. Under parameter parity and matched BN semantics, the bias yields lower head‑averaged error and markedly better preservation of cross‑head correlations, with no practical loss of monotonicity. KAN trained faster on CPU in this setting but at higher error.

\subsubsection{Dense-head shifts (DenseHeadShift; 12D, $m{=}64$): SN vs.\ KAN}
\label{subsubsec:dense-head-shift-12d-64h}

\textbf{Task.} Multi-head regression in which each output head is a smoothly shifted version of a shared latent index (we denote the \emph{task}'s head-shift coefficient by $a_{\mathrm{shift}}$ to avoid confusion with the SN channel-spacing constant $\alpha$):
$$y_j(x)\;=\;\tanh\!\Big(\beta\big[\mu(x)+\sigma(x)z_j+a_{\mathrm{shift}}\,q_j + q_{\mathrm{bias}}\,q_j\big]\Big),$$
with $x\in[0,1]^D$, $j=0,\dots,m{-}1$, $q_j\in[-1,1]$ on a uniform grid, and $z_j=\Phi_{\mathcal{N}}^{-1}(\tau_j)$. Here we set $D=12$ and $m=64$ and construct $\mu(\mathbf{x})$ and $\sigma(\mathbf{x})$ as follows.
For each coordinate,
$$h_i(x_i)=\operatorname{sigmoid}\!\bigl(a_i(x_i-c_i)\bigr),$$
where $\operatorname{sigmoid}(z)=\frac{1}{1+e^{-z}}$, $a_i\sim \mathrm{Unif}[2,8]$, and $c_i\sim \mathrm{Unif}[0.2,0.8]$.
Draw raw positive weights $w_i^{\mathrm{raw}},v_i^{\mathrm{raw}}\sim \mathrm{Unif}[0,1]$ and normalize
$w_i=w_i^{\mathrm{raw}}/\sum_k w_k^{\mathrm{raw}}$ and
$v_i=v_{\mathrm{scale}}\,v_i^{\mathrm{raw}}/\sum_k v_k^{\mathrm{raw}}$ with $v_{\mathrm{scale}}=0.25$.
Let $\mathcal{P}$ be a set of $\max(1,\mathrm{round}(0.15D))$ random distinct pairs $(i,k)$ with $1\le i<k\le D$, and set $s_0=0.15$. Then
$$\sigma(\mathbf{x})=s_0+\max\!\Bigl(10^{-5},\sum_{i=1}^D v_i h_i(x_i)\Bigr),$$
$$\mu(\mathbf{x})=\sum_{i=1}^D w_i h_i(x_i)+0.08\sum_{(i,k)\in\mathcal{P}} h_i(x_i)h_k(x_k).$$ We use $\tau_j=\mathrm{linspace}(0.1,0.9)$ and $q_j$ as a uniform grid on $[-1,1]$. We use the same evaluation protocol throughout this subsection: all metrics are computed at test time; when BatchNorm is enabled, we use per-batch normalization statistics while freezing the running-statistics buffers.

\textbf{Setup.} $D{=}12$, $m{=}64$, $a_{\mathrm{shift}}{=}0.7$, $\beta{=}0.8$, $q_{\mathrm{bias}}{=}0.6$; train for 4000 epochs on CPU on a fixed i.i.d.\ training set of $n_{\mathrm{train}}{=}1024$ samples; $n_{\mathrm{test}}{=}50{,}000$. BN evaluation uses per-batch statistics while freezing the running-statistics buffers (i.e., no running-mean/variance updates during evaluation). Parameter parity was enforced by choosing the KAN basis size $K$ to be as close as possible to (and not exceeding) the SN parameter count, yielding \#params $\approx 3072$ for both models; the resulting KAN basis size was $K{=}4$. The specific model configurations were:
\begin{itemize}
\item \textbf{SN:} Two hidden Sprecher blocks of width $32$ (architecture $12\to32\to32\to64$), with $60$ knots for each inner spline $\phi$ and each outer spline $\Phi$; BatchNorm after each block except the first; linear residual connections; no lateral mixing; and dynamic spline-domain updates for the first $400$ epochs, frozen thereafter.
\item \textbf{KAN baseline:} A lightweight edge-wise KAN with hidden widths $(3,24,4)$ (architecture $12\to3\to24\to4\to64$), degree-$3$ splines with $K{=}4$ learnable spline coefficients per edge; BatchNorm after each layer except the first; linear residual connections; and linear extrapolation outside the knot interval.
\end{itemize}

\textbf{Metric.} Mean per-head RMSE over the $m{=}64$ heads on the held-out test set (we report both best-of-10 seeds and across-seed aggregates).

\textbf{Results (10 seeds).}
\begin{itemize}
\item \emph{Best-of-10 (lower is better).} SN: $\mathbf{2.546\times 10^{-3}}$ (seed 2) vs.\ KAN: $5.191\times 10^{-3}$; ratio $=$ \textbf{$2.04\times$} (KAN/SN).
\item \emph{Across seeds (means).} SN: $5.47\times 10^{-3}$; KAN: $6.94\times 10^{-3}$ (i.e., KAN is $\approx1.27\times$ higher RMSE on average).
\item \emph{Monotonicity (fraction of test points violating head-wise order).} KAN: $\approx0$ across runs (all ten had $0.0$). SN: varied by seed, with notable high-violation outliers (e.g., $0.276$ and $0.476$ on some seeds). This reflects that the SN here does not impose head-wise monotonicity explicitly, whereas the KAN runs empirically preserved the ordering under this configuration.
\item \emph{Wall-clock (train time; CPU).} Averaging the ten recorded wall-clock training times: KAN $\,\approx$ \textbf{476\,s}, SN $\,\approx$ \textbf{1{,}433\,s}. For reference, KAN runs were in the $\sim[338,624]$\,s range and SN runs in the $\sim[1{,}241,1{,}607]$\,s range.
\end{itemize}

\textbf{Takeaways.} On this dense-head shift task, whose structure matches SN's shared-$\phi/\Phi$ plus head-wise shift inductive bias, SN attains markedly lower error (best-of-10 $2.04\times$ better, and $\approx21\%$ lower RMSE on average (equivalently, KAN is $\approx27\%$ higher)) under strict parameter parity ($\sim$3k parameters each, KAN $K{=}4$). KAN trains $\approx3\times$ faster on CPU but shows consistently higher RMSE in this setting. The occasional SN monotonicity violations suggest that adding an explicit head-order regularizer or constraint would likely remove those outliers without changing the overall accuracy picture.

\paragraph{Summary across baseline tasks.}
Table~\ref{tab:baseline-summary} collates the head-to-heads above. In both cases we observe SNs outperform KANs at matched parameter budgets and training epochs. The MQSI task, which directly matches the SN compositional/shift bias, shows the clearest gap in correlation-structure fidelity; the DenseHeadShift task is closer, with SNs typically ahead by a modest margin.

\begin{table}[h!]
\centering
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.12}
\footnotesize
\begin{tabularx}{\linewidth}{@{}>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X c c c c@{}}
\hline
\textbf{Task} & \textbf{Metric} & \textbf{SN (mean)} & \textbf{KAN (mean)} & \textbf{Ratio (KAN/SN)} & \textbf{\shortstack{Time\\(KAN/SN) (s)}} \\
\hline
MQSI (20D, 9 heads) & Mean RMSE $\downarrow$ (10 seeds) & $8.31\times 10^{-3}$ & $1.280\times 10^{-2}$ & $1.54\times$ & 393 / 589 (avg) \\
DenseHeadShift (12D, 64 heads) & Mean RMSE $\downarrow$ (10 seeds) & $5.47\times10^{-3}$ & $6.94\times10^{-3}$ & $1.27\times$ & 476 / 1{,}433 (avg) \\
\hline
\end{tabularx}
\caption{Summary across baseline tasks at matched parameter counts and $4000$ epochs; wall-clock time shown for completeness. Times are per-task averages across seeds.}
\label{tab:baseline-summary}
\end{table}

\paragraph{Reproducibility checklist (concise).}
\begin{itemize}[leftmargin=*,nosep]
\item \textbf{Epochs/optimizer:} $4000$ epochs for both models. We use Adam with gradient-norm clipping $1.0$. For SNs we use learning rate $3\times 10^{-4}$ for the main parameters, $5\times 10^{-4}$ for lateral-mixing parameters (when enabled), and $10^{-3}$ for $(c_c,c_r)$ codomain parameters (when enabled); weight decay $10^{-7}$. For KANs we use learning rate $10^{-3}$ and weight decay $10^{-6}$ (no other tuning beyond the fairness knobs).
\item \textbf{Residuals/BN:} Linear residuals in both; BatchNorm layers placed ``after'' each layer/block, and evaluated using per-batch statistics without updating running-mean/variance buffers.
\item \textbf{Parity:} Parameter matching via a closed-loop count of KAN parameters as a function of $K$ (prefer $\leq$).
\item \textbf{Data:} Identical $(x_{\text{train}},y_{\text{train}})$ and test sets across models per task; fixed seeds.
\end{itemize}

\paragraph{Takeaways.}
These controlled baselines demonstrate that there exist natural tasks, particularly those exhibiting shared-index families or smooth saturations, on which SNs achieve lower test error than parameter-matched KANs under equal training budgets. The observed margins range from modest to substantial (especially on structured multi-head problems); we expect this picture to sharpen as we expand the suite with $\sim$8 additional tasks in the same format.

\section{Limitations and future work}
The primary limitation of this work is the gap between our theoretically-grounded single-layer model and our empirically-driven deep architecture. While single-layer SNs inherit universal approximation properties directly from Sprecher's theorem, the universality and approximation bounds of deep, compositional SNs remain open theoretical questions (Conjectures \ref{conj:vector} and \ref{conj:deep_universal}). The role of lateral mixing in these theoretical properties is particularly unclear, while it empirically improves performance, its theoretical justification within the Sprecher framework remains elusive.

The Sprecher block design imposes strong constraints: forcing all feature interactions through two shared splines and using weight vectors rather than matrices heavily restricts expressive power compared to standard architectures. While lateral mixing partially alleviates this constraint by enabling limited cross-dimensional communication, it represents an ad-hoc enhancement rather than a principled extension of Sprecher's theory. This represents a fundamental trade-off between parameter efficiency and flexibility that may limit performance on functions not aligned with this compositional structure.

Current implementations may require more training iterations than MLPs for certain tasks, though the per-iteration computational cost is typically lower due to fewer parameters. When memory-efficient sequential computation is employed to enable training of wider architectures, wall-clock training time increases, representing a fundamental trade-off between memory usage and computational efficiency. The sequential computation mode proves most beneficial for architectures with individual layers exceeding 128-256 units in width, while offering minimal advantage for very deep networks with modest layer widths where the memory bottleneck lies in storing activations across many layers rather than within-block computations. The development of adaptive knot placement strategies that concentrate resolution where data lives while maintaining fixed parameter counts could improve both efficiency and interpretability.

The lateral mixing mechanism, while empirically beneficial, lacks theoretical justification within the Sprecher framework. Understanding whether this enhancement can be connected to the underlying mathematical structure or represents a purely empirical improvement remains an open question. Future work could explore adaptive mixing topologies beyond cyclic patterns, potentially learning the neighborhood structure $\mathcal{N}(q)$ itself, or investigating connections to graph neural networks where the mixing pattern could be viewed as a learnable graph structure over output dimensions.

Beyond addressing these immediate challenges, SNs open several intriguing research directions. The weight-sharing structure combined with lateral mixing raises fundamental theoretical questions about potential equivariance properties. Just as CNNs exhibit translation equivariance through spatial weight sharing, SNs' sharing across output dimensions with structured mixing may satisfy a related form of equivariance, possibly connected to permutations of output indices or transformations of the function domain. Understanding such properties could provide deeper insight into when and why the architecture succeeds.

The architecture's properties suggest unique opportunities in scientific machine learning where both interpretability and parameter efficiency are valued. One particularly compelling possibility is dimensionality discovery: the architecture's sensitive dependence on input dimension could enable inference of the intrinsic dimensionality of data-generating processes. By training SNs with varying $d_{\mathrm{in}}$ and using model selection criteria that balance fit quality against complexity, researchers might determine the true number of relevant variables in systems where this is unknown, a valuable capability in many scientific domains. The lateral mixing patterns learned by the network could additionally reveal structural relationships between output dimensions, potentially uncovering hidden symmetries or conservation laws in physical systems. Furthermore, the explicit structure of learned splines could enable integration with symbolic regression tools to extract closed-form expressions for the learned $\phi^{(\ell)}$ and $\Phi^{(\ell)}$ functions, potentially revealing underlying mathematical relationships in data.

Architectural enhancements also merit exploration. Unlike KANs where edge pruning can significantly reduce parameters, SNs' vector structure suggests different optimization strategies. Automatic pruning of entire Sprecher blocks based on their contribution to network output could yield more compact architectures. The learned lateral mixing weights could guide this pruning: blocks with near-zero mixing weights might be candidates for removal. For very high-dimensional problems where even $O(N)$ scaling becomes prohibitive, hybrid approaches using low-rank approximations for $\lambda^{(\ell)}$ or replacing splines with small neural sub-networks in certain blocks could maintain efficiency while improving expressivity. Alternatively, hierarchical lateral mixing patterns (e.g., mixing within local groups before global mixing) could provide a middle ground between full connectivity and the current nearest-neighbor approach.

The interaction between lateral mixing and other architectural components deserves systematic investigation. How do different mixing topologies (cyclic, bidirectional, or more complex patterns) interact with network depth? Can theoretical guarantees be established for specific mixing patterns? Is there an optimal ratio between the lateral scale $\tau$ and the main transformation strength? These questions highlight how theorem-inspired architectures enhanced with empirical innovations can open new research avenues beyond simply providing alternative implementations of existing methods.

\section{Conclusion}
We have introduced Sprecher Networks (SNs), a trainable architecture built by re-imagining the components of David Sprecher's 1965 constructive proof as the building blocks for a modern deep learning model. By composing functional blocks that utilize shared monotonic and general splines, learnable mixing weights, explicit shifts, and optionally lateral mixing connections, SNs offer a distinct approach to function approximation that differs fundamentally from MLPs, KANs, and other existing architectures.

The key contributions are demonstrating that Sprecher's shallow, highly-structured formula can be extended into an effective deep architecture with remarkable parameter efficiency: $O(LN + LG)$ compared to MLPs' $O(LN^2)$ or KANs' $O(LN^2G)$, and enabling $O(LN)$ \emph{forward working memory} (peak intermediate storage) through sequential computation strategies compared to MLPs' $O(LN^2)$. This dual efficiency in both parameters and memory comes from adhering to Sprecher's use of weight vectors rather than matrices, representing a strong architectural constraint that may serve as either beneficial inductive bias or limitation depending on the problem domain. The addition of lateral mixing connections provides a parameter-efficient mechanism for intra-block communication, partially addressing the limitations of the constrained weight structure while maintaining the overall efficiency of the architecture.

Our initial demonstrations show SNs can successfully learn diverse functions and achieve competitive performance across synthetic regression benchmarks and PDE-constrained learning problems, with the added benefit of interpretable spline visualizations. The lateral mixing mechanism proves particularly valuable for vector-valued outputs and deeper networks, often improving optimization and convergence behavior even though the learned $\Phi$ splines can remain highly oscillatory. The sequential computation mode enables training of architectures with wide layers that would otherwise exhaust available memory, making SNs particularly suitable for exploring high-capacity models under memory constraints. However, the need for more training iterations in some cases and theoretical gaps regarding deep network universality remain open questions. The theoretical status of lateral mixing --- whether it represents a principled extension of Sprecher's construction or merely an empirical enhancement --- requires further investigation.

Whether SNs prove broadly useful or remain a fascinating special case, they demonstrate the value of mining classical mathematical results for architectural inspiration in modern deep learning. The successful integration of lateral mixing shows how theorem-inspired designs can be enhanced with empirical innovations while maintaining their core theoretical structure. The ability to evaluate SN layers with $O(LN)$ \emph{forward working memory} (peak intermediate storage) distinguishes SNs from other major architectures, suggesting applications in memory-constrained settings and when exploring extremely wide output layers. This synthesis of rigorous mathematical foundations with practical deep learning techniques points toward a promising direction for developing novel architectures that balance theoretical elegance with empirical effectiveness.

\bigskip

\begin{thebibliography}{15}

\bibitem{arnold}
Arnold, V. I. (1963). ``On functions of three variables,'' \emph{Doklady Akademii Nauk SSSR}, \textbf{48}.

\bibitem{cybenko1989approximation}
Cybenko, G. (1989). ``Approximation by superpositions of a sigmoidal function,'' \emph{Mathematics of control, signals and systems}, \textbf{2}(4), 303--314.

\bibitem{goyal2019learning}
Goyal, M., Goyal, R., Lall, B. (2019). ``Learning activation functions: A new paradigm for understanding neural networks,'' arXiv preprint \href{https://arxiv.org/abs/1906.09529}{arXiv:1906.09529}.

\bibitem{haykin1994neural}
Haykin, S. (1994). \emph{Neural networks: a comprehensive foundation}. Prentice Hall PTR.

\bibitem{hornik1989multilayer}
Hornik, K., Stinchcombe, M., White, H. (1989). ``Multilayer feedforward networks are universal approximators,'' \emph{Neural networks}, \textbf{2}(5), 359--366.

\bibitem{kingma2014adam}
Kingma, D. P., Ba, J. (2014). ``Adam: A Method for Stochastic Optimization,'' arXiv preprint \href{https://arxiv.org/abs/1412.6980}{arXiv:1412.6980}.

\bibitem{kolmogorov}
Kolmogorov, A. N. (1957). ``On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition,'' \emph{Doklady Akademii Nauk SSSR}, \textbf{114}(5), 953--956.

\bibitem{koppen}
K{\"o}ppen, M. (2002). ``On the training of a Kolmogorov Network,'' in \emph{Artificial Neural Networks---ICANN 2002: International Conference, Madrid, Spain, August 28--30, 2002 Proceedings 12}, pp. 474--479. Springer.

\bibitem{liu2024kan}
Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Solja\v{c}i\'{c}, M., Hou, T. Y., Tegmark, M. (2025).
``KAN: Kolmogorov-Arnold Networks,'' \emph{ICLR 2025 (to appear)}.
arXiv preprint \href{https://arxiv.org/abs/2404.19756}{arXiv:2404.19756}.

\bibitem{sprecher1965}
Sprecher, D. A. (1965). ``On the Structure of Continuous Functions of Several Variables,'' \emph{Transactions of the American Mathematical Society}, \textbf{115}, 340--355.

\bibitem{zhang2022neural}
Zhang, S., Shen, Z., Yang, H. (2022). ``Neural network architecture beyond width and depth,'' \emph{Advances in Neural Information Processing Systems}, \textbf{35}, 5669--5681.

\end{thebibliography}
\end{document}