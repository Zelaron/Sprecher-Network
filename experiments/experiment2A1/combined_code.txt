Files in /Users/zelaron/Desktop/MoreSprecherStuff/experiment2A:

----------
sn_experiments.py:
----------

"""Command-line interface for Sprecher Network experiments."""

import os
import argparse
import torch
import numpy as np
import matplotlib
from sn_core import (
    train_network,
    get_dataset,
    plot_results,
    plot_loss_curve,
    export_parameters,
    parse_param_types,
)


def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(description="Train Sprecher Networks")

    parser.add_argument(
        "--dataset",
        type=str,
        default="toy_1d_poly",
        help="Dataset name (default: toy_1d_poly)",
    )
    parser.add_argument(
        "--arch",
        type=str,
        default="15,15",
        help="Architecture as comma-separated values (default: 15,15)",
    )
    parser.add_argument(
        "--phi_knots",
        type=int,
        default=100,
        help="Number of knots for phi splines (default: 100)",
    )
    parser.add_argument(
        "--Phi_knots",
        type=int,
        default=100,
        help="Number of knots for Phi splines (default: 100)",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=4000,
        help="Number of training epochs (default: 4000)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=45,
        help="Random seed (default: 45)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="Device: auto, cpu, or cuda (default: auto)",
    )
    parser.add_argument(
        "--save_plots", action="store_true", help="Save plots to files"
    )
    parser.add_argument(
        "--no_show",
        action="store_true",
        help="Don't show plots (useful for batch runs)",
    )
    parser.add_argument(
        "--debug_domains",
        action="store_true",
        help="Enable domain debugging output",
    )
    parser.add_argument(
        "--track_violations",
        action="store_true",
        help="Track domain violations during training",
    )

    # Normalization arguments
    parser.add_argument(
        "--norm_type",
        type=str,
        choices=["none", "batch", "layer"],
        help="Type of normalization to use (default: from CONFIG)",
    )
    parser.add_argument(
        "--norm_position",
        type=str,
        default="after",
        choices=["before", "after"],
        help="Position of normalization relative to blocks (default: after)",
    )
    parser.add_argument(
        "--norm_skip_first",
        action="store_true",
        default=True,
        help="Skip normalization for the first block (default: True)",
    )
    parser.add_argument(
        "--norm_first",
        action="store_true",
        help="Enable normalization for the first block (overrides norm_skip_first)",
    )

    # Debugging/testing arguments
    parser.add_argument(
        "--no_load_best",
        action="store_true",
        help="Don't load best checkpoint at end of training (for debugging)",
    )
    parser.add_argument(
        "--bn_recalc_on_load",
        action="store_true",
        help="Recalculate BatchNorm statistics when loading best checkpoint (default: use saved stats)",
    )

    # Feature control arguments
    parser.add_argument(
        "--no_residual",
        action="store_true",
        help="Disable residual connections (default: enabled)",
    )
    parser.add_argument(
        "--residual_style",
        type=str,
        default=None,
        choices=["node", "linear", "standard", "matrix"],
        help=(
            "Residual style: 'node' (original) or 'linear' (standard). "
            "Aliases: 'standard'/'matrix' -> 'linear'."
        ),
    )
    parser.add_argument(
        "--no_norm",
        action="store_true",
        help="Disable normalization (default: enabled with batch norm)",
    )
    parser.add_argument(
        "--use_advanced_scheduler",
        action="store_true",
        help="Use PlateauAwareCosineAnnealingLR scheduler (default: disabled)",
    )

    # Lateral mixing arguments
    parser.add_argument(
        "--no_lateral",
        action="store_true",
        help="Disable lateral mixing connections (default: enabled)",
    )
    parser.add_argument(
        "--lateral_type",
        type=str,
        default=None,
        choices=["cyclic", "bidirectional"],
        help="Type of lateral mixing (default: from CONFIG)",
    )

    # Parameter export argument
    parser.add_argument(
        "--export_params",
        nargs="?",
        const="all",
        default=None,
        help=(
            "Export parameters to text file. Options: all, or comma-separated: "
            "lambda,eta,spline,residual,codomain,norm,output,lateral"
        ),
    )

    # Memory optimization arguments
    parser.add_argument(
        "--low_memory_mode",
        action="store_true",
        help="Use memory-efficient computation (O(N) memory instead of O(N²))",
    )
    parser.add_argument(
        "--memory_debug",
        action="store_true",
        help="Print memory usage statistics during forward pass",
    )

    return parser.parse_args()


def _parse_architecture(arch_str: str):
    """Parse architecture string into a list of ints (robust to spaces/empties)."""
    if not arch_str:
        return []
    parts = [p.strip() for p in arch_str.split(",")]
    parts = [p for p in parts if p]
    try:
        return [int(p) for p in parts]
    except ValueError:
        raise ValueError(f"Invalid --arch value '{arch_str}'. Use comma-separated integers, e.g. '15,15'.")


def get_config_suffix(args, CONFIG):
    """Build filename suffix for non-default configurations."""
    parts = []

    # Check normalization (default is enabled with batch)
    if args.no_norm or (hasattr(args, "norm_type") and args.norm_type == "none"):
        parts.append("NoNorm")
    elif hasattr(args, "norm_type") and args.norm_type and args.norm_type not in [
        "none",
        "batch",
    ]:
        parts.append(f"Norm{args.norm_type.capitalize()}")

    # Check residuals (default is enabled)
    if not CONFIG.get("use_residual_weights", True):
        parts.append("NoResidual")
    else:
        # Include style if not default 'node'
        style = CONFIG.get("residual_style", "node")
        if style not in (None, "node"):
            # Compact tag
            parts.append("ResLinear")

    # Check lateral mixing (default is enabled)
    if not CONFIG.get("use_lateral_mixing", True):
        parts.append("NoLateral")
    elif CONFIG.get("lateral_mixing_type", "cyclic") != "cyclic":
        parts.append(f"Lateral{CONFIG['lateral_mixing_type'].capitalize()}")

    # Check scheduler (default is disabled)
    if CONFIG.get("use_advanced_scheduler", False):
        parts.append("AdvScheduler")

    # Check memory mode
    if CONFIG.get("low_memory_mode", False):
        parts.append("LowMem")

    # Join with dashes
    return "-" + "-".join(parts) if parts else ""


def profile_memory_usage(model, x_sample):
    """Profile memory usage of forward pass."""
    import torch.cuda

    if x_sample.is_cuda:
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()
        start_memory = torch.cuda.memory_allocated()

        with torch.no_grad():
            _ = model(x_sample)

        torch.cuda.synchronize()
        peak_memory = torch.cuda.max_memory_allocated()
        end_memory = torch.cuda.memory_allocated()

        print(f"Memory usage:")
        print(f"  Start: {start_memory / 1024**2:.2f} MB")
        print(f"  Peak:  {peak_memory / 1024**2:.2f} MB")
        print(f"  End:   {end_memory / 1024**2:.2f} MB")
        print(f"  Delta: {(peak_memory - start_memory) / 1024**2:.2f} MB")
    else:
        print("Memory profiling only available for CUDA tensors")


def verify_memory_efficient_mode(device="cpu"):
    """Verify that memory-efficient mode produces identical results."""
    from sn_core import CONFIG
    from sn_core.model import SprecherLayerBlock

    print("\nVerifying memory-efficient mode...")
    torch.manual_seed(42)

    # Create a test layer with reasonable dimensions
    layer = SprecherLayerBlock(d_in=100, d_out=200, layer_num=0).to(device)
    x = torch.randn(32, 100, device=device)  # batch_size=32, d_in=100

    # Get output with original method
    CONFIG["low_memory_mode"] = False
    with torch.no_grad():
        output_original = layer._forward_original(x, None)

    # Get output with memory-efficient method
    CONFIG["low_memory_mode"] = True
    with torch.no_grad():
        output_efficient = layer._forward_memory_efficient(x, None)

    # Check if outputs are identical (within floating point precision)
    max_diff = torch.abs(output_original - output_efficient).max().item()
    mean_diff = torch.abs(output_original - output_efficient).mean().item()

    print(f"Maximum difference: {max_diff:.2e}")
    print(f"Mean difference: {mean_diff:.2e}")

    if max_diff < 1e-6:
        print("✓ Memory-efficient mode produces mathematically identical results")
    else:
        print(f"✗ WARNING: Outputs differ by {max_diff:.2e}")

    # Profile memory usage if on CUDA
    if device == "cuda" and torch.cuda.is_available():
        print("\nMemory comparison:")
        CONFIG["low_memory_mode"] = False
        print("Original mode:")
        profile_memory_usage(layer, x)

        CONFIG["low_memory_mode"] = True
        print("\nMemory-efficient mode:")
        profile_memory_usage(layer, x)

    # Reset CONFIG
    CONFIG["low_memory_mode"] = False

    return max_diff < 1e-6


def main():
    """Main training script."""
    args = parse_args()

    # If --no_show is used (e.g., in batch runs), force the 'Agg' backend upfront.
    if args.no_show:
        matplotlib.use("Agg")
        print("Using non-interactive 'Agg' backend for plotting (as requested by --no_show).")

    # Now, import pyplot. We will wrap the first plotting call in a try-except
    # block to handle systems without a working GUI backend.
    import matplotlib.pyplot as plt

    # Parse architecture (robust)
    architecture = _parse_architecture(args.arch)

    # Get dataset
    dataset = get_dataset(args.dataset)

    # Determine device (robust fallback if CUDA not available)
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    elif args.device == "cuda" and not torch.cuda.is_available():
        print("WARNING: --device cuda was requested but CUDA is not available; falling back to CPU.")
        device = "cpu"
    else:
        device = args.device

    # Enable domain debugging if requested
    from sn_core.config import CONFIG

    if args.debug_domains:
        CONFIG["debug_domains"] = True
    if args.track_violations:
        CONFIG["track_domain_violations"] = True
        print("Domain violation tracking enabled.")

    # Handle memory optimization flags
    if args.low_memory_mode:
        CONFIG["low_memory_mode"] = True
        print("Low memory mode: ENABLED (sequential output computation)")

        # Run verification test if requested
        if args.memory_debug:
            verify_success = verify_memory_efficient_mode(device)
            if not verify_success:
                print("WARNING: Memory-efficient mode verification failed!")
                print("Continuing anyway...")

    if args.memory_debug:
        CONFIG["memory_debug"] = True
        print("Memory debugging: ENABLED")

    # Handle new/updated feature control flags
    # Residuals on/off + style
    if args.no_residual:
        CONFIG["use_residual_weights"] = False
        if args.residual_style is not None:
            print("NOTE: --residual_style is ignored because residuals are disabled via --no_residual.")
    else:
        # Only apply style if residuals are enabled
        if args.residual_style is not None:
            style = args.residual_style.lower()
            # Normalize aliases
            if style in ("standard", "matrix"):
                style = "linear"
            CONFIG["residual_style"] = style  # used by model/train

    # Normalization on/off
    if args.no_norm:
        CONFIG["use_normalization"] = False
    if args.use_advanced_scheduler:
        CONFIG["use_advanced_scheduler"] = True

    # Lateral mixing configuration
    if args.no_lateral:
        CONFIG["use_lateral_mixing"] = False
    if args.lateral_type is not None:
        CONFIG["lateral_mixing_type"] = args.lateral_type

    # Parameter export configuration
    if args.export_params is not None:
        CONFIG["export_params"] = args.export_params

    # Determine effective normalization settings
    if CONFIG.get("use_normalization", True) and not args.no_norm:
        # Use CONFIG defaults or argparser overrides
        if args.norm_type == "none":
            effective_norm_type = "none"
        elif args.norm_type is not None:
            effective_norm_type = args.norm_type
        else:
            effective_norm_type = CONFIG.get("norm_type", "batch")
    else:
        # Normalization is disabled
        effective_norm_type = "none"

    # Summarize run configuration
    print(f"Using device: {device}")
    print(f"Dataset: {args.dataset}")
    print(f"Architecture: {architecture}")
    print(f"phi knots: {args.phi_knots}, Phi knots: {args.Phi_knots}")
    print(f"Epochs: {args.epochs}")
    print(f"Seed: {args.seed}")
    print(f"Theoretical domains: {CONFIG.get('use_theoretical_domains', True)}")
    print(f"Domain safety margin: {CONFIG.get('domain_safety_margin', 0.0)}")
    print(
        f"Residual connections: {'enabled' if CONFIG.get('use_residual_weights', True) else 'disabled'}"
    )
    if CONFIG.get("use_residual_weights", True):
        print(f"  Residual style: {CONFIG.get('residual_style', 'node')}")
    print(f"Lateral mixing: {'enabled' if CONFIG.get('use_lateral_mixing', True) else 'disabled'}")
    if CONFIG.get("use_lateral_mixing", True):
        print(f"  Type: {CONFIG.get('lateral_mixing_type', 'cyclic')}")
    if CONFIG.get("low_memory_mode", False):
        print(f"Memory mode: LOW MEMORY (sequential computation)")
    else:
        print(f"Memory mode: STANDARD (parallel computation)")
    if effective_norm_type != "none":
        norm_position = args.norm_position if args.norm_position else CONFIG.get("norm_position", "after")
        # Handle the --norm_first flag
        if args.norm_first:
            norm_skip_first = False
        else:
            norm_skip_first = (
                args.norm_skip_first if hasattr(args, "norm_skip_first") else CONFIG.get("norm_skip_first", True)
            )
        print(f"Normalization: {effective_norm_type} (position: {norm_position}, skip_first: {norm_skip_first})")
    else:
        print("Normalization: disabled")
    print(
        f"Scheduler: {'PlateauAwareCosineAnnealingLR' if CONFIG.get('use_advanced_scheduler', False) else 'Adam (fixed LR)'}"
    )

    # Print parameter export configuration
    if CONFIG.get("export_params"):
        if CONFIG["export_params"] == "all" or CONFIG["export_params"] is True:
            print("Parameter export: all parameters")
        else:
            param_types = parse_param_types(CONFIG["export_params"])
            print(f"Parameter export: {', '.join(param_types)}")

    if args.no_load_best:
        print("WARNING: Best model loading disabled (--no_load_best)")
    if args.bn_recalc_on_load:
        print("BatchNorm stats will be recalculated when loading best checkpoint")
    print()

    # Determine final normalization parameters
    if effective_norm_type != "none":
        final_norm_position = args.norm_position if args.norm_position else CONFIG.get("norm_position", "after")
        # Handle the --norm_first flag for final parameters
        if args.norm_first:
            final_norm_skip_first = False
        else:
            final_norm_skip_first = (
                args.norm_skip_first if hasattr(args, "norm_skip_first") else CONFIG.get("norm_skip_first", True)
            )
    else:
        final_norm_position = args.norm_position  # Keep for backward compatibility
        final_norm_skip_first = args.norm_skip_first

    # Determine residual style override to pass to train()
    residual_style_override = None
    if not CONFIG.get("use_residual_weights", True):
        residual_style_override = None  # style irrelevant if residuals disabled
    else:
        residual_style_override = CONFIG.get("residual_style", "node")

    # Train network - returns a snapshot and losses
    plotting_snapshot, losses = train_network(
        dataset=dataset,
        architecture=architecture,
        total_epochs=args.epochs,
        print_every=max(1, args.epochs // 10),
        device=device,
        phi_knots=args.phi_knots,
        Phi_knots=args.Phi_knots,
        seed=args.seed,
        norm_type=effective_norm_type,
        norm_position=final_norm_position,
        norm_skip_first=final_norm_skip_first,
        no_load_best=args.no_load_best,
        bn_recalc_on_load=args.bn_recalc_on_load,
        residual_style=residual_style_override,
    )

    # Extract components from the snapshot
    model = plotting_snapshot["model"]
    x_train = plotting_snapshot["x_train"]
    y_train = plotting_snapshot["y_train"]
    layers = model.layers

    # Verify the snapshot works correctly with multiple computations
    print("\nVerifying plotting snapshot consistency...")
    print("Computing loss multiple times to ensure perfect reproducibility:")

    losses_verification = []
    for i in range(5):
        with torch.no_grad():
            snapshot_output = model(x_train)
            snapshot_loss = torch.mean((snapshot_output - y_train) ** 2).item()
            losses_verification.append(snapshot_loss)
            print(f"  Computation {i+1}: loss = {snapshot_loss:.4e}")

    print(f"\nSaved loss from snapshot: {plotting_snapshot['loss']:.4e}")
    print(f"Mean of verifications: {np.mean(losses_verification):.4e}")
    print(f"Std of verifications: {np.std(losses_verification):.4e}")

    max_diff = max(abs(l - plotting_snapshot["loss"]) for l in losses_verification)
    print(f"Max difference from saved: {max_diff:.4e}")

    if max_diff < 1e-8:
        print("[OK] Perfect consistency achieved! The snapshot is completely isolated.")

    # Keep model in training mode for consistency with checkpoint
    # model.eval()  # Commented out - we maintain the mode from training

    # Print final domain information
    print("\nFinal domain ranges:")
    for idx, layer in enumerate(layers):
        print(f"Layer {idx}:")
        print(f"  phi domain: [{layer.phi.in_min:.3f}, {layer.phi.in_max:.3f}]")
        print(f"  Phi domain: [{layer.Phi.in_min:.3f}, {layer.Phi.in_max:.3f}]")
        if hasattr(layer, "input_range") and layer.input_range is not None:
            print(f"  Input range: {layer.input_range}")
        if hasattr(layer, "output_range") and layer.output_range is not None:
            print(f"  Output range: {layer.output_range}")

    # Memory usage profiling if requested
    if CONFIG.get("memory_debug", False) and device == "cuda":
        print("\nFinal memory usage profile:")
        profile_memory_usage(model, x_train)

    # DEBUG: Model structure check before plotting
    print("\nDEBUG: Model structure check before plotting:")
    print(f"Model type: {type(model)}")
    print(f"Model has {len(model.layers)} Sprecher layers")
    print(f"Model has {len(model.norm_layers) if hasattr(model, 'norm_layers') else 0} norm layers")

    # Test the full model vs just the layers
    # Create test input with correct dimensions for the dataset
    if dataset.input_dim == 1:
        test_input = torch.linspace(0, 1, 5).unsqueeze(1).to(device)
    else:
        # For multi-dimensional inputs, create random test points
        test_input = torch.rand(5, dataset.input_dim).to(device)

    with torch.no_grad():
        full_model_output = model(test_input)
        print(f"Full model output shape: {full_model_output.shape}")
        print(f"Full model output: {full_model_output.flatten().cpu().numpy()}")

    # Also check if we're in eval mode
    print(f"Model training mode: {model.training}")

    # DEBUG: Testing with same points as checkpoint loading:
    print("\nDEBUG: Testing with same points as checkpoint loading:")
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    n_samples = 32 if dataset.input_dim == 1 else 32 * 32
    x_train_debug, _ = dataset.sample(n_samples, device)
    with torch.no_grad():
        output_debug = model(x_train_debug[:5])
        print(f"Output on training points: {output_debug.cpu().numpy().flatten()[:5]}")

    print("=" * 60)

    # Export parameters if requested
    if CONFIG.get("export_params"):
        # Create params directory if saving
        os.makedirs(CONFIG.get("export_params_dir", "params"), exist_ok=True)

        # Build filename similar to plots
        prefix = "OneVar" if dataset.input_dim == 1 else f"{dataset.input_dim}Vars"
        arch_str = "-".join(map(str, architecture)) if len(architecture) > 0 else "None"
        config_suffix = get_config_suffix(args, CONFIG)
        params_filename = (
            f"{prefix}-{args.dataset}-{arch_str}-{args.epochs}-epochs-"
            f"outdim{dataset.output_dim}{config_suffix}-params.txt"
        )
        params_path = os.path.join(CONFIG.get("export_params_dir", "params"), params_filename)

        # Prepare dataset info
        dataset_info = {
            "name": args.dataset,
            "architecture": architecture,
            "input_dim": dataset.input_dim,
            "output_dim": dataset.output_dim,
            "epochs": args.epochs,
        }

        # Prepare checkpoint info
        checkpoint_info = {
            "epoch": plotting_snapshot.get("epoch", "Unknown"),
            "loss": plotting_snapshot.get("loss", "Unknown"),
        }

        # Export parameters
        export_parameters(
            model,
            CONFIG["export_params"],
            params_path,
            dataset_info=dataset_info,
            checkpoint_info=checkpoint_info,
        )

    # --- Graceful Fallback Plotting Logic ---
    try:
        # Create plots directory if saving
        if args.save_plots:
            os.makedirs("plots", exist_ok=True)

        # Plot results
        prefix = "OneVar" if dataset.input_dim == 1 else f"{dataset.input_dim}Vars"
        arch_str = "-".join(map(str, architecture)) if len(architecture) > 0 else "None"
        config_suffix = get_config_suffix(args, CONFIG)
        filename = (
            f"{prefix}-{args.dataset}-{arch_str}-{args.epochs}-epochs-"
            f"outdim{dataset.output_dim}{config_suffix}.png"
        )

        save_path = os.path.join("plots", filename) if args.save_plots else None
        fig_results = plot_results(
            model, layers, dataset, save_path, x_train=x_train, y_train=y_train
        )

        # Plot loss curve
        loss_filename = f"loss-{args.dataset}-{arch_str}-{args.epochs}-epochs{config_suffix}.png"
        loss_save_path = os.path.join("plots", loss_filename) if args.save_plots else None
        plot_loss_curve(losses, loss_save_path)

        # Show plots if requested and not in Agg mode
        if not args.no_show:
            print("Displaying plots. Close the plot windows to exit.")
            plt.show()
        else:
            plt.close("all")  # Free up memory in non-interactive mode

    except Exception as e:
        # This block will catch the TclError on Windows or other GUI-related errors
        print("\n" + "=" * 60)
        print("WARNING: A plotting error occurred.")
        print(f"Error type: {type(e).__name__}")
        print("The default plotting backend on your system has failed.")
        print("This is common on systems without a configured GUI toolkit.")
        print("\nSwitching to the reliable 'Agg' backend to proceed.")
        print("=" * 60 + "\n")

        # Set the backend and re-run the plotting logic
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt  # re-import after backend change

        # Ensure we save plots if this fallback is triggered
        if not args.save_plots:
            print("Plots could not be shown interactively. Enabling file saving automatically.")
            args.save_plots = True

        # Re-run plotting logic with the safe backend
        os.makedirs("plots", exist_ok=True)

        prefix = "OneVar" if dataset.input_dim == 1 else f"{dataset.input_dim}Vars"
        arch_str = "-".join(map(str, architecture)) if len(architecture) > 0 else "None"
        config_suffix = get_config_suffix(args, CONFIG)
        filename = (
            f"{prefix}-{args.dataset}-{arch_str}-{args.epochs}-epochs-"
            f"outdim{dataset.output_dim}{config_suffix}.png"
        )
        save_path = os.path.join("plots", filename)

        plot_results(model, layers, dataset, save_path, x_train=x_train, y_train=y_train)

        loss_filename = f"loss-{args.dataset}-{arch_str}-{args.epochs}-epochs{config_suffix}.png"
        loss_save_path = os.path.join("plots", loss_filename)
        plot_loss_curve(losses, loss_save_path)

        plt.close("all")
        print("Plots were successfully saved to the 'plots' directory.")


if __name__ == "__main__":
    main()

----------
sn_core/config.py:
----------

"""Configuration for Sprecher Networks.

This module holds all run-time configuration used across the project.
"""

# Core configuration
CONFIG = {
    # ------------------------------
    # Model settings
    # ------------------------------
    'train_phi_codomain': True,   # Whether to train Φ codomain parameters (cc, cr)
    'use_residual_weights': True, # Toggle residual connections globally
    'residual_style': 'node',     # 'node' (default, original) or 'linear' (standard residuals)
    'seed': 45,

    # ------------------------------
    # Training settings
    # ------------------------------
    'use_advanced_scheduler': False,
    'weight_decay': 1e-6,
    'max_grad_norm': 1.0,

    # ------------------------------
    # Normalization settings
    # ------------------------------
    'use_normalization': True,    # Global switch for normalization
    'norm_type': 'batch',         # Default normalization type when enabled: 'none', 'batch', 'layer'
    'norm_position': 'after',     # 'before' or 'after' each block
    'norm_skip_first': True,      # Skip normalization for the first block

    # ------------------------------
    # Scheduler settings (used when use_advanced_scheduler=True)
    # ------------------------------
    'scheduler_type': 'plateau_cosine',  # Currently informational
    'scheduler_base_lr': 1e-4,
    'scheduler_max_lr': 1e-2,
    'scheduler_patience': 500,
    'scheduler_threshold': 1e-5,

    # ------------------------------
    # Domain settings
    # ------------------------------
    'use_theoretical_domains': True, # Use theoretical domain computation
    'domain_safety_margin': 0.0,     # Safety margin around computed domains
    'debug_domains': False,          # Print domain information during training

    # ------------------------------
    # Domain violation tracking
    # ------------------------------
    'track_domain_violations': False,     # Track out-of-domain evaluations
    'verbose_domain_violations': False,   # Print violations as they occur

    # ------------------------------
    # Checkpoint & debugging
    # ------------------------------
    'debug_checkpoint_loading': False,    # Detailed logs during checkpoint save/load

    # ------------------------------
    # Parameter export
    # ------------------------------
    'export_params': False,               # False, True/'all', or comma-separated list
    'export_params_dir': 'params',

    # ------------------------------
    # Lateral mixing settings
    # ------------------------------
    'use_lateral_mixing': True,     # Enable intra-block lateral connections
    'lateral_mixing_type': 'cyclic',# 'cyclic' or 'bidirectional'
    'lateral_scale_init': 0.1,      # Initial global scale for lateral mixing
    'lateral_weight_init': 0.1,     # Initial per-channel mixing weights

    # ------------------------------
    # Memory optimization
    # ------------------------------
    'low_memory_mode': False,   # O(B × max(d_in, d_out)) memory forward
    'memory_debug': False,      # Print CUDA memory usage profiles when enabled
}

# MNIST-specific defaults (used by the MNIST example script, kept unchanged)
MNIST_CONFIG = {
    'architecture': [100],
    'phi_knots': 50,
    'Phi_knots': 50,
    'learning_rate': 0.001,
    'weight_decay': 1e-6,
    'batch_size': 64,
    'epochs': 3,
    'model_file': 'sn_mnist_model.pth',
    'data_directory': './data',
}

# Mathematical constants (not configuration)
Q_VALUES_FACTOR = 1.0

# Parameter categories for export
PARAM_CATEGORIES = [
    'lambda', 'eta', 'spline', 'residual',
    'codomain', 'norm', 'output', 'lateral'
]

----------
sn_core/plotting.py:
----------

"""Plotting utilities for Sprecher Networks."""

import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401  (imported for side-effect: 3D projection)
from contextlib import contextmanager

from .config import CONFIG
from .train import (
    has_batchnorm,
    freeze_bn_running_stats,
    use_batch_stats_without_updating_bn,
)


# ---------------------------------------------------------------------
# Small helper: run a forward pass for visualization WITHOUT mutating BN
# ---------------------------------------------------------------------
@contextmanager
def nonmutating_infer(model):
    """
    Context manager for visualization/evaluation forwards that:
      - does NOT change BN running_mean/var or num_batches_tracked
      - does NOT change the model's global training/eval mode
      - runs under torch.no_grad()

    Behavior:
      * If the model has BatchNorms and is currently in TRAIN mode:
          use per-batch stats WITHOUT updating running buffers.
      * If the model has BatchNorms and is currently in EVAL mode:
          just freeze/restore BN buffers exactly (no-op for stats).
      * If there are no BatchNorms:
          just no_grad().
    """
    bn_present = has_batchnorm(model)
    if not bn_present:
        with torch.no_grad():
            yield
        return

    if model.training:
        # Keep train semantics for BN (batch stats) but do NOT update buffers.
        with use_batch_stats_without_updating_bn(model), torch.no_grad():
            yield
    else:
        # Eval mode already uses running stats; just make sure nothing is mutated.
        with freeze_bn_running_stats(model), torch.no_grad():
            yield


def plot_network_structure_ax(ax, layers, input_dim, final_dim=1):
    """Plot network structure diagram."""
    num_blocks = len(layers)
    if num_blocks == 0:
        ax.text(0.5, 0.5, "No network", ha='center', va='center', fontsize=12)
        ax.axis('off')
        return

    # Check if we need an additional output node for summing
    need_sum_node = any(layer.is_final and final_dim == 1 for layer in layers)
    total_columns = num_blocks + 1 + (1 if need_sum_node else 0)

    # Create evenly spaced x-coordinates for all nodes including potential sum node
    layer_x = np.linspace(0.2, 0.8, total_columns)

    if input_dim is None and num_blocks > 0:
        input_dim = layers[0].d_in

    if input_dim == 1:
        input_y = [0.5]
    else:
        input_y = np.linspace(0.2, 0.8, input_dim)

    # Plot input nodes
    for i, y in enumerate(input_y):
        ax.scatter(layer_x[0], y, c='black', s=100)
        ax.text(layer_x[0] - 0.03, y, f'$x_{{{i+1}}}$', ha='right', fontsize=8)

    prev_y = input_y
    sum_node_placed = False

    for block_index, block in enumerate(layers):
        j = block_index + 1
        d_out = block.d_out
        new_x = layer_x[block_index + 1]
        color = 'red' if block.is_final else 'blue'

        if d_out == 1:
            new_y = [0.5]
        else:
            new_y = np.linspace(0.2, 0.8, d_out)

        # Plot this layer's nodes
        for ny in new_y:
            ax.scatter(new_x, ny, c=color, s=100)

        # Connect previous layer to this layer
        for py in prev_y:
            for ny in new_y:
                ax.plot([layer_x[block_index], new_x], [py, ny], 'k-', alpha=0.5)

        # Place φ and Φ labels
        mid_x = 0.5 * (layer_x[block_index] + new_x)
        ax.text(mid_x, 0.14, f"$\\phi^{{({j})}}$", ha='center', fontsize=9, color='green')
        ax.text(mid_x, 0.09, f"$\\Phi^{{({j})}}$", ha='center', fontsize=9, color='green')

        # Handle final output/sum node if needed
        if block.is_final and final_dim == 1 and not sum_node_placed:
            sum_x = layer_x[block_index + 2]  # Use next x position from our evenly spaced grid
            sum_y = 0.5
            ax.scatter(sum_x, sum_y, c='red', s=100)
            for ny in new_y:
                ax.plot([new_x, sum_x], [ny, sum_y], 'k-', alpha=0.5)
            prev_y = [sum_y]
            sum_node_placed = True
        else:
            prev_y = new_y

    ax.set_title("Network Structure", fontsize=11)
    ax.axis('off')


def plot_high_dim_function(ax, model, dataset, device):
    """Plot high-dimensional function using dimension slices or statistics."""
    print(f"DEBUG in plot_high_dim_function: model type = {type(model)}")
    print(f"DEBUG in plot_high_dim_function: model.training = {model.training}")

    input_dim = dataset.input_dim
    output_dim = dataset.output_dim

    if input_dim <= 10:
        # For moderately high dimensions, plot 1D slices
        num_slices = min(input_dim, 4)  # Limit to 4 for clarity

        # Create base input with all dimensions at 0.5
        x_base = torch.full((100, input_dim), 0.5, device=device)
        x_vals = torch.linspace(0, 1, 100, device=device)

        # Define base colors for each output dimension
        base_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']

        # Store lines for legend ordering
        pred_lines = []
        true_lines = []
        pred_labels = []
        true_labels = []

        # For single output, use different approach
        if output_dim == 1:
            colors = plt.cm.viridis(np.linspace(0, 1, num_slices))
            for i in range(num_slices):
                # Vary dimension i
                x_test = x_base.clone()
                for j in range(100):
                    x_test[j, i] = x_vals[j]

                # Get predictions and true values
                with nonmutating_infer(model):
                    y_pred = model(x_test)
                y_true = dataset.evaluate(x_test)

                # Plot with distinct colors
                pred_line, = ax.plot(x_vals.cpu(), y_pred.cpu(), '-',
                                     color=colors[i], label=f'Pred (vary dim {i})',
                                     linewidth=2)
                true_line, = ax.plot(x_vals.cpu(), y_true.cpu(), '--',
                                     color=colors[i], label=f'True (vary dim {i})',
                                     linewidth=1.5, alpha=0.7)
                pred_lines.append(pred_line)
                true_lines.append(true_line)
        else:
            # Multiple outputs - use color families
            for out_idx in range(min(output_dim, 3)):  # Show first 3 outputs
                base_color = base_colors[out_idx % len(base_colors)]

                # Create color variations for this output dimension
                if base_color == 'tab:blue':
                    color_variations = plt.cm.Blues(np.linspace(0.4, 0.9, num_slices))
                elif base_color == 'tab:orange':
                    color_variations = plt.cm.Oranges(np.linspace(0.4, 0.9, num_slices))
                elif base_color == 'tab:green':
                    color_variations = plt.cm.Greens(np.linspace(0.4, 0.9, num_slices))
                else:
                    color_variations = plt.cm.Greys(np.linspace(0.4, 0.9, num_slices))

                # Plot all input dimension variations for this output
                for i in range(num_slices):
                    # Vary dimension i
                    x_test = x_base.clone()
                    for j in range(100):
                        x_test[j, i] = x_vals[j]

                    # Get predictions and true values
                    with nonmutating_infer(model):
                        y_pred = model(x_test)
                    y_true = dataset.evaluate(x_test)

                    # Plot predictions with color variation
                    pred_line, = ax.plot(x_vals.cpu(), y_pred[:, out_idx].cpu(), '-',
                                         color=color_variations[i],
                                         linewidth=1.5,
                                         label=f'Pred out{out_idx} (vary in{i})')
                    pred_lines.append(pred_line)
                    pred_labels.append(f'Pred out{out_idx} (vary in{i})')

                    # Plot true values with same color but dashed
                    true_line, = ax.plot(x_vals.cpu(), y_true[:, out_idx].cpu(), '--',
                                         color=color_variations[i],
                                         linewidth=1,
                                         alpha=0.8,
                                         label=f'True out{out_idx} (vary in{i})')
                    true_lines.append(true_line)
                    true_labels.append(f'True out{out_idx} (vary in{i})')

            # Create custom legend with better organization
            # Group by output dimension
            handles = []
            labels = []
            for out_idx in range(min(output_dim, 3)):
                # Add predicted lines for this output
                for i in range(num_slices):
                    idx = out_idx * num_slices + i
                    if idx < len(pred_lines):
                        handles.append(pred_lines[idx])
                        labels.append(pred_labels[idx])
                # Add true lines for this output
                for i in range(num_slices):
                    idx = out_idx * num_slices + i
                    if idx < len(true_lines):
                        handles.append(true_lines[idx])
                        labels.append(true_labels[idx])
                # Add spacer if not last output
                if out_idx < min(output_dim, 3) - 1:
                    handles.append(plt.Line2D([0], [0], color='none'))
                    labels.append('')

            ax.legend(handles, labels, fontsize=7, ncol=2, loc='center left', bbox_to_anchor=(1, 0.5))

        ax.set_xlabel('Variable value')
        ax.set_ylabel('Output')
        ax.set_title(f'1D Slices - Varying input dims 0-{num_slices-1}')
        ax.grid(True, alpha=0.3)

        # Adjust layout to prevent legend cutoff
        plt.tight_layout()

    else:
        # For very high dimensions, plot statistics
        n_samples = 1000
        x_test, y_true = dataset.sample(n_samples, device)

        with nonmutating_infer(model):
            y_pred = model(x_test)

        # Convert to numpy
        y_true_np = y_true.cpu().numpy().flatten()
        y_pred_np = y_pred.cpu().numpy().flatten()

        # Calculate appropriate bin range
        y_min = min(y_true_np.min(), y_pred_np.min())
        y_max = max(y_true_np.max(), y_pred_np.max())

        # Add some padding
        y_range = y_max - y_min
        if y_range > 0:
            y_min -= 0.1 * y_range
            y_max += 0.1 * y_range
        else:
            y_min -= 0.1
            y_max += 0.1

        bins = np.linspace(y_min, y_max, 50)

        # Plot histograms
        ax.hist(y_true_np, bins=bins, alpha=0.5, label='True', density=True)
        ax.hist(y_pred_np, bins=bins, alpha=0.5, label='Predicted', density=True)

        # Add vertical lines for means
        ax.axvline(y_true_np.mean(), linestyle='--', linewidth=2, alpha=0.8)
        ax.axvline(y_pred_np.mean(), linestyle='--', linewidth=2, alpha=0.8)

        ax.set_xlabel('Output value')
        ax.set_ylabel('Density')
        ax.set_title(f'Output Distribution ({input_dim}D input)\nTrue mean: {y_true_np.mean():.3f}, Pred mean: {y_pred_np.mean():.3f}')
        ax.legend()
        ax.grid(True, alpha=0.3)


def plot_results(model, layers, dataset=None, save_path=None,
                 plot_network=True, plot_function=True, plot_splines=True,
                 title_suffix="", x_train=None, y_train=None):
    """
    Plot network structure, splines, and function approximation.

    Args:
        model: Trained Sprecher network (or KAN)
        layers: Model layers
        dataset: Dataset instance (optional, for function comparison)
        save_path: Path to save figure (optional)
        plot_network: Whether to plot network structure (default: True)
        plot_function: Whether to plot function comparison (default: True)
        plot_splines: Whether to plot splines (default: True)
        title_suffix: Additional text to add to figure title (default: "")
        x_train: Training input data (optional, for consistent evaluation)
        y_train: Training target data (optional, for consistent evaluation)

    Returns:
        fig: Matplotlib figure
    """
    print(f"\nDEBUG in plot_results: model type = {type(model)}")
    print(f"DEBUG in plot_results: model.training = {model.training}")
    print(f"DEBUG in plot_results: number of layers = {len(layers)}")

    # Determine the target device for plotting. Use the device of the first parameter.
    # If the model has no parameters, default to CPU.
    try:
        device = next(model.parameters()).device
    except StopIteration:
        device = torch.device('cpu')  # Fallback if model has no parameters

    # Ensure the entire model (including buffers) is on that device.
    model.to(device)

    if len(layers) > 0:
        input_dim = layers[0].d_in
    else:
        input_dim = 1
    num_blocks = len(layers)
    final_dim = getattr(model, 'final_dim', 1)

    # Calculate layout based on what we're plotting
    num_rows = 0
    if plot_network or plot_splines:
        num_rows += 1
    if plot_function and dataset is not None:
        num_rows += 1

    if num_rows == 0:
        print("Nothing to plot!")
        return None

    # Calculate columns for top row
    if plot_network and plot_splines:
        total_cols = 1 + 2 * num_blocks
    elif plot_splines:
        total_cols = 2 * num_blocks
    else:
        total_cols = 1

    plt.rcParams.update({'xtick.labelsize': 8, 'ytick.labelsize': 8})

    # Calculate figure size
    if plot_network and plot_splines:
        fig_width = 4 * total_cols * 0.75
    else:
        fig_width = max(12, 3 * total_cols)  # Larger cells when no network plot

    fig_height = 7 * num_rows if plot_function else 5

    fig = plt.figure(figsize=(fig_width, fig_height))

    # Add title with suffix if provided - but don't add it for MNIST to avoid overlap
    if title_suffix and "MNIST" not in title_suffix:
        fig.suptitle(title_suffix, fontsize=14, y=0.98)

    # Create layout based on what we're plotting
    if num_rows == 2:
        # Both splines/network and function plots
        gs_top = gridspec.GridSpec(1, total_cols)
        gs_top.update(left=0.05, right=0.95, top=0.95, bottom=0.55, wspace=0.4)
        gs_bottom_top = 0.45
    else:
        # Only splines/network or only function
        gs_top = gridspec.GridSpec(1, total_cols)
        gs_top.update(left=0.05, right=0.95, top=0.95, bottom=0.05, wspace=0.4)
        gs_bottom_top = None

    # Plot network structure and/or splines in top row
    col_offset = 0
    if plot_network and (plot_splines or not plot_function):
        # Only plot network structure if input dim is reasonable
        if input_dim <= 10:
            ax_net = fig.add_subplot(gs_top[0, 0])
            plot_network_structure_ax(ax_net, layers, input_dim, final_dim)
            col_offset = 1

    if plot_splines:
        for i, layer in enumerate(layers):
            j = i + 1

            # When no network plot, use sequential columns starting from 0
            if plot_network and input_dim <= 10:
                col_phi = 2 * i + 1
                col_Phi = 2 * i + 2
            else:
                col_phi = 2 * i
                col_Phi = 2 * i + 1

            # Plot phi spline
            ax_phi = fig.add_subplot(gs_top[0, col_phi])
            block_label = f"Block {j}"
            ax_phi.set_title(f"{block_label} $\\phi^{{({j})}}$", fontsize=11)
            with torch.no_grad():  # splines don't involve BN
                phi_device = next(layer.phi.parameters()).device
                # Get current domain of phi
                x_min, x_max = layer.phi.in_min, layer.phi.in_max
                x_vals = torch.linspace(x_min, x_max, 200).to(phi_device)
                y_vals = layer.phi(x_vals)
            ax_phi.plot(x_vals.cpu(), y_vals.cpu(), 'c-', linewidth=2)
            ax_phi.grid(True, alpha=0.3)
            ax_phi.set_xlabel('Input', fontsize=9)
            ax_phi.set_ylabel('Output', fontsize=9)

            # Add theoretical range info if available
            domain_info = f"[{x_min:.2f}, {x_max:.2f}]"
            if hasattr(layer, 'input_range') and layer.input_range is not None:
                domain_info += f"\nInput range: [{layer.input_range.min:.2f}, {layer.input_range.max:.2f}]"
            ax_phi.set_title(f"{block_label} $\\phi^{{({j})}}$ Domain: {domain_info}", fontsize=9)

            # Plot Phi spline
            ax_Phi = fig.add_subplot(gs_top[0, col_Phi])
            ax_Phi.set_title(f"{block_label} $\\Phi^{{({j})}}$", fontsize=11)
            with torch.no_grad():  # splines don't involve BN
                # Get current domain of Phi
                in_min, in_max = layer.Phi.in_min, layer.Phi.in_max
                Phi_device = next(layer.Phi.parameters()).device
                x_vals = torch.linspace(in_min, in_max, 200).to(Phi_device)
                y_vals = layer.Phi(x_vals)

                # Get codomain info if trainable
                if CONFIG['train_phi_codomain'] and hasattr(layer, 'phi_codomain_params') and layer.phi_codomain_params is not None:
                    cc = layer.phi_codomain_params.cc.item()
                    cr = layer.phi_codomain_params.cr.item()
                    codomain_str = f", Codomain: [{cc-cr:.2f}, {cc+cr:.2f}]"
                else:
                    codomain_str = ""

            ax_Phi.plot(x_vals.cpu(), y_vals.cpu(), 'm-', linewidth=2)
            ax_Phi.grid(True, alpha=0.3)
            ax_Phi.set_xlabel('Input', fontsize=9)
            ax_Phi.set_ylabel('Output', fontsize=9)

            # Add output range info if available
            title_str = f"{block_label} $\\Phi^{{({j})}}$ D: [{in_min:.2f}, {in_max:.2f}]{codomain_str}"
            if hasattr(layer, 'output_range') and layer.output_range is not None:
                title_str += f"\nOut range: [{layer.output_range.min:.2f}, {layer.output_range.max:.2f}]"
            ax_Phi.set_title(title_str, fontsize=9)

    # Plot function comparison if dataset is provided and requested
    if plot_function and dataset is not None and gs_bottom_top is not None:
        print("DEBUG: Plotting function comparison")
        print(f"DEBUG: Using model: {type(model)}")

        if input_dim == 1:
            # Use training data if provided, otherwise create test points
            if x_train is not None and y_train is not None:
                print("DEBUG: Using provided training data for plotting")
                x_test = x_train.to(device)
                y_true = y_train.to(device)
            else:
                print("DEBUG: Generating new test data for plotting")
                x_test, y_true = dataset.sample(200, device=device)

            print(f"DEBUG: About to evaluate model on test data")
            print(f"DEBUG: Test data shape: {x_test.shape}")
            with nonmutating_infer(model):
                y_pred = model(x_test)
            print(f"DEBUG: Model predictions shape: {y_pred.shape}")
            print(f"DEBUG: Sample predictions: {y_pred[:5].flatten().cpu().numpy()}")

            if final_dim == 1:
                # For scalar output, plot the target vs. prediction
                gs_bottom = gridspec.GridSpec(1, 1)
                gs_bottom.update(left=0.05, right=0.95, top=gs_bottom_top, bottom=0.05)
                ax_func = fig.add_subplot(gs_bottom[0, 0])
                ax_func.set_title("Function Comparison", fontsize=12)
                ax_func.plot(x_test.cpu(), y_true.cpu(), 'k--', label='Target f(x)')
                ax_func.plot(x_test.cpu(), y_pred.cpu(), 'r-', label='Network output')
                ax_func.grid(True)
                ax_func.legend()
            else:
                # For vector output, plot each dimension separately
                gs_bottom = gridspec.GridSpec(1, final_dim)
                gs_bottom.update(left=0.05, right=0.95, top=gs_bottom_top, bottom=0.05, wspace=0.3)

                for d in range(final_dim):
                    ax_func = fig.add_subplot(gs_bottom[0, d])
                    ax_func.set_title(f"Output dim {d}", fontsize=12)
                    ax_func.plot(x_test.cpu(), y_true[:, d].cpu(), 'k--', label='Target f(x)')
                    ax_func.plot(x_test.cpu(), y_pred[:, d].cpu(), 'r-', label=f'Out dim {d}')
                    ax_func.grid(True)
                    ax_func.legend()

        elif input_dim == 2:
            # For 2D input, create grid for surface plots
            n = 50
            points, y_true = dataset.sample(n * n, device=device)

            if final_dim == 1:
                # For scalar output from 2D input, plot target and prediction as surfaces
                gs_bottom = gridspec.GridSpec(1, 2)
                gs_bottom.update(left=0.05, right=0.95, top=gs_bottom_top, bottom=0.05, wspace=0.3)

                x = torch.linspace(0, 1, n)
                y = torch.linspace(0, 1, n)
                X, Y = torch.meshgrid(x, y, indexing='ij')

                Z_true = y_true.reshape(n, n)
                ax_target = fig.add_subplot(gs_bottom[0, 0], projection='3d')
                ax_target.set_title("Target Function", fontsize=12)
                ax_target.plot_surface(X.cpu().numpy(), Y.cpu().numpy(), Z_true.cpu().numpy(), cmap='viridis')

                ax_output = fig.add_subplot(gs_bottom[0, 1], projection='3d')
                ax_output.set_title("Network Output", fontsize=12)
                with nonmutating_infer(model):
                    Z_pred = model(points).reshape(n, n)
                ax_output.plot_surface(X.cpu().numpy(), Y.cpu().numpy(), Z_pred.cpu().numpy(), cmap='viridis')
            else:
                # For vector output from 2D input, plot each output dimension
                gs_bottom = gridspec.GridSpec(1, final_dim)
                gs_bottom.update(left=0.05, right=0.95, top=gs_bottom_top, bottom=0.05, wspace=0.3)

                with nonmutating_infer(model):
                    Z_pred_all = model(points)

                x = torch.linspace(0, 1, n)
                y = torch.linspace(0, 1, n)
                X, Y = torch.meshgrid(x, y, indexing='ij')

                for d in range(final_dim):
                    ax_out = fig.add_subplot(gs_bottom[0, d], projection='3d')
                    ax_out.set_title(f"Output dim {d}", fontsize=12)
                    Z_pred_d = Z_pred_all[:, d].reshape(n, n)
                    Z_true_d = y_true[:, d].reshape(n, n)
                    ax_out.plot_surface(X.cpu().numpy(), Y.cpu().numpy(), Z_true_d.cpu().numpy(),
                                        cmap='viridis', alpha=0.5)
                    ax_out.plot_surface(X.cpu().numpy(), Y.cpu().numpy(), Z_pred_d.cpu().numpy(),
                                        cmap='autumn', alpha=0.5)
        else:
            # For high-dimensional inputs, use specialized plotting
            gs_bottom = gridspec.GridSpec(1, 1)
            gs_bottom.update(left=0.05, right=0.95, top=gs_bottom_top, bottom=0.05)
            ax_func = fig.add_subplot(gs_bottom[0, 0])
            plot_high_dim_function(ax_func, model, dataset, device)

    if save_path:
        fig.savefig(save_path, dpi=240, bbox_inches='tight')
        print(f"Figure saved to {save_path}")

    return fig


def plot_loss_curve(losses, save_path=None):
    """Plot training loss curve on logarithmic scale."""
    plt.figure(figsize=(10, 6))
    plt.semilogy(losses, label="Training Loss", linewidth=1.5)
    plt.xlabel("Epoch", fontsize=12)
    plt.ylabel("Loss (log scale)", fontsize=12)
    plt.title("Loss Curve - Logarithmic Scale", fontsize=14)
    plt.grid(True, which="both", ls="-", alpha=0.2)
    plt.legend(fontsize=12)

    # Add minor ticks for better readability
    plt.gca().xaxis.set_minor_locator(plt.MultipleLocator(5000))

    # Optionally add a text box with final loss value
    final_loss = losses[-1]
    textstr = f'Final Loss: {final_loss:.2e}'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    plt.text(0.65, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=props)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150)
        print(f"Loss curve saved to {save_path}")

----------
sn_core/__init__.py:
----------

"""Sprecher Network (SN) core package."""

from contextlib import contextmanager
import torch

from .model import (
    Interval,
    TheoreticalRange,
    PhiCodomainParams,
    SimpleSpline,
    SprecherLayerBlock,
    SprecherMultiLayerNetwork,
    compute_batchnorm_bounds,
    compute_layernorm_bounds,
    test_domain_tightness
)
from .train import (
    train_network,
    PlateauAwareCosineAnnealingLR,
    recalculate_bn_stats,
    has_batchnorm,
    # Export BN-only mode helpers so callers can freeze BN at eval
    # without switching the entire model to eval() globally.
    set_bn_eval,
    set_bn_train,
    # BN safety utilities for verification / checkpoint checks
    freeze_bn_running_stats,
    use_batch_stats_without_updating_bn,
)
from .data import get_dataset, DATASETS
from .plotting import plot_results, plot_loss_curve
from .config import CONFIG, MNIST_CONFIG, Q_VALUES_FACTOR, PARAM_CATEGORIES
from .export import export_parameters, parse_param_types


@contextmanager
def evaluation_mode(model: torch.nn.Module):
    """
    Temporarily run the given model in eval() under torch.no_grad(), and then
    restore its previous train/eval state when done.

    Usage:
        with evaluation_mode(model):
            y = model(x)       # no grad, BN uses running stats
            ...compute metrics...

    This helper is intentionally tiny and local: it does NOT change any global
    flags and does not alter training behavior outside the 'with' block.
    """
    was_training = model.training
    try:
        model.eval()
        with torch.no_grad():
            yield
    finally:
        model.train(was_training)


__all__ = [
    'Interval',
    'TheoreticalRange',
    'PhiCodomainParams',
    'SimpleSpline',
    'SprecherLayerBlock',
    'SprecherMultiLayerNetwork',
    'compute_batchnorm_bounds',
    'compute_layernorm_bounds',
    'test_domain_tightness',
    'train_network',
    'PlateauAwareCosineAnnealingLR',
    'recalculate_bn_stats',
    'has_batchnorm',
    'set_bn_eval',
    'set_bn_train',
    'freeze_bn_running_stats',
    'use_batch_stats_without_updating_bn',
    'evaluation_mode',
    'get_dataset',
    'DATASETS',
    'plot_results',
    'plot_loss_curve',
    'CONFIG',
    'MNIST_CONFIG',
    'Q_VALUES_FACTOR',
    'PARAM_CATEGORIES',
    'export_parameters',
    'parse_param_types'
]

----------
sn_core/export.py:
----------

"""Parameter export utilities for Sprecher Networks.

Fully supports both residual styles:
- 'node'   : original node-centric residuals (scalar / pooling / broadcast)
- 'linear' : standard residuals (α·x when d_in == d_out; projection W when d_in != d_out)
"""

import os
import torch
import numpy as np
from datetime import datetime
from .config import CONFIG, PARAM_CATEGORIES


def parse_param_types(param_string):
    """
    Parse parameter type string into a list of categories.
    
    Args:
        param_string: None, 'all', or comma-separated categories
        
    Returns:
        List of parameter categories to export
    """
    if param_string is None or param_string is False:
        return []
    elif param_string is True or param_string == 'all':
        return PARAM_CATEGORIES
    else:
        # Parse comma-separated categories
        requested = [p.strip().lower() for p in param_string.split(',')]
        # Validate categories
        valid_categories = []
        for cat in requested:
            if cat in PARAM_CATEGORIES:
                valid_categories.append(cat)
            else:
                print(f"Warning: Unknown parameter category '{cat}', skipping")
        return valid_categories


def format_tensor(tensor, name="", indent="  ", max_per_line=8):
    """
    Format a tensor for readable text output. Always prints ALL values.
    
    Args:
        tensor: PyTorch tensor to format
        name: Optional name for the tensor
        indent: Indentation string
        max_per_line: Maximum values per line for 1D tensors
        
    Returns:
        Formatted string representation
    """
    if tensor is None:
        return f"{indent}{name}: None\n" if name else f"{indent}None\n"
    
    # Convert to numpy for easier formatting
    arr = tensor.detach().cpu().numpy()
    
    lines = []
    if name:
        lines.append(f"{indent}{name}:")
        indent = indent + "  "
    
    # Format based on dimensions
    if arr.ndim == 0:
        # Scalar (0D)
        lines.append(f"{indent}{arr.item():.6f}")
    elif arr.ndim == 1:
        # Vector (1D)
        lines.append(f"{indent}Shape: {arr.shape}")
        if len(arr) <= max_per_line:
            values_str = ", ".join(f"{v:.6f}" for v in arr)
            lines.append(f"{indent}Values: [{values_str}]")
        else:
            lines.append(f"{indent}Values: [")
            for i in range(0, len(arr), max_per_line):
                chunk = arr[i:i+max_per_line]
                values_str = ", ".join(f"{v:.6f}" for v in chunk)
                suffix = "," if i + max_per_line < len(arr) else ""
                lines.append(f"{indent}  {values_str}{suffix}")
            lines.append(f"{indent}]")
    elif arr.ndim == 2:
        # Matrix (2D) - print the FULL matrix
        lines.append(f"{indent}Shape: {arr.shape}")
        lines.append(f"{indent}Values:")
        lines.append(f"{indent}[")
        for i in range(arr.shape[0]):
            row_str = ", ".join(f"{v:.6f}" for v in arr[i])
            suffix = "," if i < arr.shape[0] - 1 else ""
            lines.append(f"{indent}  [{row_str}]{suffix}")
        lines.append(f"{indent}]")
    else:
        # This should never happen in Sprecher Networks
        raise ValueError(f"Unexpected tensor dimension {arr.ndim} for parameter '{name}'. "
                         f"Shape: {arr.shape}. "
                         f"Sprecher Networks should only have 0D, 1D, or 2D parameters. "
                         f"This likely indicates a bug in the code.")
    
    return "\n".join(lines)


def format_integer_tensor(tensor, name="", indent="  ", max_per_line=16):
    """
    Format an integer tensor for readable text output.
    
    Args:
        tensor: PyTorch tensor with integer values
        name: Optional name for the tensor
        indent: Indentation string
        max_per_line: Maximum values per line
        
    Returns:
        Formatted string representation
    """
    if tensor is None:
        return f"{indent}{name}: None\n" if name else f"{indent}None\n"
    
    # Convert to numpy for easier formatting
    arr = tensor.detach().cpu().numpy()
    
    lines = []
    if name:
        lines.append(f"{indent}{name}:")
        indent = indent + "  "
    
    # Format 1D integer array
    lines.append(f"{indent}Shape: {arr.shape}")
    if len(arr) <= max_per_line:
        values_str = ", ".join(f"{int(v)}" for v in arr)
        lines.append(f"{indent}Values: [{values_str}]")
    else:
        lines.append(f"{indent}Values: [")
        for i in range(0, len(arr), max_per_line):
            chunk = arr[i:i+max_per_line]
            values_str = ", ".join(f"{int(v)}" for v in chunk)
            suffix = "," if i + max_per_line < len(arr) else ""
            lines.append(f"{indent}  {values_str}{suffix}")
        lines.append(f"{indent}]")
    
    return "\n".join(lines)


def _format_loss_for_header(loss_value):
    """Safely format a loss value in scientific notation, or return as-is if not convertible."""
    try:
        return f"{float(loss_value):.6e}"
    except Exception:
        return str(loss_value)


def export_parameters(model, param_types, save_path, dataset_info=None, checkpoint_info=None):
    """
    Export specified parameter types to a formatted text file.
    
    Args:
        model: Trained SprecherMultiLayerNetwork
        param_types: List of parameter types to export or 'all'
        save_path: Path to save the text file
        dataset_info: Optional dict with dataset metadata
        checkpoint_info: Optional dict with checkpoint information
    """
    # Parse parameter types
    categories = parse_param_types(param_types)
    if not categories:
        return
    
    # Create output directory if needed
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # Start building the output
    lines = []
    lines.append("=" * 60)
    lines.append("Sprecher Network Parameters Export")
    lines.append("=" * 60)
    
    # Add metadata if provided
    if dataset_info:
        lines.append(f"Dataset: {dataset_info.get('name', 'Unknown')}")
        lines.append(f"Architecture: {dataset_info.get('architecture', [])}")
        lines.append(f"Input Dimension: {dataset_info.get('input_dim', 'Unknown')}")
        lines.append(f"Output Dimension: {dataset_info.get('output_dim', 'Unknown')}")
        lines.append(f"Training Epochs: {dataset_info.get('epochs', 'Unknown')}")
    
    if checkpoint_info:
        lines.append(f"Checkpoint Epoch: {checkpoint_info.get('epoch', 'Unknown')}")
        # Safer formatting for loss (avoids crash if 'Unknown')
        loss_val = _format_loss_for_header(checkpoint_info.get('loss', 'Unknown'))
        lines.append(f"Best Loss: {loss_val}")
    
    lines.append(f"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"Exported Parameters: {', '.join(categories)}")
    lines.append("=" * 60)
    lines.append("")
    
    # Add layer mapping information
    lines.append("LAYER MAPPING")
    lines.append("-" * 40)
    for i, layer in enumerate(model.layers):
        is_final_str = " (FINAL - summed)" if layer.is_final else ""
        lines.append(f"Layer {i}: {layer.d_in} → {layer.d_out}{is_final_str}")
    lines.append("")
    
    # Export Lambda parameters
    if 'lambda' in categories:
        lines.append("LAMBDA PARAMETERS (Weight Vectors)")
        lines.append("-" * 40)
        for i, layer in enumerate(model.layers):
            lines.append(f"Layer {i} (d_in={layer.d_in}):")
            lines.append(format_tensor(layer.lambdas, indent="  "))
        lines.append("")
    
    # Export Eta parameters
    if 'eta' in categories:
        lines.append("ETA PARAMETERS (Shift Values)")
        lines.append("-" * 40)
        for i, layer in enumerate(model.layers):
            lines.append(f"Layer {i}: {layer.eta.item():.6f}")
        lines.append("")
    
    # Export Spline parameters
    if 'spline' in categories:
        lines.append("SPLINE PARAMETERS")
        lines.append("-" * 40)
        for i, layer in enumerate(model.layers):
            lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}):")
            
            # phi spline (monotonic)
            lines.append("  phi spline (monotonic):")
            lines.append(f"    Domain: [{layer.phi.in_min:.6f}, {layer.phi.in_max:.6f}]")
            lines.append(f"    Codomain: [0.0, 1.0] (fixed)")
            lines.append(f"    Number of knots: {layer.phi.num_knots}")
            if hasattr(layer.phi, 'log_increments'):
                lines.append(format_tensor(layer.phi.log_increments, "Log increments", "    "))
            coeffs_phi = layer.phi.get_coeffs()
            lines.append(format_tensor(coeffs_phi, "Coefficients", "    "))
            
            # Phi spline (general)
            lines.append("  Phi spline (general):")
            lines.append(f"    Domain: [{layer.Phi.in_min:.6f}, {layer.Phi.in_max:.6f}]")
            if CONFIG.get('train_phi_codomain', False) and hasattr(layer, 'phi_codomain_params'):
                cc = layer.phi_codomain_params.cc.item()
                cr = layer.phi_codomain_params.cr.item()
                lines.append(f"    Codomain: [{cc-cr:.6f}, {cc+cr:.6f}] (trainable)")
            lines.append(f"    Number of knots: {layer.Phi.num_knots}")
            lines.append(format_tensor(layer.Phi.coeffs, "Coefficients", "    "))
        lines.append("")
    
    # Export Residual parameters (supports both styles)
    if 'residual' in categories and CONFIG.get('use_residual_weights', True):
        style = CONFIG.get('residual_style', 'node')
        lines.append(f"RESIDUAL CONNECTION PARAMETERS (style: {style})")
        lines.append("-" * 40)
        has_any_residual = False
        
        for i, layer in enumerate(model.layers):
            # Prefer explicit attributes; we don't rely only on the style flag
            # so this works regardless of how the model was constructed.
            
            # 1) Linear style projection matrix (d_in != d_out) — present only if implemented
            if hasattr(layer, 'residual_projection') and layer.residual_projection is not None:
                has_any_residual = True
                lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}): PROJECTION residual")
                lines.append(format_tensor(layer.residual_projection, "Projection matrix W", "  "))
                lines.append("  Note: residual = x @ W (added pre-sum / per-output)")
                continue  # Exclusive with other residual types in typical builds
            
            # 2) Scalar residual (same dims) — used in both 'node' and 'linear'
            if hasattr(layer, 'residual_weight') and layer.residual_weight is not None:
                has_any_residual = True
                lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}): SCALAR residual")
                lines.append(f"  residual_weight (alpha): {layer.residual_weight.item():.6f}")
                continue
            
            # 3) Pooling residual (node-centric; d_in > d_out)
            if hasattr(layer, 'residual_pooling_weights') and layer.residual_pooling_weights is not None:
                has_any_residual = True
                lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}): POOLING residual")
                lines.append(format_tensor(layer.residual_pooling_weights, "Pooling weights", "  "))
                if hasattr(layer, 'pooling_assignment'):
                    lines.append(format_integer_tensor(layer.pooling_assignment, "Pooling assignment", "  "))
                if hasattr(layer, 'pooling_counts'):
                    lines.append(format_tensor(layer.pooling_counts, "Inputs per output", "  "))
                lines.append("  Note: input j contributes to output pooling_assignment[j] with its weight")
                continue
            
            # 4) Broadcast residual (node-centric; d_in < d_out)
            if hasattr(layer, 'residual_broadcast_weights') and layer.residual_broadcast_weights is not None:
                has_any_residual = True
                lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}): BROADCAST residual")
                lines.append(format_tensor(layer.residual_broadcast_weights, "Broadcast weights", "  "))
                if hasattr(layer, 'broadcast_sources'):
                    lines.append(format_integer_tensor(layer.broadcast_sources, "Broadcast sources", "  "))
                lines.append("  Note: output k receives scaled copy of input broadcast_sources[k]")
                continue
        
        if not has_any_residual:
            lines.append("  No residual parameters in this model")
        lines.append("")
    
    # Export Codomain parameters
    if 'codomain' in categories and CONFIG.get('train_phi_codomain', False):
        lines.append("PHI CODOMAIN PARAMETERS")
        lines.append("-" * 40)
        for i, layer in enumerate(model.layers):
            if hasattr(layer, 'phi_codomain_params') and layer.phi_codomain_params is not None:
                cc = layer.phi_codomain_params.cc.item()
                cr = layer.phi_codomain_params.cr.item()
                lines.append(f"Layer {i}:")
                lines.append(f"  Center (cc): {cc:.6f}")
                lines.append(f"  Radius (cr): {cr:.6f}")
                lines.append(f"  Codomain: [{cc-cr:.6f}, {cc+cr:.6f}]")
        lines.append("")
    
    # Export Normalization parameters
    if 'norm' in categories and getattr(model, 'norm_type', 'none') != 'none':
        lines.append("NORMALIZATION PARAMETERS")
        lines.append("-" * 40)
        for i, norm_layer in enumerate(model.norm_layers):
            if hasattr(norm_layer, 'weight'):  # Skip Identity layers
                lines.append(f"Layer {i} ({type(norm_layer).__name__}):")
                lines.append(format_tensor(norm_layer.weight, "Weight", "  "))
                lines.append(format_tensor(norm_layer.bias, "Bias", "  "))
                if hasattr(norm_layer, 'running_mean'):
                    lines.append(format_tensor(norm_layer.running_mean, "Running mean", "  "))
                    lines.append(format_tensor(norm_layer.running_var, "Running variance", "  "))
                    lines.append(f"  Num batches tracked: {norm_layer.num_batches_tracked.item()}")
        lines.append("")
    
    # Export Lateral Mixing parameters
    if 'lateral' in categories and CONFIG.get('use_lateral_mixing', False):
        lines.append("LATERAL MIXING PARAMETERS (Intra-block Communication)")
        lines.append("-" * 40)
        has_lateral = False
        for i, layer in enumerate(model.layers):
            if hasattr(layer, 'lateral_scale') and layer.lateral_scale is not None:
                has_lateral = True
                lines.append(f"Layer {i} ({layer.d_in} → {layer.d_out}):")
                lines.append(f"  Lateral scale: {layer.lateral_scale.item():.6f}")
                
                ltype = CONFIG.get('lateral_mixing_type', 'cyclic')
                if ltype == 'bidirectional':
                    lines.append("  Type: Bidirectional mixing")
                    lines.append(format_tensor(layer.lateral_weights_forward, "Forward weights", "  "))
                    lines.append(format_tensor(layer.lateral_weights_backward, "Backward weights", "  "))
                    lines.append("  Note: Each output mixes with both cyclic neighbors")
                else:  # 'cyclic'
                    lines.append("  Type: Cyclic mixing")
                    lines.append(format_tensor(layer.lateral_weights, "Lateral weights", "  "))
                    lines.append("  Note: Output q receives contribution from output (q+1) % d_out")
        if not has_lateral:
            lines.append("  No lateral mixing parameters in this model")
        lines.append("")
    
    # Export Output parameters
    if 'output' in categories:
        lines.append("OUTPUT PARAMETERS")
        lines.append("-" * 40)
        lines.append(f"Output scale: {model.output_scale.item():.6f}")
        lines.append(f"Output bias: {model.output_bias.item():.6f}")
        lines.append("")
    
    # Write to file with UTF-8 encoding
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"Parameters exported to: {save_path}")

----------
sn_core/model.py:
----------

"""Sprecher Network model components: splines, layers, and networks."""

import torch
import torch.nn as nn
import numpy as np
from .config import CONFIG, Q_VALUES_FACTOR


class Interval:
    """Interval arithmetic for optimal domain computation."""
    def __init__(self, min_val, max_val):
        self.min = float(min_val)
        self.max = float(max_val)
        assert self.min <= self.max, f"Invalid interval: [{self.min}, {self.max}]"
    
    def __add__(self, other):
        if isinstance(other, (int, float)):
            return Interval(self.min + other, self.max + other)
        elif isinstance(other, Interval):
            return Interval(self.min + other.min, self.max + other.max)
        else:
            return NotImplemented
    
    def __radd__(self, other):
        return self.__add__(other)
    
    def __mul__(self, scalar):
        if isinstance(scalar, (int, float)):
            if scalar >= 0:
                return Interval(self.min * scalar, self.max * scalar)
            else:
                return Interval(self.max * scalar, self.min * scalar)
        else:
            return NotImplemented
    
    def __rmul__(self, scalar):
        return self.__mul__(scalar)
    
    def __sub__(self, other):
        if isinstance(other, (int, float)):
            return Interval(self.min - other, self.max - other)
        elif isinstance(other, Interval):
            return Interval(self.min - other.max, self.max - other.min)
        else:
            return NotImplemented
    
    def __truediv__(self, scalar):
        if isinstance(scalar, (int, float)):
            if scalar > 0:
                return Interval(self.min / scalar, self.max / scalar)
            elif scalar < 0:
                return Interval(self.max / scalar, self.min / scalar)
            else:
                raise ValueError("Division by zero")
        else:
            return NotImplemented
    
    def union(self, other):
        """Union of two intervals."""
        return Interval(min(self.min, other.min), max(self.max, other.max))
    
    def __repr__(self):
        return f"[{self.min:.6f}, {self.max:.6f}]"


class TheoreticalRange:
    """Tracks theoretical min/max values for a layer's input/output."""
    def __init__(self, min_val, max_val):
        self.min = min_val
        self.max = max_val
    
    def __repr__(self):
        return f"TheoreticalRange({self.min:.3f}, {self.max:.3f})"


class PhiCodomainParams(nn.Module):
    """
    Trainable codomain parameters for Φ splines.
    The codomain is defined via a center and a radius:
      codomain: (cc - cr, cc + cr)
    """
    def __init__(self, cc=0.0, cr=10.0):
        super().__init__()
        self.cc = nn.Parameter(torch.tensor(cc, dtype=torch.float32))  # cc: codomain center for Φ
        self.cr = nn.Parameter(torch.tensor(cr, dtype=torch.float32))  # cr: codomain radius for Φ


class SimpleSpline(nn.Module):
    """
    A piecewise-linear spline.
    
    For φ splines: domain is dynamically updated, codomain is fixed [0,1]
    For Φ splines: domain is dynamically updated, codomain can be fixed or trainable
    """
    def __init__(self, num_knots=30, in_range=(0, 1), out_range=(0, 1), monotonic=False,
                 train_codomain=False, codomain_params=None):
        super().__init__()
        self.num_knots = num_knots
        self.monotonic = monotonic
        self.train_codomain = train_codomain
        self.codomain_params = codomain_params
        
        # Store initial ranges
        self.in_min, self.in_max = in_range
        self.out_min, self.out_max = out_range
        
        # Create knots buffer - will be updated dynamically
        self.register_buffer('knots', torch.linspace(self.in_min, self.in_max, num_knots))
        
        # Domain violation tracking
        self.domain_violations = 0
        self.total_evaluations = 0
        
        # Initialize spline coefficients
        torch.manual_seed(CONFIG['seed'])
        if monotonic:
            # For monotonic splines, use log-space parameters to ensure monotonicity
            self.log_increments = nn.Parameter(torch.zeros(num_knots))
            with torch.no_grad():
                # Initialize to approximate linear function
                self.log_increments.data = torch.log(torch.ones(num_knots) / num_knots + 1e-6)
        else:
            # For general splines (Φ), initialize to zeros
            # Will be properly initialized when domain is computed
            self.coeffs = nn.Parameter(torch.zeros(num_knots))
    
    def update_domain(self, new_range, allow_resampling=True, force_resample=False):
        """
        Update the domain of the spline by adjusting knot positions.
        For non-monotonic splines (Φ), this method resamples the spline to preserve
        its learned functional shape in the new domain, preventing information loss
        during training.
        
        Args:
            new_range: Tuple of (new_min, new_max) for the domain
            allow_resampling: If False, skip resampling even if conditions are met
            force_resample: If True, force resampling regardless of total_evaluations
        """
        with torch.no_grad():
            new_min, new_max = new_range
            
            # Debug logging for checkpoint operations
            if CONFIG.get('debug_checkpoint_loading', False):
                print(f"\n[CHECKPOINT DEBUG] SimpleSpline.update_domain called:")
                print(f"  - Monotonic: {self.monotonic}")
                print(f"  - Old domain: [{self.in_min:.6f}, {self.in_max:.6f}]")
                print(f"  - New domain: [{new_min:.6f}, {new_max:.6f}]")
                print(f"  - Allow resampling: {allow_resampling}")
                print(f"  - Total evaluations: {self.total_evaluations}")

            # Guard clause: If the domain isn't changing significantly, do nothing.
            # This prevents floating point drift and unnecessary computation.
            if abs(self.in_min - new_min) < 1e-6 and abs(self.in_max - new_max) < 1e-6:
                if CONFIG.get('debug_checkpoint_loading', False):
                    print("  - Domain unchanged, skipping update")
                return

            # --- Spline Resampling to Preserve Learned Shape (for Φ splines) ---
            if not self.monotonic and (self.total_evaluations > 0 or force_resample) and allow_resampling:
                if CONFIG.get('debug_checkpoint_loading', False):
                    print(f"  - RESAMPLING WILL OCCUR (non-monotonic, evaluations={self.total_evaluations}, force={force_resample}, resampling allowed)")
                    
                # Store the state of the old spline before changing anything
                old_knots = self.knots.clone()
                old_coeffs = self.get_coeffs().clone()
                
                if CONFIG.get('debug_checkpoint_loading', False):
                    print(f"  - Old coeffs min/max: {old_coeffs.min():.6f}/{old_coeffs.max():.6f}")
                
                def old_spline_eval(x_vals):
                    # Clamp inputs to the old domain for robust interpolation.
                    x_clamped = torch.clamp(x_vals, old_knots[0], old_knots[-1])
                    
                    intervals = torch.searchsorted(old_knots, x_clamped) - 1
                    intervals = torch.clamp(intervals, 0, self.num_knots - 2)
                    
                    knot_spacing = old_knots[intervals + 1] - old_knots[intervals]
                    t = torch.where(knot_spacing > 1e-8,
                                    (x_clamped - old_knots[intervals]) / knot_spacing,
                                    torch.zeros_like(x_clamped))
                    
                    interpolated = (1 - t) * old_coeffs[intervals] + t * old_coeffs[intervals + 1]
                    return interpolated

                # Define the new knot positions in the new domain
                new_knots_pos = torch.linspace(new_min, new_max, self.num_knots, 
                                               device=self.knots.device, dtype=self.knots.dtype)
                
                # Resample: Evaluate the OLD spline at the NEW knot positions
                resampled_coeffs = old_spline_eval(new_knots_pos)

                # Update the learnable parameters with the resampled values
                self.coeffs.data = resampled_coeffs
                
                if CONFIG.get('debug_checkpoint_loading', False):
                    print(f"  - New coeffs min/max after resampling: {resampled_coeffs.min():.6f}/{resampled_coeffs.max():.6f}")
                    print(f"  - Coeffs changed: {not torch.allclose(old_coeffs, resampled_coeffs)}")
            else:
                if CONFIG.get('debug_checkpoint_loading', False) and not self.monotonic:
                    print(f"  - NO RESAMPLING (evaluations={self.total_evaluations}, allow={allow_resampling})")

            # --- Update the internal state for the new domain ---
            self.in_min, self.in_max = new_min, new_max

            # Add safety margin only if explicitly configured (default is 0)
            if CONFIG.get('domain_safety_margin', 0.0) > 0:
                margin = CONFIG['domain_safety_margin'] * (self.in_max - self.in_min)
                self.in_min -= margin
                self.in_max += margin
            
            self.knots.data = torch.linspace(self.in_min, self.in_max, self.num_knots, 
                                           device=self.knots.device, dtype=self.knots.dtype)

    def initialize_as_identity(self, domain_min, domain_max):
        """Initialize spline coefficients to approximate identity function on given domain."""
        with torch.no_grad():
            if self.monotonic:
                # For monotonic splines, keep uniform increments
                self.log_increments.data = torch.log(torch.ones(self.num_knots) / self.num_knots + 1e-6)
            else:
                # For general splines, set coefficients to be linear from domain_min to domain_max
                self.coeffs.data = torch.linspace(domain_min, domain_max, self.num_knots, 
                                                device=self.coeffs.device, dtype=self.coeffs.dtype)
    
    def get_coeffs(self):
        """Get the actual spline coefficients (for both monotonic and non-monotonic)"""
        if self.monotonic:
            # Use softplus to ensure positive increments, then cumsum for monotonicity
            increments = torch.nn.functional.softplus(self.log_increments)
            cumulative = torch.cumsum(increments, 0)
            # Normalize to [0, 1] range
            cumulative = cumulative / (cumulative[-1] + 1e-8)
            # For monotonic splines, output range is always [0, 1]
            return cumulative
        else:
            # For non-monotonic splines, return coefficients directly
            # They will be scaled by codomain if trainable
            return self.coeffs
    
    def get_actual_output_range(self):
        """Compute the actual range of outputs this spline can produce."""
        with torch.no_grad():
            coeffs = self.get_coeffs()
            
            if self.monotonic:
                # Monotonic splines always output [0, 1]
                return TheoreticalRange(0.0, 1.0)
            else:
                # For Φ splines, the piecewise linear function achieves its extrema at knots
                # So we just need to check the coefficient values
                if self.train_codomain and self.codomain_params is not None:
                    # Apply codomain transformation to coefficients
                    cc = self.codomain_params.cc.item()
                    cr = self.codomain_params.cr.item()
                    coeff_min = coeffs.min().item()
                    coeff_max = coeffs.max().item()
                    
                    # Transform each coefficient
                    normalized_coeffs = (coeffs - coeff_min) / (coeff_max - coeff_min + 1e-8)
                    transformed_coeffs = cc - cr + 2 * cr * normalized_coeffs
                    
                    # True min/max are among the transformed coefficients
                    actual_min = transformed_coeffs.min().item()
                    actual_max = transformed_coeffs.max().item()
                else:
                    # No transformation needed - min/max are just coefficient extrema
                    actual_min = coeffs.min().item()
                    actual_max = coeffs.max().item()
                
                return TheoreticalRange(actual_min, actual_max)
    
    def forward(self, x):
        x = x.to(self.knots.device)
        
        # Track domain violations if debugging
        if CONFIG.get('track_domain_violations', False):
            self.total_evaluations += x.numel()
            violations = ((x < self.in_min) | (x > self.in_max)).sum().item()
            self.domain_violations += violations
            if violations > 0 and CONFIG.get('verbose_domain_violations', False):
                print(f"Domain violation: {violations}/{x.numel()} values outside [{self.in_min:.3f}, {self.in_max:.3f}]")
        
        # Handle out-of-domain inputs by extending the spline linearly
        # This is more theoretically sound than clamping
        below_domain = x < self.in_min
        above_domain = x > self.in_max
        
        # For in-domain values, proceed as normal
        x_clamped = torch.clamp(x, self.in_min, self.in_max)
        
        # Find which segment each input value belongs to
        intervals = torch.searchsorted(self.knots, x_clamped) - 1
        intervals = torch.clamp(intervals, 0, self.num_knots - 2)
        
        # Compute interpolation parameter t in [0,1]
        knot_spacing = self.knots[intervals + 1] - self.knots[intervals]
        # Avoid division by zero for very small domains
        t = torch.where(knot_spacing > 1e-8,
                        (x_clamped - self.knots[intervals]) / knot_spacing,
                        torch.zeros_like(x_clamped))
        
        # Get coefficients
        coeffs = self.get_coeffs()
        
        # Linear interpolation
        interpolated = (1 - t) * coeffs[intervals] + t * coeffs[intervals + 1]
        
        # For monotonic splines, handle out-of-domain with constant extension (slope 0)
        if self.monotonic:
            # Extend with slope 0 outside domain (keeps values at 0 or 1)
            result = torch.where(below_domain, torch.zeros_like(x), interpolated)
            result = torch.where(above_domain, torch.ones_like(x), result)
        else:
            # For general splines, extend linearly based on boundary slopes
            if self.train_codomain and self.codomain_params is not None:
                # Apply trainable codomain scaling
                cc = self.codomain_params.cc
                cr = self.codomain_params.cr
                # The coefficients are in the original scale, map to trainable codomain
                coeff_min = coeffs.min()
                coeff_max = coeffs.max()
                # Normalize and rescale
                normalized = (interpolated - coeff_min) / (coeff_max - coeff_min + 1e-8)
                result = cc - cr + 2 * cr * normalized
                
                # Handle out-of-domain with appropriate linear extension
                left_slope = (coeffs[1] - coeffs[0]) / (self.knots[1] - self.knots[0] + 1e-8)
                right_slope = (coeffs[-1] - coeffs[-2]) / (self.knots[-1] - self.knots[-2] + 1e-8)
                
                # Scale slopes according to codomain transformation
                scale_factor = 2 * cr / (coeff_max - coeff_min + 1e-8)
                left_slope_scaled = left_slope * scale_factor
                right_slope_scaled = right_slope * scale_factor
                
                left_value = cc - cr + 2 * cr * (coeffs[0] - coeff_min) / (coeff_max - coeff_min + 1e-8)
                right_value = cc - cr + 2 * cr * (coeffs[-1] - coeff_min) / (coeff_max - coeff_min + 1e-8)
                
                result = torch.where(below_domain,
                                   left_value + left_slope_scaled * (x - self.in_min),
                                   result)
                result = torch.where(above_domain,
                                   right_value + right_slope_scaled * (x - self.in_max),
                                   result)
            else:
                # Fixed codomain case - extend linearly
                left_slope = (coeffs[1] - coeffs[0]) / (self.knots[1] - self.knots[0] + 1e-8)
                right_slope = (coeffs[-1] - coeffs[-2]) / (self.knots[-1] - self.knots[-2] + 1e-8)
                
                result = torch.where(below_domain,
                                   coeffs[0] + left_slope * (x - self.in_min),
                                   interpolated)
                result = torch.where(above_domain,
                                   coeffs[-1] + right_slope * (x - self.in_max),
                                   result)
        
        return result
    
    def get_domain_violation_stats(self):
        """Return domain violation statistics."""
        if self.total_evaluations == 0:
            return 0.0
        return self.domain_violations / self.total_evaluations
    
    def reset_domain_violation_stats(self):
        """Reset domain violation tracking."""
        self.domain_violations = 0
        self.total_evaluations = 0
    
    def get_domain_state(self):
        """Get the complete domain state for checkpoint saving."""
        state = {
            'knots': self.knots.data.clone(),
            'in_min': self.in_min,
            'in_max': self.in_max,
            'coeffs': self.get_coeffs().clone(),
            'domain_violations': self.domain_violations,
            'total_evaluations': self.total_evaluations
        }
        
        # Save the raw parameters to ensure exact restoration
        if self.monotonic:
            # For monotonic splines, save log_increments
            state['log_increments'] = self.log_increments.data.clone()
        else:
            # For non-monotonic splines, save raw coefficients
            state['raw_coeffs'] = self.coeffs.data.clone()
        
        if CONFIG.get('debug_checkpoint_loading', False):
            print(f"\n[CHECKPOINT DEBUG] SimpleSpline.get_domain_state:")
            print(f"  - Monotonic: {self.monotonic}")
            print(f"  - Domain: [{state['in_min']:.6f}, {state['in_max']:.6f}]")
            print(f"  - Knots shape: {state['knots'].shape}")
            print(f"  - Coeffs min/max: {state['coeffs'].min():.6f}/{state['coeffs'].max():.6f}")
            print(f"  - Total evaluations: {state['total_evaluations']}")
            if 'raw_coeffs' in state:
                print(f"  - Raw coeffs min/max: {state['raw_coeffs'].min():.6f}/{state['raw_coeffs'].max():.6f}")
            if 'log_increments' in state:
                print(f"  - Log increments min/max: {state['log_increments'].min():.6f}/{state['log_increments'].max():.6f}")
            
        return state
    
    def set_domain_state(self, state):
        """Restore domain state from checkpoint."""
        if CONFIG.get('debug_checkpoint_loading', False):
            print(f"\n[CHECKPOINT DEBUG] SimpleSpline.set_domain_state:")
            print(f"  - Monotonic: {self.monotonic}")
            print(f"  - Old domain: [{self.in_min:.6f}, {self.in_max:.6f}]")
            print(f"  - New domain: [{state['in_min']:.6f}, {state['in_max']:.6f}]")
            print(f"  - New knots shape: {state['knots'].shape}")
            print(f"  - New total evaluations: {state.get('total_evaluations', 0)}")
            
        self.knots.data = state['knots']
        self.in_min = state['in_min']
        self.in_max = state['in_max']
        
        # Restore raw parameters to ensure exact restoration
        if self.monotonic and 'log_increments' in state:
            # For monotonic splines, restore log_increments
            self.log_increments.data = state['log_increments']
            if CONFIG.get('debug_checkpoint_loading', False):
                print(f"  - Log increments restored: min={state['log_increments'].min():.6f}, max={state['log_increments'].max():.6f}")
        elif not self.monotonic and 'raw_coeffs' in state:
            # For non-monotonic splines, restore raw coefficients
            self.coeffs.data = state['raw_coeffs']
            if CONFIG.get('debug_checkpoint_loading', False):
                print(f"  - Raw coeffs restored: min={state['raw_coeffs'].min():.6f}/{state['raw_coeffs'].max():.6f}")
        
        self.domain_violations = state.get('domain_violations', 0)
        self.total_evaluations = state.get('total_evaluations', 0)
        
        if CONFIG.get('debug_checkpoint_loading', False):
            print(f"  - Domain restored successfully")
            print(f"  - Knots updated: {torch.allclose(self.knots, state['knots'])}")


def compute_batchnorm_bounds(input_interval, norm_layer, training_mode=True):
    """
    Compute output interval after BatchNorm.
    
    Args:
        input_interval: Interval of possible input values
        norm_layer: BatchNorm1d layer
        training_mode: Whether in training mode (affects statistics used)
    
    Returns:
        Output interval after BatchNorm
    """
    eps = norm_layer.eps
    
    if training_mode:
        # Conservative standardized range during training
        k = 4.0  # Covers ~99.99% of standard normal
        standardized = Interval(-k, k)
        
        # Apply affine transformation if enabled (conservative across channels)
        if norm_layer.affine:
            weight = norm_layer.weight
            bias = norm_layer.bias
            
            weight_min = weight.min().item()
            weight_max = weight.max().item()
            bias_min = bias.min().item()
            bias_max = bias.max().item()
            
            corners = []
            for w in [weight_min, weight_max]:
                for b in [bias_min, bias_max]:
                    if w >= 0:
                        corners.append(Interval(
                            standardized.min * w + b,
                            standardized.max * w + b
                        ))
                    else:
                        corners.append(Interval(
                            standardized.max * w + b,
                            standardized.min * w + b
                        ))
            result = corners[0]
            for corner in corners[1:]:
                result = result.union(corner)
            return result
        else:
            return standardized
    else:
        # EVAL MODE: per-channel running stats (tight union across channels)
        running_mean = norm_layer.running_mean
        running_var = norm_layer.running_var
        
        a = input_interval.min
        b = input_interval.max
        
        std = torch.sqrt(running_var + eps)
        # Standardize per channel, then get per-channel intervals
        lo = (a - running_mean) / std
        hi = (b - running_mean) / std
        std_min = torch.minimum(lo, hi)
        std_max = torch.maximum(lo, hi)
        
        if norm_layer.affine:
            w = norm_layer.weight
            b_bias = norm_layer.bias
            out_min = torch.where(w >= 0, w * std_min + b_bias, w * std_max + b_bias)
            out_max = torch.where(w >= 0, w * std_max + b_bias, w * std_min + b_bias)
            return Interval(out_min.min().item(), out_max.max().item())
        else:
            return Interval(std_min.min().item(), std_max.max().item())


def compute_layernorm_bounds(input_interval, norm_layer, num_features):
    """
    Compute output interval after LayerNorm.
    
    LayerNorm normalizes across features for each sample independently,
    making exact bounds intractable. We use conservative bounds.
    
    Args:
        input_interval: Interval of possible input values per feature
        norm_layer: LayerNorm layer  
        num_features: Number of features being normalized
    
    Returns:
        Output interval after LayerNorm
    """
    eps = norm_layer.eps
    
    # Exact worst-case bound for LayerNorm
    # This occurs when one feature is at the max and all others at min (or vice versa)
    k = (num_features - 1) ** 0.5  # Exact worst-case bound
    standardized = Interval(-k, k)
    
    # Apply affine transformation
    if norm_layer.elementwise_affine:
        weight = norm_layer.weight
        bias = norm_layer.bias
        
        # Get bounds on weight and bias
        weight_min = weight.min().item()
        weight_max = weight.max().item()
        bias_min = bias.min().item()
        bias_max = bias.max().item()
        
        # Compute output bounds
        corners = []
        for w in [weight_min, weight_max]:
            for b in [bias_min, bias_max]:
                if w >= 0:
                    corners.append(Interval(
                        standardized.min * w + b,
                        standardized.max * w + b
                    ))
                else:
                    corners.append(Interval(
                        standardized.max * w + b,
                        standardized.min * w + b
                    ))
        
        result = corners[0]
        for corner in corners[1:]:
            result = result.union(corner)
        return result
    else:
        return standardized


class SprecherLayerBlock(nn.Module):
    """
    A single Sprecher block that transforms d_in -> d_out using:
      - A monotonic spline φ with dynamically computed domain and fixed [0,1] codomain
      - A general spline Φ with dynamically computed domain and optionally trainable codomain
      - A trainable shift (η)
      - A trainable weight VECTOR (λ) - one weight per input dimension
      - Optional lateral mixing for cross-output communication
      - Residual connection (configurable style):
          * 'node'   (default): node-centric residuals (scalar/pooling/broadcast)
          * 'linear': standard residuals (scalar if d_in==d_out, else projection matrix)
    If is_final is True, the block sums its outputs to produce a scalar.
    """
    def __init__(self, d_in, d_out, layer_num=0, is_final=False, phi_knots=100, Phi_knots=100,
                 phi_domain=None, Phi_domain=None):
        super().__init__()
        self.d_in = d_in
        self.d_out = d_out
        self.layer_num = layer_num
        self.is_final = is_final
        
        # Theoretical range tracking
        self.input_range = TheoreticalRange(0.0, 1.0) if layer_num == 0 else None
        self.output_range = None

        # NEW: per-channel input/output intervals (populated by network during updates)
        self.input_min_per_dim = None   # tensor[d_in] or None
        self.input_max_per_dim = None   # tensor[d_in] or None
        self.output_min_per_q = None    # tensor[d_out] (post-Φ+residual, pre-sum) or None
        self.output_max_per_q = None    # tensor[d_out] or None
        
        # Track whether codomain has been initialized from domain
        self.codomain_initialized_from_domain = False
        
        # Create codomain parameters for Φ if trainable
        if CONFIG['train_phi_codomain']:
            self.phi_codomain_params = PhiCodomainParams(cc=0.0, cr=10.0)
        else:
            self.phi_codomain_params = None
        
        # The inner monotonic spline φ (with fixed codomain [0,1])
        # Domain will be updated dynamically based on inputs and η
        phi_in_range = phi_domain if phi_domain is not None else (0, 1)
        self.phi = SimpleSpline(
            num_knots=phi_knots,
            in_range=phi_in_range,  # Use provided domain or default
            out_range=(0, 1),  # Fixed codomain
            monotonic=True
        )
        
        # The outer general spline Φ
        # Both domain and (optionally) codomain are dynamic
        Phi_in_range = Phi_domain if Phi_domain is not None else (0, 1)
        self.Phi = SimpleSpline(
            num_knots=Phi_knots,
            in_range=Phi_in_range,  # Use provided domain or default
            out_range=(0, 1),  # Temporary - will be updated immediately
            train_codomain=CONFIG['train_phi_codomain'],
            codomain_params=self.phi_codomain_params
        )
        
        # Weight vector (TRUE Sprecher λ: one per input dim)
        self.lambdas = nn.Parameter(torch.randn(d_in) * np.sqrt(2.0 / d_in))
        
        # Initialize eta to a reasonable value based on d_out
        self.eta = nn.Parameter(torch.tensor(1.0 / (d_out + 10)))
        
        # Q-values for indexing
        self.register_buffer('q_values', torch.arange(d_out, dtype=torch.float32))
        
        # Lateral mixing parameters
        if CONFIG['use_lateral_mixing'] and d_out > 1:
            self.lateral_scale = nn.Parameter(torch.tensor(CONFIG['lateral_scale_init']))
            
            if CONFIG['lateral_mixing_type'] == 'bidirectional':
                # Bidirectional mixing - mix with both neighbors
                self.lateral_weights_forward = nn.Parameter(
                    torch.ones(d_out) * CONFIG['lateral_weight_init'] * 0.5
                )
                self.lateral_weights_backward = nn.Parameter(
                    torch.ones(d_out) * CONFIG['lateral_weight_init'] * 0.5
                )
                # Indices for cyclic neighbors
                self.register_buffer('lateral_indices_forward', 
                                   torch.arange(d_out).roll(-1))
                self.register_buffer('lateral_indices_backward', 
                                   torch.arange(d_out).roll(1))
            else:  # 'cyclic' - default
                # Simple cyclic shift mixing
                self.lateral_weights = nn.Parameter(
                    torch.ones(d_out) * CONFIG['lateral_weight_init']
                )
                # Each output q receives from (q+1) % d_out
                self.register_buffer('lateral_indices', 
                                   torch.arange(d_out).roll(-1))
        else:
            # No lateral mixing
            self.lateral_scale = None
            self.lateral_weights = None
            self.lateral_weights_forward = None
            self.lateral_weights_backward = None
        
        # -------------------------------
        # Residual connection parameters
        # -------------------------------
        self.residual_style = str(CONFIG.get('residual_style', 'node')).lower()
        # Normalize potential synonyms
        if self.residual_style in ['standard', 'matrix']:
            self.residual_style = 'linear'

        # Initialize all residual attributes to None (then set as needed)
        self.residual_weight = None                  # scalar (only when d_in == d_out)
        self.residual_pooling_weights = None         # node-centric (d_in > d_out)
        self.residual_broadcast_weights = None       # node-centric (d_in < d_out)
        self.residual_projection = None              # standard linear projection (d_in != d_out)

        if CONFIG['use_residual_weights']:
            if d_in == d_out:
                # When dimensions match, use a simple scalar weight in both styles
                self.residual_weight = nn.Parameter(torch.tensor(0.1))
            else:
                if self.residual_style == 'linear':
                    # Standard residual: full projection matrix W (d_in x d_out)
                    self.residual_projection = nn.Parameter(torch.empty(d_in, d_out))
                    nn.init.xavier_uniform_(self.residual_projection)
                else:
                    # Node-centric (existing) behavior
                    if d_in > d_out:
                        # Pooling case: aggregate multiple inputs per output
                        init_value = 1.0 / d_in  # Normalized initialization
                        self.residual_pooling_weights = nn.Parameter(
                            torch.ones(d_in) * init_value
                        )
                        # Create balanced cyclic assignment
                        assignments = [(i % d_out) for i in range(d_in)]
                        self.register_buffer('pooling_assignment', torch.tensor(assignments, dtype=torch.long))
                        counts = torch.zeros(d_out)
                        for i in range(d_in):
                            counts[assignments[i]] += 1
                        self.register_buffer('pooling_counts', counts)
                    else:  # d_in < d_out
                        # Broadcasting case: expand inputs to outputs
                        self.residual_broadcast_weights = nn.Parameter(
                            torch.ones(d_out) * 0.1
                        )
                        sources = [(i % d_in) for i in range(d_out)]
                        self.register_buffer('broadcast_sources', torch.tensor(sources, dtype=torch.long))
        # else: all residual attributes remain None
    
    def update_phi_domain_theoretical(self, input_range, allow_resampling=True, force_resample=False):
        """Update φ domain based on theoretical bounds."""
        self.input_range = input_range
        
        # Domain needs to handle [in_min, in_max] + η*q for q in [0, d_out-1]
        # We need to handle both positive and negative η
        eta_val = self.eta.item()
        
        if eta_val >= 0:
            # Normal case: η is positive
            phi_min = input_range.min
            phi_max = input_range.max + eta_val * (self.d_out - 1)
        else:
            # When η is negative, the max shift is at q=0 and min shift is at q=(d_out-1)
            phi_min = input_range.min + eta_val * (self.d_out - 1)  # This will be less than input_range.min
            phi_max = input_range.max  # No shift when q=0
        
        # Ensure domain is valid (min < max)
        if phi_min >= phi_max:
            # This shouldn't happen in theory, but let's add a small epsilon to ensure validity
            phi_max = phi_min + 1e-4
        
        self.phi.update_domain((phi_min, phi_max), allow_resampling=allow_resampling, force_resample=force_resample)
    
    def _apply_lateral_mixing_bounds_per_q(self, s_min_per_q, s_max_per_q):
        """
        Apply **sign-aware** lateral mixing bounds to per-q intervals.
        
        For cyclic mixing: s'_q = s_q + α * w_q * s_{q+1}
          min: s_min_q + [ w_q^+ * s_min_{q+1} + w_q^- * s_max_{q+1} ]
          max: s_max_q + [ w_q^+ * s_max_{q+1} + w_q^- * s_min_{q+1} ]
        
        For bidirectional mixing: s'_q = s_q + α * ( w^f_q * s_{q+1} + w^b_q * s_{q-1} )
          apply the same sign-splitting to each neighbor and sum.
        
        This is tight for linear mixing when only intervals are known.
        """
        if self.lateral_scale is None:
            return s_min_per_q, s_max_per_q
        
        device = s_min_per_q.device
        
        if CONFIG['lateral_mixing_type'] == 'bidirectional':
            eff_f = (self.lateral_scale * self.lateral_weights_forward).to(device)
            eff_b = (self.lateral_scale * self.lateral_weights_backward).to(device)
            idx_f = self.lateral_indices_forward.to(device)
            idx_b = self.lateral_indices_backward.to(device)
            
            efp, efn = torch.clamp(eff_f, min=0), torch.clamp(eff_f, max=0)
            ebp, ebn = torch.clamp(eff_b, min=0), torch.clamp(eff_b, max=0)
            
            neighbor_min = (
                efp * s_min_per_q[idx_f] + efn * s_max_per_q[idx_f] +
                ebp * s_min_per_q[idx_b] + ebn * s_max_per_q[idx_b]
            )
            neighbor_max = (
                efp * s_max_per_q[idx_f] + efn * s_min_per_q[idx_f] +
                ebp * s_max_per_q[idx_b] + ebn * s_min_per_q[idx_b]
            )
            
            mixed_min = s_min_per_q + neighbor_min
            mixed_max = s_max_per_q + neighbor_max
        else:  # 'cyclic'
            eff = (self.lateral_scale * self.lateral_weights).to(device)
            idx = self.lateral_indices.to(device)
            ep, en = torch.clamp(eff, min=0), torch.clamp(eff, max=0)
            
            mixed_min = s_min_per_q + ep * s_min_per_q[idx] + en * s_max_per_q[idx]
            mixed_max = s_max_per_q + ep * s_max_per_q[idx] + en * s_min_per_q[idx]
        
        return mixed_min, mixed_max
    
    def _compute_s_bounds_per_q(self, a_vec, b_vec):
        """
        Helper: compute tight s_min/s_max per q using per-dimension intervals.
        a_vec, b_vec: tensors of shape [d_in] on correct device.
        Returns: s_min_per_q, s_max_per_q (tensors [d_out])
        """
        device = self.phi.knots.device
        q_values = torch.arange(self.d_out, device=device, dtype=torch.float32)

        # Broadcast a_i + η q and b_i + η q: shapes (d_in, d_out)
        eta_val = self.eta.item()
        a_mat = a_vec.view(-1, 1) + eta_val * q_values.view(1, -1)
        b_mat = b_vec.view(-1, 1) + eta_val * q_values.view(1, -1)

        # Evaluate φ elementwise on these matrices
        phi_a = self.phi(a_mat)  # (d_in, d_out)
        phi_b = self.phi(b_mat)  # (d_in, d_out)

        lambdas_col = self.lambdas.view(-1, 1)  # (d_in, 1)

        # Sign-aware contributions
        contrib_min = torch.where(lambdas_col >= 0, lambdas_col * phi_a, lambdas_col * phi_b)  # (d_in,d_out)
        contrib_max = torch.where(lambdas_col >= 0, lambdas_col * phi_b, lambdas_col * phi_a)  # (d_in,d_out)

        s_min_per_q = contrib_min.sum(dim=0) + Q_VALUES_FACTOR * q_values
        s_max_per_q = contrib_max.sum(dim=0) + Q_VALUES_FACTOR * q_values
        return s_min_per_q, s_max_per_q
    
    def update_Phi_domain_theoretical(self, allow_resampling=True, force_resample=False):
        """Update Φ domain based on theoretical bounds using per-q tight bounds (uses per-channel intervals when available)."""
        with torch.no_grad():
            device = self.phi.knots.device
            # Use per-channel input intervals if provided, else fall back to global [a,b]
            if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                if a_vec.numel() != self.d_in or b_vec.numel() != self.d_in:
                    a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                    b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                s_min_per_q, s_max_per_q = self._compute_s_bounds_per_q(a_vec, b_vec)
            else:
                # Fallback: use global bounds with λ+ / λ− trick (original behavior)
                eta_val = self.eta.item()
                q_values = torch.arange(self.d_out, device=device, dtype=torch.float32)
                phi_inputs_min = self.input_range.min + eta_val * q_values
                phi_inputs_max = self.input_range.max + eta_val * q_values
                phi_at_min = self.phi(phi_inputs_min)
                phi_at_max = self.phi(phi_inputs_max)
                phi_min_per_q = torch.minimum(phi_at_min, phi_at_max)
                phi_max_per_q = torch.maximum(phi_at_min, phi_at_max)
                lambda_pos = torch.clamp(self.lambdas, min=0).sum().item()
                lambda_neg = torch.clamp(self.lambdas, max=0).sum().item()
                s_min_per_q = lambda_pos * phi_min_per_q + lambda_neg * phi_max_per_q + Q_VALUES_FACTOR * q_values
                s_max_per_q = lambda_pos * phi_max_per_q + lambda_neg * phi_min_per_q + Q_VALUES_FACTOR * q_values
            
            # Apply lateral mixing bounds if enabled (now sign-aware)
            if self.lateral_scale is not None:
                s_min_per_q, s_max_per_q = self._apply_lateral_mixing_bounds_per_q(s_min_per_q, s_max_per_q)
            
            # Union across q to form Φ input domain
            Phi_domain_min = s_min_per_q.min().item()
            Phi_domain_max = s_max_per_q.max().item()
            if Phi_domain_min >= Phi_domain_max:
                Phi_domain_max = Phi_domain_min + 1e-4
            
            self.Phi.update_domain((Phi_domain_min, Phi_domain_max), 
                                   allow_resampling=allow_resampling,
                                   force_resample=force_resample)
            
            # Initialize codomain to match domain (one time only)
            if CONFIG['train_phi_codomain'] and not self.codomain_initialized_from_domain:
                self.initialize_codomain_from_domain(Phi_domain_min, Phi_domain_max)
                self.codomain_initialized_from_domain = True
    
    def initialize_codomain_from_domain(self, domain_min, domain_max):
        """Initialize Φ codomain to match its computed domain."""
        with torch.no_grad():
            domain_center = (domain_min + domain_max) / 2
            domain_radius = (domain_max - domain_min) / 2
            
            # Update codomain parameters
            self.phi_codomain_params.cc.data = torch.tensor(domain_center, 
                                                           device=self.phi_codomain_params.cc.device,
                                                           dtype=self.phi_codomain_params.cc.dtype)
            self.phi_codomain_params.cr.data = torch.tensor(domain_radius,
                                                           device=self.phi_codomain_params.cr.device,
                                                           dtype=self.phi_codomain_params.cr.dtype)
            
            # Also initialize the spline coefficients to be approximately linear
            # This makes Φ start as identity-like
            self.Phi.initialize_as_identity(domain_min, domain_max)
            
            if CONFIG.get('debug_domains', False):
                print(f"Layer {self.layer_num}: Initialized Phi codomain to match domain [{domain_min:.3f}, {domain_max:.3f}]")
                print(f"  cc = {domain_center:.3f}, cr = {domain_radius:.3f}")
    
    def compute_pooling_residual_bounds(self, input_interval):
        """
        Compute tight bounds for pooling residual contributions (union across outputs).
        
        NOTE: Kept for backward compatibility. The per-output variant is used for
        tighter composition in compute_output_range_theoretical.
        """
        if self.residual_pooling_weights is None:
            return Interval(0, 0)
        
        with torch.no_grad():
            # Initialize bounds for each output
            output_bounds = []
            
            for out_idx in range(self.d_out):
                # Find which inputs contribute to this output
                contributing_inputs = (self.pooling_assignment == out_idx).nonzero(as_tuple=True)[0]
                
                if len(contributing_inputs) == 0:
                    output_bounds.append(Interval(0, 0))
                    continue
                
                # Compute contribution from each assigned input
                contribution_min = 0
                contribution_max = 0
                
                for in_idx in contributing_inputs:
                    weight = self.residual_pooling_weights[in_idx].item()
                    
                    if weight >= 0:
                        contribution_min += weight * input_interval.min
                        contribution_max += weight * input_interval.max
                    else:
                        contribution_min += weight * input_interval.max
                        contribution_max += weight * input_interval.min
                
                output_bounds.append(Interval(contribution_min, contribution_max))
            
            # Return the union of all output bounds
            final_bounds = output_bounds[0]
            for bounds in output_bounds[1:]:
                final_bounds = final_bounds.union(bounds)
            
            return final_bounds

    def compute_pooling_residual_bounds_per_output(self, input_interval, a_per_dim=None, b_per_dim=None):
        """
        NEW (tight): Compute per-output pooling residual intervals for composition.
        Supports optional per-dimension intervals (a_per_dim, b_per_dim).
        Returns:
            r_min_per_q (tensor shape [d_out]), r_max_per_q (tensor shape [d_out])
        """
        device = self.phi.knots.device
        if self.residual_pooling_weights is None:
            return (torch.zeros(self.d_out, device=device),
                    torch.zeros(self.d_out, device=device))
        
        with torch.no_grad():
            w = self.residual_pooling_weights.to(device)  # (d_in,)
            assign = self.pooling_assignment.to(device)   # (d_in,)
            
            if a_per_dim is not None and b_per_dim is not None:
                a_vec = torch.as_tensor(a_per_dim, device=device, dtype=torch.float32)
                b_vec = torch.as_tensor(b_per_dim, device=device, dtype=torch.float32)
                # Contribution of each input (min/max depending on weight sign) USING per-dim intervals
                contrib_min = torch.where(w >= 0, w * a_vec, w * b_vec)
                contrib_max = torch.where(w >= 0, w * b_vec, w * a_vec)
            else:
                # Fallback to scalar interval
                a, b = input_interval.min, input_interval.max
                contrib_min = torch.where(w >= 0, w * a, w * b)
                contrib_max = torch.where(w >= 0, w * b, w * a)
            
            # Sum contributions per assigned output using scatter_add
            r_min_per_q = torch.zeros(self.d_out, device=device)
            r_max_per_q = torch.zeros(self.d_out, device=device)
            r_min_per_q.scatter_add_(0, assign, contrib_min)
            r_max_per_q.scatter_add_(0, assign, contrib_max)
            
            return r_min_per_q, r_max_per_q

    def _compute_linear_residual_bounds_per_output(self, a_vec, b_vec):
        """
        STANDARD residual bounds for projection matrix W (d_in x d_out):
            r_q = sum_i x_i * W_{i,q}
        Per-output tight intervals using per-dimension bounds:
            min = a·W^+ + b·W^- ,  max = b·W^+ + a·W^-
        Returns: r_min_per_q, r_max_per_q with shape [d_out].
        """
        device = self.phi.knots.device
        W = self.residual_projection.to(device)  # (d_in, d_out)
        W_pos = torch.clamp(W, min=0)
        W_neg = torch.clamp(W, max=0)
        a_vec = torch.as_tensor(a_vec, device=device, dtype=torch.float32)
        b_vec = torch.as_tensor(b_vec, device=device, dtype=torch.float32)
        # Row-vector times matrix -> (d_out,)
        r_min = torch.matmul(a_vec, W_pos) + torch.matmul(b_vec, W_neg)
        r_max = torch.matmul(b_vec, W_pos) + torch.matmul(a_vec, W_neg)
        return r_min, r_max
    
    def compute_output_range_theoretical(self):
        """Compute theoretical output range of this block using per-q tight composition (uses per-channel intervals when available)."""
        with torch.no_grad():
            device = self.phi.knots.device
            q_values = torch.arange(self.d_out, device=device, dtype=torch.float32)
            
            # Prepare per-dimension input intervals if provided
            if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                if a_vec.numel() != self.d_in or b_vec.numel() != self.d_in:
                    a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                    b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                # Tight s-bounds using per-dim intervals
                s_min_per_q, s_max_per_q = self._compute_s_bounds_per_q(a_vec, b_vec)
            else:
                # Fallback: use global bounds with λ+ / λ− trick (original behavior)
                eta_val = self.eta.item()
                phi_inputs_min = self.input_range.min + eta_val * q_values
                phi_inputs_max = self.input_range.max + eta_val * q_values
                phi_at_min = self.phi(phi_inputs_min)
                phi_at_max = self.phi(phi_inputs_max)
                phi_min_per_q = torch.minimum(phi_at_min, phi_at_max)
                phi_max_per_q = torch.maximum(phi_at_min, phi_at_max)
                lambda_pos = torch.clamp(self.lambdas, min=0).sum().item()
                lambda_neg = torch.clamp(self.lambdas, max=0).sum().item()
                s_min_per_q = lambda_pos * phi_min_per_q + lambda_neg * phi_max_per_q + Q_VALUES_FACTOR * q_values
                s_max_per_q = lambda_pos * phi_max_per_q + lambda_neg * phi_min_per_q + Q_VALUES_FACTOR * q_values
            
            # Lateral mixing (now sign-aware endpoint bounds)
            if self.lateral_scale is not None:
                s_min_per_q, s_max_per_q = self._apply_lateral_mixing_bounds_per_q(s_min_per_q, s_max_per_q)
            
            # Per-q Φ output ranges (evaluate endpoints + interior knots)
            Phi_at_smin = self.Phi(s_min_per_q)
            Phi_at_smax = self.Phi(s_max_per_q)
            y_min_per_q = torch.minimum(Phi_at_smin, Phi_at_smax)
            y_max_per_q = torch.maximum(Phi_at_smin, Phi_at_smax)
            
            # Check interior knots per q
            knots = self.Phi.knots  # (K,)
            Phi_at_knots = self.Phi(knots)  # (K,)
            K = knots.shape[0]
            mask = (knots.unsqueeze(0) >= s_min_per_q.unsqueeze(1)) & (knots.unsqueeze(0) <= s_max_per_q.unsqueeze(1))
            if mask.any():
                Yk = Phi_at_knots.unsqueeze(0).expand(self.d_out, K)
                Yk_min = torch.where(mask, Yk, torch.full_like(Yk, float('inf'))).min(dim=1).values
                Yk_max = torch.where(mask, Yk, torch.full_like(Yk, float('-inf'))).max(dim=1).values
                has_knots = mask.any(dim=1)
                y_min_per_q = torch.where(has_knots, torch.minimum(y_min_per_q, Yk_min), y_min_per_q)
                y_max_per_q = torch.where(has_knots, torch.maximum(y_max_per_q, Yk_max), y_max_per_q)
            
            # Residuals per q (now includes standard 'linear' projection case)
            if CONFIG['use_residual_weights'] and self.input_range is not None:
                if self.residual_weight is not None:
                    # Scalar residual per output (d_in == d_out): per-q bounds via per-dim intervals
                    w = self.residual_weight.to(device)
                    if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                        a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                        b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                        if a_vec.numel() != self.d_in or b_vec.numel() != self.d_in:
                            a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                            b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                        r_min_per_q = torch.where(w >= 0, w * a_vec, w * b_vec)
                        r_max_per_q = torch.where(w >= 0, w * b_vec, w * a_vec)
                    else:
                        # Fallback to global interval
                        a, b = self.input_range.min, self.input_range.max
                        r_lo, r_hi = (w * a, w * b) if w.item() >= 0 else (w * b, w * a)
                        r_min_per_q = torch.full((self.d_out,), r_lo.item(), device=device)
                        r_max_per_q = torch.full((self.d_out,), r_hi.item(), device=device)
                elif self.residual_projection is not None:
                    # STANDARD linear projection case (d_in != d_out)
                    if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                        a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                        b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                        if a_vec.numel() != self.d_in or b_vec.numel() != self.d_in:
                            a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                            b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                    else:
                        a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                        b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                    r_min_per_q, r_max_per_q = self._compute_linear_residual_bounds_per_output(a_vec, b_vec)
                elif self.residual_pooling_weights is not None:
                    if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                        a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                        b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                        r_min_per_q, r_max_per_q = self.compute_pooling_residual_bounds_per_output(
                            Interval(self.input_range.min, self.input_range.max),
                            a_per_dim=a_vec, b_per_dim=b_vec
                        )
                    else:
                        # Fallback to global interval
                        a, b = self.input_range.min, self.input_range.max
                        r_min_per_q, r_max_per_q = self.compute_pooling_residual_bounds_per_output(Interval(a, b))
                        r_min_per_q = r_min_per_q.to(device)
                        r_max_per_q = r_max_per_q.to(device)
                elif self.residual_broadcast_weights is not None:
                    # Broadcasting case: per-q via per-dim intervals if available
                    w = self.residual_broadcast_weights.to(device)  # (d_out,)
                    src = self.broadcast_sources.to(device)         # (d_out,)
                    if self.input_min_per_dim is not None and self.input_max_per_dim is not None:
                        a_vec = torch.as_tensor(self.input_min_per_dim, device=device, dtype=torch.float32)
                        b_vec = torch.as_tensor(self.input_max_per_dim, device=device, dtype=torch.float32)
                        if a_vec.numel() != self.d_in or b_vec.numel() != self.d_in:
                            a_vec = torch.full((self.d_in,), self.input_range.min, device=device)
                            b_vec = torch.full((self.d_in,), self.input_range.max, device=device)
                        a_src = a_vec[src]  # (d_out,)
                        b_src = b_vec[src]  # (d_out,)
                        r_min_per_q = torch.where(w >= 0, w * a_src, w * b_src)
                        r_max_per_q = torch.where(w >= 0, w * b_src, w * a_src)
                    else:
                        a, b = self.input_range.min, self.input_range.max
                        r_min_per_q = torch.where(w >= 0, w * a, w * b)
                        r_max_per_q = torch.where(w >= 0, w * b, w * a)
                else:
                    r_min_per_q = torch.zeros(self.d_out, device=device)
                    r_max_per_q = torch.zeros(self.d_out, device=device)
            else:
                r_min_per_q = torch.zeros(self.d_out, device=device)
                r_max_per_q = torch.zeros(self.d_out, device=device)
            
            # Compose Φ and residual per-q (safe Minkowski sum)
            final_min_per_q = y_min_per_q + r_min_per_q
            final_max_per_q = y_max_per_q + r_max_per_q

            # Store per-q outputs for downstream per-channel propagation
            self.output_min_per_q = final_min_per_q.detach().clone()
            self.output_max_per_q = final_max_per_q.detach().clone()

            # Aggregate to layer output_range
            if self.is_final:
                # Final block sums its outputs -> use Minkowski sum across q
                final_min = final_min_per_q.sum().item()
                final_max = final_max_per_q.sum().item()
            else:
                # Vector output: union across components for global min/max
                final_min = final_min_per_q.min().item()
                final_max = final_max_per_q.max().item()
        
        self.output_range = TheoreticalRange(final_min, final_max)
        return self.output_range
    
    def get_theoretical_ranges(self):
        """Get theoretical ranges for checkpoint saving."""
        ranges = {}
        if self.input_range is not None:
            ranges['input_range'] = (self.input_range.min, self.input_range.max)
        if self.output_range is not None:
            ranges['output_range'] = (self.output_range.min, self.output_range.max)
        # Also save the codomain initialization flag
        ranges['codomain_initialized'] = self.codomain_initialized_from_domain
        return ranges
    
    def set_theoretical_ranges(self, ranges):
        """Restore theoretical ranges from checkpoint."""
        if 'input_range' in ranges:
            self.input_range = TheoreticalRange(ranges['input_range'][0], ranges['input_range'][1])
        if 'output_range' in ranges:
            self.output_range = TheoreticalRange(ranges['output_range'][0], ranges['output_range'][1])
        # Restore the codomain initialization flag
        if 'codomain_initialized' in ranges:
            self.codomain_initialized_from_domain = ranges['codomain_initialized']
    
    def forward(self, x, x_original=None):
        """Forward pass with configurable memory mode."""
        # Check if we should use memory-efficient mode
        if CONFIG.get('low_memory_mode', False):
            return self._forward_memory_efficient(x, x_original)
        else:
            return self._forward_original(x, x_original)
    
    def _forward_original(self, x, x_original=None):
        """Original forward pass - O(B × d_in × d_out) memory."""
        # Ensure the q_values buffer is on the same device as the input tensor.
        q_on_device = self.q_values.to(x.device)

        x_expanded = x.unsqueeze(-1)  # shape: (batch_size, d_in, 1)
        q = q_on_device.view(1, 1, -1)  # shape: (1, 1, d_out)
        
        # Apply translation by η * q (part of Sprecher's construction)
        shifted = x_expanded + self.eta * q
        
        # Apply inner monotonic spline φ to the shifted inputs
        phi_out = self.phi(shifted)  # shape: (batch_size, d_in, d_out)
        
        # Weight by λ (now a VECTOR) and sum over input dimension
        weighted = phi_out * self.lambdas.view(1, -1, 1)  # Broadcast: (1, d_in, 1)
        # Use the device-corrected q_on_device tensor here as well
        s = weighted.sum(dim=1) + Q_VALUES_FACTOR * q_on_device  # shape: (batch_size, d_out)
        
        # Apply lateral mixing if enabled
        if self.lateral_scale is not None:
            s = self._apply_lateral_mixing_to_s(s, x.device)
        
        # Apply Phi
        activated = self.Phi(s)
        
        # Add residual connections
        activated = self._add_residual(activated, x_original)
        
        if self.is_final:
            # Sum outputs if this is the final block (produces scalar output)
            return activated.sum(dim=1, keepdim=True)
        else:
            return activated
    
    def _forward_memory_efficient(self, x, x_original=None):
        """Memory-efficient forward pass - O(B × max(d_in, d_out)) memory."""
        batch_size = x.shape[0]
        device = x.device
        
        # Pre-allocate output tensor for s values
        s = torch.zeros(batch_size, self.d_out, device=device, dtype=x.dtype)
        
        # Process each output dimension sequentially
        for q_idx in range(self.d_out):
            # Get the q value for this output dimension (as scalar to avoid tensor ops)
            q_val = float(q_idx)
            
            # Compute phi for all inputs with this specific q
            # Memory: O(B × d_in) instead of O(B × d_in × d_out)
            shifted = x + self.eta * q_val  # (batch_size, d_in)
            phi_out = self.phi(shifted)     # (batch_size, d_in)
            
            # Weight by lambdas and sum over input dimension
            weighted = phi_out * self.lambdas  # (batch_size, d_in) 
            s[:, q_idx] = weighted.sum(dim=1) + Q_VALUES_FACTOR * q_val  # (batch_size,)
        
        # Apply lateral mixing if enabled (on s before Phi)
        if self.lateral_scale is not None:
            s = self._apply_lateral_mixing_to_s(s, device)
        
        # Apply Phi to all outputs (this is memory-efficient as it's element-wise)
        activated = self.Phi(s)  # (batch_size, d_out)
        
        # Add residual connections
        activated = self._add_residual(activated, x_original)
        
        # Handle final summing if needed
        if self.is_final:
            return activated.sum(dim=1, keepdim=True)
        else:
            return activated
    
    def _apply_lateral_mixing_to_s(self, s, device):
        """Apply lateral mixing to the intermediate representation s."""
        if CONFIG['lateral_mixing_type'] == 'bidirectional':
            s_forward = s[:, self.lateral_indices_forward.to(device)]
            s_backward = s[:, self.lateral_indices_backward.to(device)]
            s_mixed = s + self.lateral_scale * (
                self.lateral_weights_forward * s_forward + 
                self.lateral_weights_backward * s_backward
            )
        else:  # 'cyclic'
            s_shifted = s[:, self.lateral_indices.to(device)]
            s_mixed = s + self.lateral_scale * (self.lateral_weights * s_shifted)
        return s_mixed
    
    def _add_residual(self, activated, x_original):
        """Add residual connections if enabled."""
        if not CONFIG['use_residual_weights'] or x_original is None:
            return activated
        
        if self.residual_projection is not None:
            # STANDARD linear projection residual: x @ W
            activated = activated + torch.matmul(x_original, self.residual_projection)
        
        elif self.residual_weight is not None:
            # Same dimension case - simple scalar weight
            activated = activated + self.residual_weight * x_original
            
        elif self.residual_pooling_weights is not None:
            # Pooling case: d_in > d_out
            weighted_input = x_original * self.residual_pooling_weights.view(1, -1)
            residual_contribution = torch.zeros(
                x_original.shape[0], self.d_out, 
                device=x_original.device, dtype=x_original.dtype
            )
            residual_contribution.scatter_add_(
                1, 
                self.pooling_assignment.unsqueeze(0).expand(x_original.shape[0], -1),
                weighted_input
            )
            activated = activated + residual_contribution
            
        elif self.residual_broadcast_weights is not None:
            # Broadcasting case: d_in < d_out
            residual_contribution = torch.gather(
                x_original, 1,
                self.broadcast_sources.unsqueeze(0).expand(x_original.shape[0], -1)
            )
            residual_contribution = residual_contribution * self.residual_broadcast_weights.view(1, -1)
            activated = activated + residual_contribution
        
        return activated
    
    def get_output_range(self):
        """Get the current output range of this block (for the next layer's input)."""
        # Return the computed theoretical output range
        if self.output_range is None:
            return self.compute_output_range_theoretical()
        return self.output_range
    
    def get_domain_violation_stats(self):
        """Get domain violation statistics for this block."""
        stats = {
            'phi': self.phi.get_domain_violation_stats(),
            'Phi': self.Phi.get_domain_violation_stats()
        }
        return stats
    
    def reset_domain_violation_stats(self):
        """Reset domain violation tracking for this block."""
        self.phi.reset_domain_violation_stats()
        self.Phi.reset_domain_violation_stats()


class SprecherMultiLayerNetwork(nn.Module):
    """
    Builds the Sprecher network with a given hidden-layer architecture and final output dimension.
    
    Args:
        input_dim: Input dimension
        architecture: List of hidden layer sizes
        final_dim: Output dimension
        phi_knots: Number of knots for phi splines
        Phi_knots: Number of knots for Phi splines
        norm_type: Type of normalization ('none', 'batch', 'layer')
        norm_position: Position of normalization ('before', 'after')
        norm_skip_first: Whether to skip normalization for first block
        initialize_domains: Whether to initialize domains on creation (set False when loading checkpoint)
        domain_ranges: Dict of domain ranges for each layer's splines (optional)
    Note:
        Residual style is controlled by CONFIG['residual_style'] ∈ {'node', 'linear'}.
        Default is 'node' to preserve original behavior.
    """
    def __init__(self, input_dim, architecture, final_dim=1, phi_knots=100, Phi_knots=100,
                 norm_type='none', norm_position='after', norm_skip_first=True, initialize_domains=True,
                 domain_ranges=None):
        super().__init__()
        self.input_dim = input_dim
        self.architecture = architecture
        self.final_dim = final_dim
        self.norm_type = norm_type
        self.norm_position = norm_position
        self.norm_skip_first = norm_skip_first
        
        layers = []
        if not architecture: # No hidden layers
            is_final = (final_dim == 1)
            # Get domain ranges if provided
            phi_domain = domain_ranges.get('layer_0_phi') if domain_ranges else None
            Phi_domain = domain_ranges.get('layer_0_Phi') if domain_ranges else None
            layers.append(SprecherLayerBlock(
                d_in=input_dim, d_out=final_dim,
                layer_num=0, is_final=is_final,
                phi_knots=phi_knots, Phi_knots=Phi_knots,
                phi_domain=phi_domain, Phi_domain=Phi_domain
            ))
        else:
            L = len(architecture)
            # Create hidden layers
            d_in = input_dim
            for i in range(L):
                d_out = architecture[i]
                # The last hidden block is summed if the final output is scalar
                is_final_block = (i == L - 1) and (self.final_dim == 1)
                # Get domain ranges if provided
                phi_domain = domain_ranges.get(f'layer_{i}_phi') if domain_ranges else None
                Phi_domain = domain_ranges.get(f'layer_{i}_Phi') if domain_ranges else None
                layers.append(SprecherLayerBlock(
                    d_in=d_in, d_out=d_out,
                    layer_num=i, is_final=is_final_block,
                    phi_knots=phi_knots, Phi_knots=Phi_knots,
                    phi_domain=phi_domain, Phi_domain=Phi_domain
                ))
                d_in = d_out
            
            # Add a final output block if output is vector-valued
            if self.final_dim > 1:
                # Get domain ranges if provided
                phi_domain = domain_ranges.get(f'layer_{L}_phi') if domain_ranges else None
                Phi_domain = domain_ranges.get(f'layer_{L}_Phi') if domain_ranges else None
                layers.append(SprecherLayerBlock(
                    d_in=d_in, d_out=self.final_dim,
                    layer_num=L, is_final=False,
                    phi_knots=phi_knots, Phi_knots=Phi_knots,
                    phi_domain=phi_domain, Phi_domain=Phi_domain
                ))
        
        self.layers = nn.ModuleList(layers)
        
        # Create normalization layers if requested
        self.norm_layers = nn.ModuleList()
        if norm_type != 'none':
            for i, layer in enumerate(self.layers):
                # Skip normalization for first block if requested
                if norm_skip_first and i == 0:
                    self.norm_layers.append(nn.Identity())
                else:
                    # Determine the number of features based on position
                    if norm_position == 'before':
                        # Before the block, use input dimension
                        num_features = layer.d_in
                    else:  # after
                        # After the block, use output dimension
                        # Note: if is_final, output is scalar (1D)
                        num_features = 1 if layer.is_final else layer.d_out
                    
                    if norm_type == 'batch':
                        self.norm_layers.append(nn.BatchNorm1d(num_features))
                    elif norm_type == 'layer':
                        self.norm_layers.append(nn.LayerNorm(num_features))
                    else:
                        raise ValueError(f"Unknown norm_type: {norm_type}")
        
        # Output scaling parameters for better initialization
        self.output_scale = nn.Parameter(torch.tensor(1.0))
        self.output_bias = nn.Parameter(torch.tensor(0.0))
        
        # Initialize domains based on theoretical bounds
        # Can be deferred when loading from checkpoint to avoid conflicts
        if initialize_domains and CONFIG.get('use_theoretical_domains', True):
            self.update_all_domains()
    
    def update_all_domains(self, allow_resampling=True, force_resample=False):
        """
        Update all spline domains based on theoretical bounds WITH normalization handling.
        
        Args:
            allow_resampling: If False, prevent spline coefficient resampling during domain updates
            force_resample: If True, force resampling regardless of total_evaluations
        """
        device = next(self.parameters()).device if any(True for _ in self.parameters()) else torch.device('cpu')

        # Start with input in [0,1]^n
        current_interval = Interval(0.0, 1.0)
        # NEW: per-channel intervals for propagation (start uniform)
        current_min_per_dim = torch.zeros(self.input_dim, device=device)
        current_max_per_dim = torch.ones(self.input_dim, device=device)
        
        for layer_idx, layer in enumerate(self.layers):
            # Handle normalization BEFORE block if configured
            if self.norm_type != 'none' and self.norm_position == 'before':
                norm_layer = self.norm_layers[layer_idx]
                if not isinstance(norm_layer, nn.Identity):
                    if isinstance(norm_layer, nn.BatchNorm1d):
                        # Compute global interval (training conservative)
                        current_interval = compute_batchnorm_bounds(
                            current_interval, norm_layer, training_mode=self.training
                        )
                        # Propagate per-dim as uniform (conservative)
                        current_min_per_dim = torch.full((layer.d_in,), current_interval.min, device=device)
                        current_max_per_dim = torch.full((layer.d_in,), current_interval.max, device=device)
                    elif isinstance(norm_layer, nn.LayerNorm):
                        current_interval = compute_layernorm_bounds(
                            current_interval, norm_layer, layer.d_in
                        )
                        # Propagate per-dim as uniform (conservative)
                        current_min_per_dim = torch.full((layer.d_in,), current_interval.min, device=device)
                        current_max_per_dim = torch.full((layer.d_in,), current_interval.max, device=device)
            
            # Update layer's per-channel inputs for tighter computations
            layer.input_min_per_dim = current_min_per_dim.detach().clone().to(layer.phi.knots.device)
            layer.input_max_per_dim = current_max_per_dim.detach().clone().to(layer.phi.knots.device)

            # Update layer's input range (global)
            current_range = TheoreticalRange(current_interval.min, current_interval.max)
            
            # Update φ domain based on input range
            layer.update_phi_domain_theoretical(current_range, allow_resampling, force_resample)
            
            # Update Φ domain using available per-channel info
            layer.update_Phi_domain_theoretical(allow_resampling, force_resample)
            
            # Compute this layer's output range
            layer.compute_output_range_theoretical()
            
            # Prepare per-channel outputs for next layer (pre-normalization)
            if layer.is_final:
                # If final (sum) block, output is scalar; next layer doesn't exist in standard arch
                next_min_per_dim = torch.tensor([layer.output_range.min], device=device)
                next_max_per_dim = torch.tensor([layer.output_range.max], device=device)
            else:
                next_min_per_dim = layer.output_min_per_q.to(device)
                next_max_per_dim = layer.output_max_per_q.to(device)
            
            # Get global output interval (for normalization and logging)
            output_interval = Interval(layer.output_range.min, layer.output_range.max)
            
            # Handle normalization AFTER block if configured
            if self.norm_type != 'none' and self.norm_position == 'after':
                norm_layer = self.norm_layers[layer_idx]
                if not isinstance(norm_layer, nn.Identity):
                    if isinstance(norm_layer, nn.BatchNorm1d):
                        # Global (safe) interval for BN (training conservative)
                        output_interval = compute_batchnorm_bounds(
                            output_interval, norm_layer, training_mode=self.training
                        )
                        # Per-channel: if in EVAL, we can transform per-dim bounds using running stats+affine
                        if not self.training:
                            eps = norm_layer.eps
                            running_mean = norm_layer.running_mean.to(device)
                            running_var = norm_layer.running_var.to(device)
                            std = torch.sqrt(running_var + eps)
                            # Standardize per channel
                            lo = (next_min_per_dim - running_mean) / std
                            hi = (next_max_per_dim - running_mean) / std
                            std_min = torch.minimum(lo, hi)
                            std_max = torch.maximum(lo, hi)
                            if norm_layer.affine:
                                w = norm_layer.weight.to(device)
                                b = norm_layer.bias.to(device)
                                next_min_per_dim = torch.where(w >= 0, w * std_min + b, w * std_max + b)
                                next_max_per_dim = torch.where(w >= 0, w * std_max + b, w * std_min + b)
                            else:
                                next_min_per_dim = std_min
                                next_max_per_dim = std_max
                        else:
                            # Training: use uniform conservative range
                            next_min_per_dim = torch.full_like(next_min_per_dim, output_interval.min)
                            next_max_per_dim = torch.full_like(next_max_per_dim, output_interval.max)
                    elif isinstance(norm_layer, nn.LayerNorm):
                        # Apply conservative global bound (feature-coupled)
                        num_features = 1 if layer.is_final else layer.d_out
                        output_interval = compute_layernorm_bounds(
                            output_interval, norm_layer, num_features
                        )
                        # Per-dim arrays become uniform conservative bounds
                        next_min_per_dim = torch.full_like(next_min_per_dim, output_interval.min)
                        next_max_per_dim = torch.full_like(next_max_per_dim, output_interval.max)
            
            # Update for next layer
            current_min_per_dim = next_min_per_dim
            current_max_per_dim = next_max_per_dim
            # Global interval for next φ update
            current_interval = Interval(current_min_per_dim.min().item(), current_max_per_dim.max().item())
            
            # Debug output if configured
            if CONFIG.get('debug_domains', False):
                print(f"Layer {layer.layer_num}: input_range={layer.input_range}, output_range={layer.output_range}")
                print(f"  phi domain: [{layer.phi.in_min:.3f}, {layer.phi.in_max:.3f}]")
                print(f"  Phi domain: [{layer.Phi.in_min:.3f}, {layer.Phi.in_max:.3f}]")
                if layer_idx < len(self.norm_layers) and self.norm_type != 'none':
                    print(f"  After normalization: [{current_interval.min:.3f}, {current_interval.max:.3f}]")
    
    def forward(self, x):
        x_in = x
        for i, layer in enumerate(self.layers):
            # Apply normalization before block if requested
            if self.norm_type != 'none' and self.norm_position == 'before':
                x_in = self.norm_layers[i](x_in)
            
            # For residual connections, pass the input of the *current* layer
            x_out = layer(x_in, x_in) 
            
            # Apply normalization after block if requested
            if self.norm_type != 'none' and self.norm_position == 'after':
                x_out = self.norm_layers[i](x_out)
            
            x_in = x_out # The output of this layer is the input to the next
        
        # Final output after all layers
        x = x_in
        
        # Apply output scaling and bias
        x = self.output_scale * x + self.output_bias
        return x
    
    def get_domain_violation_stats(self):
        """Get domain violation statistics for all layers."""
        stats = {}
        for i, layer in enumerate(self.layers):
            stats[f'layer_{i}'] = layer.get_domain_violation_stats()
        return stats
    
    def reset_domain_violation_stats(self):
        """Reset domain violation tracking for all layers."""
        for layer in self.layers:
            layer.reset_domain_violation_stats()
    
    def print_domain_violation_report(self):
        """Print a summary of domain violations."""
        if not CONFIG.get('track_domain_violations', False):
            print("Domain violation tracking is not enabled.")
            return
        
        print("\nDomain Violation Report:")
        print("-" * 50)
        stats = self.get_domain_violation_stats()
        for layer_name, layer_stats in stats.items():
            print(f"{layer_name}:")
            print(f"  phi violations: {layer_stats['phi']:.2%}")
            print(f"  Phi violations: {layer_stats['Phi']:.2%}")
    
    def get_all_domain_states(self):
        """Get domain states for all splines in all layers."""
        domain_states = {}
        for i, layer in enumerate(self.layers):
            domain_states[f'layer_{i}_phi'] = layer.phi.get_domain_state()
            domain_states[f'layer_{i}_Phi'] = layer.Phi.get_domain_state()
            # Also save theoretical ranges
            domain_states[f'layer_{i}_ranges'] = layer.get_theoretical_ranges()
        return domain_states
    
    def get_domain_ranges(self):
        """Get just the domain ranges for all splines (for initialization)."""
        domain_ranges = {}
        for i, layer in enumerate(self.layers):
            domain_ranges[f'layer_{i}_phi'] = (layer.phi.in_min, layer.phi.in_max)
            domain_ranges[f'layer_{i}_Phi'] = (layer.Phi.in_min, layer.Phi.in_max)
        return domain_ranges
    
    def set_all_domain_states(self, domain_states):
        """Restore domain states for all splines in all layers."""
        for i, layer in enumerate(self.layers):
            phi_key = f'layer_{i}_phi'
            Phi_key = f'layer_{i}_Phi'
            ranges_key = f'layer_{i}_ranges'
            
            if phi_key in domain_states:
                layer.phi.set_domain_state(domain_states[phi_key])
            if Phi_key in domain_states:
                layer.Phi.set_domain_state(domain_states[Phi_key])
            # Restore theoretical ranges
            if ranges_key in domain_states:
                layer.set_theoretical_ranges(domain_states[ranges_key])


def test_domain_tightness(model, dataset, n_samples=10000):
    """Test that computed domains are both safe and tight."""
    device = next(model.parameters()).device
    
    # Generate many samples to test domain coverage
    x_test = torch.rand(n_samples, dataset.input_dim, device=device)
    
    # Track actual min/max values seen at each spline
    actual_ranges = {}
    
    def hook_fn(name):
        def hook(module, input, output):
            if hasattr(module, 'in_min'):  # It's a spline
                actual_min = input[0].min().item()
                actual_max = input[0].max().item()
                
                if name not in actual_ranges:
                    actual_ranges[name] = Interval(actual_min, actual_max)
                else:
                    actual_ranges[name] = actual_ranges[name].union(
                        Interval(actual_min, actual_max)
                    )
                
                # Check safety: no values outside computed domain
                if actual_min < module.in_min - 1e-6 or actual_max > module.in_max + 1e-6:
                    print(f"SAFETY VIOLATION in {name}:")
                    print(f"  Computed: [{module.in_min:.6f}, {module.in_max:.6f}]")
                    print(f"  Actual:   [{actual_min:.6f}, {actual_max:.6f}]")
        return hook
    
    # Register hooks
    handles = []
    for i, layer in enumerate(model.layers):
        handles.append(layer.phi.register_forward_hook(hook_fn(f"layer_{i}_phi")))
        handles.append(layer.Phi.register_forward_hook(hook_fn(f"layer_{i}_Phi")))
    
    # Run forward passes
    with torch.no_grad():
        for batch_start in range(0, n_samples, 100):
            batch_end = min(batch_start + 100, n_samples)
            _ = model(x_test[batch_start:batch_end])
    
    # Remove hooks
    for handle in handles:
        handle.remove()
    
    # Check tightness: how much slack in computed domains?
    print("\nDomain Tightness Analysis:")
    print("=" * 60)
    violations_found = False
    
    for i, layer in enumerate(model.layers):
        phi_name = f"layer_{i}_phi"
        Phi_name = f"layer_{i}_Phi"
        
        if phi_name in actual_ranges:
            actual = actual_ranges[phi_name]
            computed = Interval(layer.phi.in_min, layer.phi.in_max)
            slack_min = actual.min - computed.min
            slack_max = computed.max - actual.max
            
            # Check for violations
            if actual.min < computed.min - 1e-6 or actual.max > computed.max + 1e-6:
                violations_found = True
                print(f"{phi_name}: VIOLATION DETECTED")
            else:
                print(f"✓ {phi_name}:")
            
            print(f"  Computed: {computed}")
            print(f"  Actual:   {actual}")
            print(f"  Slack:    [{slack_min:.6f}, {slack_max:.6f}]")
        
        if Phi_name in actual_ranges:
            actual = actual_ranges[Phi_name]
            computed = Interval(layer.Phi.in_min, layer.Phi.in_max)
            slack_min = actual.min - computed.min
            slack_max = computed.max - actual.max
            
            # Check for violations
            if actual.min < computed.min - 1e-6 or actual.max > computed.max + 1e-6:
                violations_found = True
                print(f"{Phi_name}: VIOLATION DETECTED")
            else:
                print(f"✓ {Phi_name}:")
            
            print(f"  Computed: {computed}")
            print(f"  Actual:   {actual}")
            print(f"  Slack:    [{slack_min:.6f}, {slack_max:.6f}]")
    
    print("=" * 60)
    if violations_found:
        print("DOMAIN VIOLATIONS DETECTED - Computed domains are not safe!")
    else:
        print("✓ All domains are safe - no violations detected")
    
    return not violations_found

----------
sn_core/train.py:
----------

# sn_core/train.py

"""Training utilities for Sprecher Networks.

Adds optional standard residuals support via CONFIG['residual_style'] ∈ {'node','linear'}.
- 'node'   (default): existing node-centric residuals (scalar/pooling/broadcast)
- 'linear': standard residuals (α·x when d_in==d_out; x @ W projection when d_in!=d_out)
"""

import torch
import torch.nn as nn
import numpy as np
import copy
from contextlib import contextmanager
from tqdm import tqdm
from .model import SprecherMultiLayerNetwork
from .config import CONFIG


class PlateauAwareCosineAnnealingLR:
    """Custom scheduler that increases learning rate when stuck in plateau."""
    
    def __init__(self, optimizer, base_lr, max_lr, patience=1000, threshold=1e-4):
        self.optimizer = optimizer
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.patience = patience
        self.threshold = threshold
        self.best_loss = float('inf')
        self.plateau_counter = 0
        self.cycle_length = 2000
        self.current_step = 0
        
    def step(self, loss):
        self.current_step += 1
        
        # Check for plateau
        if abs(self.best_loss - loss) < self.threshold:
            self.plateau_counter += 1
        else:
            self.plateau_counter = 0
            if loss < self.best_loss:
                self.best_loss = loss
        
        # If in plateau, use higher learning rate
        if self.plateau_counter > self.patience:
            lr = self.max_lr
            self.plateau_counter = 0  # Reset counter
        else:
            # Cosine annealing
            progress = (self.current_step % self.cycle_length) / self.cycle_length
            lr = self.base_lr + 0.5 * (self.max_lr - self.base_lr) * (1 + np.cos(np.pi * progress))
        
        # Update learning rates
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr * param_group.get('lr_scale', 1.0)
        
        return lr


def recalculate_bn_stats(model, x_train, num_passes=10):
    """Recalculate BatchNorm statistics using the training data."""
    was_training = model.training
    
    # First, reset all BatchNorm running statistics
    for module in model.modules():
        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            module.reset_running_stats()
            # Also reset momentum to accumulate stats faster
            module.momentum = 0.1
    
    # Set to train mode to update running stats
    model.train()
    
    # Run through the data multiple times to accumulate statistics
    with torch.no_grad():
        for pass_idx in range(num_passes):
            _ = model(x_train)
            # For the last few passes, reduce momentum to stabilize
            if pass_idx >= num_passes - 3:
                for module in model.modules():
                    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                        module.momentum = 0.01
    
    # Restore original mode
    if not was_training:
        model.eval()


def has_batchnorm(model):
    """Check if model contains any BatchNorm layers (1D/2D/3D)."""
    for module in model.modules():
        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            return True
    return False


# ---------------------------------------------------------------------
# BN control utilities (safe evaluation for SNs)
# ---------------------------------------------------------------------

def set_bn_eval(model: nn.Module):
    """
    Put only BatchNorm layers into eval() (freeze running stats usage)
    without flipping the entire model into eval mode.
    """
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            m.eval()


def set_bn_train(model: nn.Module):
    """
    Put only BatchNorm layers back into train() without changing the
    train/eval state of the rest of the model.
    """
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            m.train()


@contextmanager
def freeze_bn_running_stats(model: nn.Module):
    """
    Context manager that preserves all BN running statistics across the block.

    This lets you run forward passes in TRAIN mode (using per-batch statistics)
    without *persistently* mutating running_mean / running_var / num_batches_tracked.
    """
    bn_modules = []
    saved = []
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            bn_modules.append(m)
            saved.append({
                "mean": (None if getattr(m, "running_mean", None) is None else m.running_mean.clone()),
                "var": (None if getattr(m, "running_var", None) is None else m.running_var.clone()),
                "nbt": (None if getattr(m, "num_batches_tracked", None) is None else m.num_batches_tracked.clone()),
                "momentum": getattr(m, "momentum", None),
            })
    try:
        yield
    finally:
        # Restore everything exactly
        for m, s in zip(bn_modules, saved):
            if s["mean"] is not None and m.running_mean is not None:
                m.running_mean.copy_(s["mean"])
            if s["var"] is not None and m.running_var is not None:
                m.running_var.copy_(s["var"])
            if s["nbt"] is not None and getattr(m, "num_batches_tracked", None) is not None:
                m.num_batches_tracked.copy_(s["nbt"])
            if s["momentum"] is not None:
                m.momentum = s["momentum"]


@contextmanager
def use_batch_stats_without_updating_bn(model: nn.Module):
    """
    Context manager that forces BN layers to use *batch* statistics (train behavior)
    while temporarily disabling running stats updates (no side effects).

    Implementation: set m.train(True) and m.track_running_stats=False for BN layers,
    then restore prior flags after the block.
    """
    bn_modules = []
    saved_flags = []
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            bn_modules.append(m)
            saved_flags.append((m.training, m.track_running_stats))
    try:
        for m in bn_modules:
            m.train(True)                 # use batch stats
            m.track_running_stats = False # but don't update buffers
        yield
    finally:
        for m, (was_train, was_track) in zip(bn_modules, saved_flags):
            m.track_running_stats = was_track
            m.train(was_train)


# ---------------------------------------------------------------------
# NEW: Tiny helper to guarantee safe evaluation everywhere
# ---------------------------------------------------------------------
@contextmanager
def evaluating(model: nn.Module):
    """
    Temporarily switch a model to eval() and run the block under torch.no_grad(),
    then restore the previous training/eval mode afterward.
    """
    was_training = model.training
    try:
        model.eval()
        with torch.no_grad():
            yield
    finally:
        model.train(was_training)


def train_network(dataset, architecture, total_epochs=4000, print_every=400, 
                  device="cpu", phi_knots=100, Phi_knots=100, seed=None,
                  norm_type='none', norm_position='after', norm_skip_first=True,
                  no_load_best=False, bn_recalc_on_load=False,
                  residual_style=None):
    """
    Train a Sprecher network on the given dataset.
    
    Args:
        dataset: Dataset instance with sample() method
        architecture: List of hidden layer sizes
        total_epochs: Number of training epochs
        print_every: Print frequency
        device: Training device
        phi_knots: Number of knots for phi splines
        Phi_knots: Number of knots for Phi splines
        seed: Random seed (uses config default if None)
        norm_type: Type of normalization ('none', 'batch', 'layer')
        norm_position: Position of normalization ('before', 'after')
        norm_skip_first: Whether to skip normalization for first block
        no_load_best: If True, don't load best checkpoint at end
        bn_recalc_on_load: If True, recalculate BatchNorm stats when loading checkpoint
        residual_style: Optional override for CONFIG['residual_style'] ∈ {'node','linear'}.
                        Aliases 'standard'/'matrix' → 'linear'. If None, uses CONFIG as-is.
    
    Returns:
        plotting_snapshot: dict with a deep-copied model and data for consistent plotting
        losses: List of training losses across epochs
    """
    # Use seed from config if not provided
    if seed is None:
        seed = CONFIG['seed']
    
    # Set seeds
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    # Optional: set residual style override & normalize synonyms
    if residual_style is not None:
        CONFIG['residual_style'] = str(residual_style).lower()
    if CONFIG.get('residual_style', 'node') in ['standard', 'matrix']:
        CONFIG['residual_style'] = 'linear'
    CONFIG.setdefault('residual_style', 'node')
    
    # Generate training data
    n_samples = 32 if dataset.input_dim == 1 else 32 * 32
    x_train, y_train = dataset.sample(n_samples, device)
    
    # Compute target statistics for better initialization
    y_mean = y_train.mean(dim=0)
    y_std = y_train.std(dim=0)
    
    # Create model
    model = SprecherMultiLayerNetwork(
        input_dim=dataset.input_dim,
        architecture=architecture,
        final_dim=dataset.output_dim,
        phi_knots=phi_knots,
        Phi_knots=Phi_knots,
        norm_type=norm_type,
        norm_position=norm_position,
        norm_skip_first=norm_skip_first
    ).to(device)
    
    # Initialize output bias to target mean for faster convergence
    with torch.no_grad():
        model.output_bias.data = y_mean.mean()
        model.output_scale.data = torch.tensor(0.1)  # Start with small scale
    
    # Compute parameter counts (now includes residual_projection for 'linear' style)
    lambda_params = 0
    eta_params = 0
    spline_params = 0
    residual_params = 0
    residual_scalar_params = 0
    residual_pooling_params = 0
    residual_broadcast_params = 0
    residual_projection_params = 0  # NEW
    codomain_params = 0
    norm_params = 0
    lateral_params = 0
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
            
        if 'lambdas' in name:
            lambda_params += param.numel()
        elif 'eta' in name:
            eta_params += param.numel()
        elif 'coeffs' in name or 'log_increments' in name:
            spline_params += param.numel()
        elif 'residual_weight' in name:
            if CONFIG.get('use_residual_weights', True):
                residual_scalar_params += param.numel()
                residual_params += param.numel()
        elif 'residual_pooling_weights' in name:
            if CONFIG.get('use_residual_weights', True):
                residual_pooling_params += param.numel()
                residual_params += param.numel()
        elif 'residual_broadcast_weights' in name:
            if CONFIG.get('use_residual_weights', True):
                residual_broadcast_params += param.numel()
                residual_params += param.numel()
        elif 'residual_projection' in name:  # NEW
            if CONFIG.get('use_residual_weights', True):
                residual_projection_params += param.numel()
                residual_params += param.numel()
        elif 'phi_codomain_params' in name:
            if CONFIG.get('train_phi_codomain', False):
                codomain_params += param.numel()
        elif 'norm_layers' in name:
            norm_params += param.numel()
        elif 'lateral' in name:
            if CONFIG.get('use_lateral_mixing', True):
                lateral_params += param.numel()
    
    # Core parameters excluding residual and output params
    output_params = 2  # output_scale and output_bias
    
    if CONFIG.get('train_phi_codomain', False):
        total_params = (lambda_params + eta_params + spline_params + residual_params +
                        output_params + codomain_params + norm_params + lateral_params)
    else:
        total_params = (lambda_params + eta_params + spline_params + residual_params +
                        output_params + norm_params + lateral_params)
    
    print(f"Dataset: {dataset} (input_dim={dataset.input_dim}, output_dim={dataset.output_dim})")
    print(f"Architecture: {architecture}")
    print(f"Residual connections: {'enabled' if CONFIG.get('use_residual_weights', True) else 'disabled'}")
    if CONFIG.get('use_residual_weights', True):
        print(f"  Residual style: {CONFIG.get('residual_style','node')}")
    print(f"Normalization: {norm_type} (position: {norm_position}, skip_first: {norm_skip_first})")
    if CONFIG.get('use_lateral_mixing', True):
        print(f"Lateral mixing: {CONFIG.get('lateral_mixing_type', 'cyclic')}")
    print(f"Total number of trainable parameters: {total_params}")
    print(f"  - Lambda weight VECTORS: {lambda_params}")
    print(f"  - Eta shift parameters: {eta_params}")
    print(f"  - Spline parameters: {spline_params}")
    if CONFIG.get('use_residual_weights', True) and residual_params > 0:
        print(f"  - Residual connection weights: {residual_params}")
        if residual_scalar_params > 0:
            print(f"    * Scalar weights (same dims): {residual_scalar_params}")
        if residual_pooling_params > 0:
            print(f"    * Pooling weights (d_in > d_out): {residual_pooling_params}")
        if residual_broadcast_params > 0:
            print(f"    * Broadcast weights (d_in < d_out): {residual_broadcast_params}")
        if residual_projection_params > 0:
            print(f"    * Projection matrices (d_in != d_out): {residual_projection_params}")
    if CONFIG.get('use_lateral_mixing', True) and lateral_params > 0:
        print(f"  - Lateral mixing parameters: {lateral_params}")
    print(f"  - Output scale and bias: {output_params}")
    if CONFIG.get('train_phi_codomain', False) and codomain_params > 0:
        print(f"  - Phi codomain parameters (cc, cr per block): {codomain_params}")
    if norm_params > 0:
        print(f"  - Normalization parameters: {norm_params}")
    
    # Setup optimizer
    if CONFIG.get('use_advanced_scheduler', False):
        # Use AdamW optimizer with weight decay and advanced scheduler
        params = [
            {"params": [p for n, p in model.named_parameters() if "phi_codomain_params" in n], 
             "lr": 0.01, "lr_scale": 1.0},  # Higher base LR for codomain params
            {"params": [p for n, p in model.named_parameters() if "output" in n], 
             "lr": 0.001, "lr_scale": 0.5},  # Medium LR for output params
            {"params": [p for n, p in model.named_parameters() if "lateral" in n], 
             "lr": 0.005, "lr_scale": 0.8},  # Higher LR for lateral params to adapt quickly
            # Everything else (includes splines, lambdas, residuals including any residual_projection)
            {"params": [p for n, p in model.named_parameters() if "phi_codomain_params" not in n 
                        and "output" not in n and "lateral" not in n], 
             "lr": 0.001, "lr_scale": 0.3}
        ]
        optimizer = torch.optim.AdamW(params, weight_decay=CONFIG.get('weight_decay', 1e-6), betas=(0.9, 0.999))
        
        # Create custom scheduler
        scheduler = PlateauAwareCosineAnnealingLR(
            optimizer, 
            base_lr=CONFIG.get('scheduler_base_lr', 1e-4), 
            max_lr=CONFIG.get('scheduler_max_lr', 1e-2),
            patience=CONFIG.get('scheduler_patience', 500),
            threshold=CONFIG.get('scheduler_threshold', 1e-5)
        )
    else:
        # Use original simple Adam setup with optional param groups
        if CONFIG.get('train_phi_codomain', False) or CONFIG.get('use_lateral_mixing', True):
            params = []
            if CONFIG.get('train_phi_codomain', False):
                params.append({"params": [p for n, p in model.named_parameters() if "phi_codomain_params" in n], "lr": 0.001})
            if CONFIG.get('use_lateral_mixing', True):
                params.append({"params": [p for n, p in model.named_parameters() if "lateral" in n], "lr": 0.0005})
            excluded = []
            if CONFIG.get('train_phi_codomain', False):
                excluded.append("phi_codomain_params")
            if CONFIG.get('use_lateral_mixing', True):
                excluded.append("lateral")
            params.append({"params": [p for n, p in model.named_parameters() if not any(exc in n for exc in excluded)], "lr": 0.0003})
        else:
            params = model.parameters()
        optimizer = torch.optim.Adam(params, weight_decay=1e-7)
        scheduler = None
    
    losses = []
    best_loss = float("inf")
    best_checkpoint = None
    
    # Gradient clipping value
    max_grad_norm = CONFIG.get('max_grad_norm', 1.0)
    
    # Reset domain violation tracking if enabled
    if CONFIG.get('track_domain_violations', False):
        model.reset_domain_violation_stats()
    
    # Check if model has BatchNorm
    has_bn = has_batchnorm(model)
    if has_bn:
        print("Model contains BatchNorm layers - will handle train/eval modes appropriately")
    
    # Set model to training mode
    model.train()
    
    pbar = tqdm(range(total_epochs), desc="Training Network")
    for epoch in pbar:
        # Update all domains based on current parameters EVERY ITERATION
        if CONFIG.get('use_theoretical_domains', True):
            model.update_all_domains(allow_resampling=True)
        
        # Regular training
        optimizer.zero_grad()
        output = model(x_train)
        
        # Simple MSE loss only
        loss = torch.mean((output - y_train) ** 2)
        
        loss.backward()
        
        # Gradient clipping to prevent instabilities
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # Get some monitoring values
        if CONFIG.get('train_phi_codomain', False):
            # Monitor codomain params from first block (if present)
            first_block = model.layers[0]
            if hasattr(first_block, 'phi_codomain_params') and first_block.phi_codomain_params is not None:
                cc_val = first_block.phi_codomain_params.cc.item()
                cr_val = first_block.phi_codomain_params.cr.item()
                cc_grad = first_block.phi_codomain_params.cc.grad.item() if first_block.phi_codomain_params.cc.grad is not None else 0.0
                cr_grad = first_block.phi_codomain_params.cr.grad.item() if first_block.phi_codomain_params.cr.grad is not None else 0.0
            else:
                cc_val = cr_val = cc_grad = cr_grad = 0.0
        else:
            cc_val = cr_val = cc_grad = cr_grad = 0.0
        
        optimizer.step()
        
        # Update learning rate if using advanced scheduler
        if scheduler is not None:
            current_lr = scheduler.step(loss.item())
        else:
            current_lr = optimizer.param_groups[0]['lr']
        
        losses.append(loss.item())
        
        # Update best loss and save checkpoint
        if loss.item() < best_loss:
            if has_bn and CONFIG.get('debug_checkpoint_loading', False):
                print(f"\n[CHECKPOINT DEBUG] New best loss at epoch {epoch}: {loss.item():.4e}")
                for name, module in model.named_modules():
                    if isinstance(module, nn.BatchNorm1d):
                        print(f"  BN stats at best loss - {name}: running_mean[:3]={module.running_mean[:3].cpu().numpy()}, "
                              f"num_batches_tracked={module.num_batches_tracked.item()}")
                        break
            
            best_loss = loss.item()
            
            # Complete snapshot for plotting (exact state) — include eval-mode outputs too
            if CONFIG.get('debug_checkpoint_loading', False):
                print(f"\n[CHECKPOINT DEBUG] Creating snapshot at epoch {epoch} with loss {loss.item():.4e}")
            # Compute eval-mode output/loss to support deterministic verification later
            with evaluating(model):
                eval_output = model(x_train)
                eval_loss = torch.mean((eval_output - y_train) ** 2).item()
            plotting_snapshot = {
                'model': copy.deepcopy(model),
                'x_train': x_train.clone(),
                'y_train': y_train.clone(),
                'output': output.detach().clone(),             # train-mode output (for reference)
                'loss': loss.item(),                           # train-mode loss (for reference)
                'output_eval': eval_output.detach().clone(),   # eval-mode output
                'loss_eval': eval_loss,                        # eval-mode loss
                'epoch': epoch,
                'device': device
            }
            
            # Save BN stats if any
            bn_statistics = {}
            if has_bn:
                for name, module in model.named_modules():
                    if isinstance(module, nn.BatchNorm1d):
                        bn_statistics[name] = {
                            'running_mean': module.running_mean.clone().cpu(),
                            'running_var': module.running_var.clone().cpu(),
                            'num_batches_tracked': module.num_batches_tracked.clone().cpu(),
                            'momentum': module.momentum,
                            'eps': module.eps,
                            'affine': module.affine,
                            'track_running_stats': module.track_running_stats,
                            'weight': module.weight.clone().cpu() if module.affine else None,
                            'bias': module.bias.clone().cpu() if module.affine else None
                        }
            
            # Create complete checkpoint with all state
            best_checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict().copy(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': loss.item(),
                'loss_eval': eval_loss,                               # NEW: eval-mode loss
                'has_batchnorm': has_bn,
                'bn_statistics': bn_statistics,
                'domain_states': model.get_all_domain_states(),
                'domain_ranges': model.get_domain_ranges(),
                'training_mode': model.training,
                'x_train': x_train.cpu().clone(),
                'y_train': y_train.cpu().clone(),
                'output': output.detach().cpu().clone(),             # train-mode output
                'output_eval': eval_output.detach().cpu().clone(),   # NEW: eval-mode output
                # Add model creation params (for exact reconstruction)
                'model_params': {
                    'input_dim': dataset.input_dim,
                    'architecture': architecture,
                    'final_dim': dataset.output_dim,
                    'phi_knots': phi_knots,
                    'Phi_knots': Phi_knots,
                    'norm_type': norm_type,
                    'norm_position': norm_position,
                    'norm_skip_first': norm_skip_first
                },
                # Persist residual style used at best epoch for deterministic reload
                'residual_style': CONFIG.get('residual_style', 'node'),
                # Plotting snapshot for perfect restoration
                'plotting_snapshot': plotting_snapshot
            }
        
        # Calculate output standard deviation for monitoring
        std_output = torch.std(output)
        std_target = torch.std(y_train)
        
        pbar_dict = {
            'loss': f'{loss.item():.2e}',
            'best': f'{best_loss:.2e}',
            'std_out': f'{std_output.item():.3f}',
            'std_tar': f'{std_target.item():.3f}'
        }
        
        if CONFIG.get('train_phi_codomain', False):
            pbar_dict['cc'] = f'{cc_val:.3f}'
            pbar_dict['cr'] = f'{cr_val:.3f}'
        
        if CONFIG.get('use_advanced_scheduler', False):
            pbar_dict['lr'] = f'{current_lr:.2e}'
            if CONFIG.get('train_phi_codomain', False):
                pbar_dict['g_cc'] = f'{cc_grad:.3e}'
                pbar_dict['g_cr'] = f'{cr_grad:.3e}'
        
        pbar.set_postfix(pbar_dict)
        
        if (epoch + 1) % print_every == 0:
            if CONFIG.get('use_advanced_scheduler', False):
                print(f"Epoch {epoch+1}: Loss = {loss.item():.4e}, Best = {best_loss:.4e}, LR = {current_lr:.4e}")
            else:
                print(f"Epoch {epoch+1}: Loss = {loss.item():.4e}, Best = {best_loss:.4e}")
            
            if CONFIG.get('debug_domains', False):
                print("\nDomain ranges:")
                for idx, layer in enumerate(model.layers):
                    print(f"  Layer {idx}: phi domain=[{layer.phi.in_min:.3f}, {layer.phi.in_max:.3f}], "
                          f"Phi domain=[{layer.Phi.in_min:.3f}, {layer.Phi.in_max:.3f}]")
                print()
            
            if CONFIG.get('track_domain_violations', False) and (epoch + 1) % (print_every * 5) == 0:
                model.print_domain_violation_report()
    
    # Load best checkpoint before returning
    if best_checkpoint is not None and not no_load_best:
        print(f"\n{'='*60}")
        print("CHECKPOINT LOADING DEBUG INFO:")
        print(f"Best checkpoint from epoch: {best_checkpoint['epoch']}")
        print(f"Best checkpoint loss: {best_checkpoint['loss']:.4e}")
        print(f"Current (final) loss: {losses[-1]:.4e}")
        print(f"Loss ratio (final/best): {losses[-1]/best_checkpoint['loss']:.2f}x")
        
        if 'plotting_snapshot' in best_checkpoint:
            print("\nUsing plotting snapshot from checkpoint - this ensures perfect restoration!")
            plotting_snapshot = best_checkpoint['plotting_snapshot']
            print(f"\nSnapshot loaded from epoch {plotting_snapshot['epoch']} with loss {plotting_snapshot['loss']:.4e}")
            x_train = plotting_snapshot['x_train'].to(device)
            y_train = plotting_snapshot['y_train'].to(device)
            print(f"\n{'='*60}\n")
        else:
            print("\nWARNING: Old checkpoint format without plotting snapshot")
        
        # Get a sample output before loading checkpoint (eval-mode; no mutation)
        with evaluating(model):
            _ = model(x_train[:5]).cpu().numpy()
        
        # Enable checkpoint debug logging
        CONFIG['debug_checkpoint_loading'] = True

        # Ensure same residual semantics as at best epoch
        if 'residual_style' in best_checkpoint:
            CONFIG['residual_style'] = best_checkpoint['residual_style']
        
        # Create a fresh model instance without domain initialization
        if 'model_params' in best_checkpoint:
            print("Creating fresh model instance for checkpoint loading...")
            domain_ranges = best_checkpoint.get('domain_ranges', None)
            fresh_model = SprecherMultiLayerNetwork(
                **best_checkpoint['model_params'],
                initialize_domains=False,
                domain_ranges=domain_ranges
            ).to(device)
            
            fresh_model.train()
            print(f"[CHECKPOINT DEBUG] Fresh model training mode: {fresh_model.training}")
            
            print("\n[CHECKPOINT DEBUG] Testing fresh model BEFORE loading state_dict:")
            with evaluating(fresh_model):
                test_out = fresh_model(x_train[:5])
                print(f"  Fresh model output: {test_out.cpu().numpy().flatten()[:5]}")
            
            print("\n[CHECKPOINT DEBUG] About to load state_dict...")
            fresh_model.load_state_dict(best_checkpoint['model_state_dict'])
            print("\nCheckpoint loaded into fresh model successfully!")
            print(f"[CHECKPOINT DEBUG] Model training mode after state_dict: {fresh_model.training}")
            
            if best_checkpoint.get('has_batchnorm', False):
                print("\n[CHECKPOINT DEBUG] BatchNorm state after loading state_dict:")
                for name, module in fresh_model.named_modules():
                    if isinstance(module, nn.BatchNorm1d):
                        print(f"  {name}: num_batches_tracked={module.num_batches_tracked.item()}, "
                              f"momentum={module.momentum}, training={module.training}")
                        break
            
            if 'domain_states' in best_checkpoint:
                print("\n[CHECKPOINT DEBUG] Restoring domain states after state_dict load...")
                fresh_model.set_all_domain_states(best_checkpoint['domain_states'])
                print("Domain states restored after state_dict!")
                
                print("\n[CHECKPOINT DEBUG] Checking theoretical ranges after restoration:")
                for i, layer in enumerate(fresh_model.layers):
                    ranges = layer.get_theoretical_ranges()
                    print(f"  Layer {i}: {ranges}")
            
            model = fresh_model
        else:
            print("WARNING: Old checkpoint format, using existing model")
            model.load_state_dict(best_checkpoint['model_state_dict'])
            print("Checkpoint loaded successfully!")
        
        # Always restore domain states if present
        if 'domain_states' in best_checkpoint:
            print("Restoring spline domain states from checkpoint...")
            model.set_all_domain_states(best_checkpoint['domain_states'])
            print("Domain states restored successfully!")
        
        # Restore exact BN stats if saved
        if best_checkpoint.get('has_batchnorm', False) and 'bn_statistics' in best_checkpoint:
            print("\n[CHECKPOINT DEBUG] Restoring exact BatchNorm statistics from best epoch...")
            for name, module in model.named_modules():
                if isinstance(module, nn.BatchNorm1d) and name in best_checkpoint['bn_statistics']:
                    saved_stats = best_checkpoint['bn_statistics'][name]
                    module.running_mean.copy_(saved_stats['running_mean'])
                    module.running_var.copy_(saved_stats['running_var'])
                    module.num_batches_tracked.copy_(saved_stats['num_batches_tracked'])
                    if module.affine and saved_stats.get('weight') is not None:
                        module.weight.data.copy_(saved_stats['weight'])
                        module.bias.data.copy_(saved_stats['bias'])
                    print(f"  {name}: Restored exact BN stats from epoch {best_checkpoint['epoch']}")
        elif best_checkpoint.get('has_batchnorm', False):
            print("\n[CHECKPOINT DEBUG] WARNING: No saved BN statistics in checkpoint (old format)")
            print("  BatchNorm statistics will be from final training epoch, not best epoch!")
        
        # Keep training mode if saved that way
        if best_checkpoint.get('training_mode', True):
            model.train()
            print("Model kept in training mode (as it was during best checkpoint)")
        else:
            model.eval()
            print("Model set to eval mode")
        
        # Verify outputs after loading — eval-mode (BN running stats); should not mutate BN
        if has_bn and CONFIG.get('debug_checkpoint_loading', False):
            bn_stats_before_forward = {}
            for name, module in model.named_modules():
                if isinstance(module, nn.BatchNorm1d):
                    bn_stats_before_forward[name] = {
                        'mean': module.running_mean.clone(),
                        'var': module.running_var.clone(),
                        'tracked': module.num_batches_tracked.clone()
                    }
        
        with evaluating(model):
            _ = model(x_train[:5]).cpu().numpy()
        print(f"Sample outputs AFTER loading checkpoint: [omitted; eval-mode verification]")
        
        if has_bn and CONFIG.get('debug_checkpoint_loading', False):
            print("\n[CHECKPOINT DEBUG] Verifying BN stats didn't change during forward pass...")
            for name, module in model.named_modules():
                if isinstance(module, nn.BatchNorm1d) and name in bn_stats_before_forward:
                    before = bn_stats_before_forward[name]
                    mean_changed = not torch.allclose(module.running_mean, before['mean'])
                    var_changed = not torch.allclose(module.running_var, before['var'])
                    tracked_changed = module.num_batches_tracked.item() != before['tracked'].item()
                    if mean_changed or var_changed or tracked_changed:
                        print(f"  WARNING: {name} stats changed during forward pass!")
                        print(f"    mean changed: {mean_changed}")
                        print(f"    var changed: {var_changed}")
                        print(f"    tracked changed: {tracked_changed} ({before['tracked'].item()} -> {module.num_batches_tracked.item()})")
                    else:
                        print(f"  [OK] {name} stats unchanged")
                    break
        
        # If we have saved training data, verify the checkpoint deterministically (EVAL MODE)
        if 'x_train' in best_checkpoint:
            print("\nVerifying checkpoint with saved training data (eval mode; BN running stats)...")
            saved_x = best_checkpoint['x_train'].to(device)
            saved_y = best_checkpoint['y_train'].to(device)

            # Prefer eval-mode reference if available
            ref_loss = best_checkpoint.get('loss_eval', best_checkpoint['loss'])
            saved_output_ref = best_checkpoint.get('output_eval', best_checkpoint['output'])
            
            with evaluating(model):
                current_output = model(saved_x)
                current_loss = torch.mean((current_output - saved_y) ** 2).item()
            
            print(f"Saved checkpoint (eval) loss: {ref_loss:.4e}")
            print(f"Current loss on saved data:  {current_loss:.4e}")
            # Tolerate tiny numerical noise
            abs_tol = 1e-8 if not best_checkpoint.get('has_batchnorm', False) else 5e-4
            rel_tol = 1e-4
            loss_close = abs(current_loss - ref_loss) <= max(abs_tol, rel_tol * abs(ref_loss))
            
            output_diff = torch.abs(current_output.cpu() - saved_output_ref).max().item()
            out_abs_tol = 5e-3 if best_checkpoint.get('has_batchnorm', False) else 1e-6
            out_close = output_diff <= out_abs_tol
            
            if not (loss_close and out_close):
                print("\n" + "="*60)
                print("WARNING: Checkpoint restoration mismatch (within tolerance report above).")
                print(f"Expected loss: {ref_loss:.4e}")
                print(f"Actual loss:   {current_loss:.4e}")
                print(f"Loss |Δ|: {abs(current_loss - ref_loss):.4e}  (tol={max(abs_tol, rel_tol*abs(ref_loss)):.2e})")
                print(f"Max |Δ output|: {output_diff:.4e}  (tol={out_abs_tol:.1e})")
                print("="*60 + "\n")
            else:
                print("[OK] Checkpoint verification passed - outputs match within tolerance!")
            
            x_train = saved_x
            y_train = saved_y
        
        CONFIG['debug_checkpoint_loading'] = False
        print("\n[CHECKPOINT DEBUG] Debug logging disabled")
        
        if best_checkpoint.get('has_batchnorm', False):
            print("\nDEBUG: BatchNorm stats AFTER loading and domain update:")
            for name, module in model.named_modules():
                if isinstance(module, nn.BatchNorm1d):
                    print(f"  {name}: mean={module.running_mean.data[:3].cpu().numpy()}, var={module.running_var.data[:3].cpu().numpy()}")
        
        if best_checkpoint.get('has_batchnorm', False):
            for name, module in model.named_modules():
                if isinstance(module, nn.BatchNorm1d):
                    print(f"\nBatchNorm layer '{name}' stats BEFORE recalc:")
                    print(f"  Running mean: {module.running_mean.data[:3].cpu().numpy()}")
                    print(f"  Running var: {module.running_var.data[:3].cpu().numpy()}")
                    break
            
            if bn_recalc_on_load:
                print("\nRecalculating BatchNorm statistics on training data...")
                recalculate_bn_stats(model, x_train, num_passes=10)
                
                for name, module in model.named_modules():
                    if isinstance(module, nn.BatchNorm1d):
                        print(f"\nBatchNorm layer '{name}' stats AFTER recalc:")
                        print(f"  Running mean: {module.running_mean.data[:3].cpu().numpy()}")
                        print(f"  Running var: {module.running_var.data[:3].cpu().numpy()}")
                        break
                
                with evaluating(model):
                    _ = model(x_train[:5]).cpu().numpy()
                print(f"Sample outputs AFTER BN recalc: [omitted; eval-mode verification]")
            else:
                print(f"Using saved BatchNorm statistics from best checkpoint (epoch {best_checkpoint['epoch']})")
        
        print(f"{'='*60}\n")
        
    elif no_load_best:
        print("\nSkipping best model loading (--no_load_best flag set)")
        print(f"Using final model state with loss: {losses[-1]:.4e}")
        print(f"Best loss during training was: {best_loss:.4e}")
        model.train()
        
        print("\nCreating snapshot of final model state for plotting...")
        with evaluating(model):
            final_output = model(x_train)
            final_loss = torch.mean((final_output - y_train) ** 2).item()
        
        plotting_snapshot = {
            'model': copy.deepcopy(model),
            'x_train': x_train.clone(),
            'y_train': y_train.clone(),
            'output': final_output.detach().clone(),
            'loss': final_loss,
            'output_eval': final_output.detach().clone(),
            'loss_eval': final_loss,
            'epoch': total_epochs - 1,
            'device': device
        }
        
        print(f"Snapshot created with final loss: {final_loss:.4e}")
    else:
        # No checkpoint was saved (shouldn't happen normally)
        model.train()
    
    print(f"\nDEBUG: Model is in {'training' if model.training else 'eval'} mode for final operations")
    
    print("\nDEBUG: Final model output:")
    with evaluating(model):
        test_out = model(x_train[:5])
        print(f"Output: {test_out.cpu().numpy().flatten()[:5]}")
    
    print("\nDEBUG: Skipping final domain update (domains already set correctly)")
    
    if CONFIG.get('track_domain_violations', False):
        print("\nFinal domain violation report:")
        model.print_domain_violation_report()
    
    # Print final parameters (compact preview for projection matrices to avoid huge logs)
    print("\nFinal parameters:")
    for idx, layer in enumerate(model.layers, start=1):
        print(f"Block {idx}: eta = {layer.eta.item():.6f}")
        print(f"Block {idx}: lambdas shape = {tuple(layer.lambdas.shape)}")
        print(f"Block {idx}: lambdas =")
        print(layer.lambdas.detach().cpu().numpy())
        if CONFIG.get('use_residual_weights', True):
            if hasattr(layer, 'residual_weight') and layer.residual_weight is not None:
                print(f"Block {idx}: residual_weight = {layer.residual_weight.item():.6f}")
            elif hasattr(layer, 'residual_projection') and layer.residual_projection is not None:
                W = layer.residual_projection.detach().cpu().numpy()
                print(f"Block {idx}: residual_projection shape = {W.shape}")
                # Print a small preview (top-left 4x4) to keep console readable
                r_preview = min(4, W.shape[0]); c_preview = min(4, W.shape[1])
                print(f"Block {idx}: residual_projection preview (top-left):")
                print(W[:r_preview, :c_preview])
            elif hasattr(layer, 'residual_pooling_weights') and layer.residual_pooling_weights is not None:
                print(f"Block {idx}: residual_pooling_weights shape = {tuple(layer.residual_pooling_weights.shape)}")
                print(f"Block {idx}: residual_pooling_weights =")
                print(layer.residual_pooling_weights.detach().cpu().numpy())
                print(f"Block {idx}: pooling assignment = {layer.pooling_assignment.cpu().numpy()}")
                print(f"Block {idx}: pooling counts = {layer.pooling_counts.cpu().numpy()}")
            elif hasattr(layer, 'residual_broadcast_weights') and layer.residual_broadcast_weights is not None:
                print(f"Block {idx}: residual_broadcast_weights shape = {tuple(layer.residual_broadcast_weights.shape)}")
                print(f"Block {idx}: residual_broadcast_weights =")
                print(layer.residual_broadcast_weights.detach().cpu().numpy())
                print(f"Block {idx}: broadcast sources = {layer.broadcast_sources.cpu().numpy()}")
        
        if CONFIG.get('use_lateral_mixing', True):
            if hasattr(layer, 'lateral_scale') and layer.lateral_scale is not None:
                print(f"Block {idx}: lateral_scale = {layer.lateral_scale.item():.6f}")
                if CONFIG.get('lateral_mixing_type', 'cyclic') == 'bidirectional':
                    print(f"Block {idx}: lateral_weights_forward =")
                    print(layer.lateral_weights_forward.detach().cpu().numpy())
                    print(f"Block {idx}: lateral_weights_backward =")
                    print(layer.lateral_weights_backward.detach().cpu().numpy())
                else:
                    print(f"Block {idx}: lateral_weights =")
                    print(layer.lateral_weights.detach().cpu().numpy())
        
        if CONFIG.get('train_phi_codomain', False):
            if hasattr(layer, 'phi_codomain_params') and layer.phi_codomain_params is not None:
                print(f"Block {idx}: Phi codomain center = {layer.phi_codomain_params.cc.item():.6f}")
                print(f"Block {idx}: Phi codomain radius = {layer.phi_codomain_params.cr.item():.6f}")
        
        if hasattr(layer, 'input_range') and layer.input_range is not None:
            print(f"Block {idx}: input_range = {layer.input_range}")
        if hasattr(layer, 'output_range') and layer.output_range is not None:
            print(f"Block {idx}: output_range = {layer.output_range}")
        print()
    
    print(f"Final loss: {best_loss:.4e}")
    
    # Ensure plotting_snapshot is defined
    if 'plotting_snapshot' not in locals():
        print("\nCreating plotting snapshot from current model state...")
        with evaluating(model):
            current_output = model(x_train)
            current_loss = torch.mean((current_output - y_train) ** 2).item()
        
        plotting_snapshot = {
            'model': copy.deepcopy(model),
            'x_train': x_train.clone(),
            'y_train': y_train.clone(),
            'output': current_output.detach().clone(),
            'loss': current_loss,
            'output_eval': current_output.detach().clone(),
            'loss_eval': current_loss,
            'epoch': best_checkpoint['epoch'] if best_checkpoint else total_epochs - 1,
            'device': device
        }
    
    return plotting_snapshot, losses

----------
sn_core/data.py:
----------

"""Dataset registry and implementations for Sprecher Network experiments."""

import torch
import numpy as np


class Dataset:
    """Base class for SN datasets."""
    
    def sample(self, n: int, device='cpu'):
        """Return (x, y) shaped (n, input_dim) and (n, output_dim)."""
        x = self.generate_inputs(n, device)
        y = self.evaluate(x)
        return x, y
    
    def generate_inputs(self, n: int, device='cpu'):
        """Generate random input samples."""
        # Default: uniform random in [0, 1]^input_dim
        return torch.rand(n, self.input_dim, device=device)
    
    def evaluate(self, x):
        """Evaluate the function at given inputs x."""
        raise NotImplementedError
    
    @property
    def input_dim(self) -> int:
        """Input dimension."""
        raise NotImplementedError
    
    @property
    def output_dim(self) -> int:
        """Output dimension."""
        raise NotImplementedError
    
    def __str__(self):
        return self.__class__.__name__


class Toy1DPoly(Dataset):
    """1D polynomial: (x - 3/10)^5 - (x - 1/3)^3 + (1/5)(x - 1/10)^2"""
    
    @property
    def input_dim(self):
        return 1
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """For 1D functions, use linspace for better visualization."""
        return torch.linspace(0, 1, n, device=device).unsqueeze(1)
    
    def evaluate(self, x):
        """Evaluate the polynomial at x."""
        return (x - 3/10)**5 - (x - 1/3)**3 + (1/5)*(x - 1/10)**2


class Toy1DComplex(Dataset):
    """Complex 1D function with multiple sines and exponentials."""
    
    @property
    def input_dim(self):
        return 1
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """For 1D functions, use linspace for better visualization."""
        return torch.linspace(0, 1, n, device=device).unsqueeze(1)
    
    def evaluate(self, x):
        """Evaluate the complex function at x."""
        e = torch.exp(torch.tensor(1.0, device=x.device))
        y = (torch.sin((torch.exp(x) - 1.0) / (2.0 * (e - 1.0))) + 
             torch.sin(1.0 - (4.0 * (torch.exp(x + 0.1) - 1.0)) / (5.0 * (e - 1.0))) + 
             torch.sin(2.0 + (torch.exp(x + 0.2) - 1.0) / (e - 1.0)) + 
             torch.sin(3.0 + (torch.exp(x + 0.3) - 1.0) / (5.0 * (e - 1.0))) + 
             torch.sin(4.0 - (6.0 * (torch.exp(x + 0.4) - 1.0)) / (5.0 * (e - 1.0))))
        return y


class Toy2D(Dataset):
    """2D function: exp(sin(11x)) + 3y + 4sin(8y)"""
    
    @property
    def input_dim(self):
        return 2
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """Generate grid if n is perfect square, otherwise random."""
        sqrt_n = int(np.sqrt(n))
        if sqrt_n * sqrt_n == n:
            x = torch.linspace(0, 1, sqrt_n, device=device)
            y = torch.linspace(0, 1, sqrt_n, device=device)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            return torch.stack([X.flatten(), Y.flatten()], dim=1)
        else:
            return torch.rand(n, 2, device=device)
    
    def evaluate(self, x):
        """Evaluate the 2D function at x."""
        return torch.exp(torch.sin(11 * x[:, [0]])) + 3 * x[:, [1]] + 4 * torch.sin(8 * x[:, [1]])


class Toy2DVector(Dataset):
    """2D vector function with 2 outputs."""
    
    @property
    def input_dim(self):
        return 2
    
    @property
    def output_dim(self):
        return 2
    
    def generate_inputs(self, n, device='cpu'):
        """Generate grid if n is perfect square, otherwise random."""
        sqrt_n = int(np.sqrt(n))
        if sqrt_n * sqrt_n == n:
            x_vals = torch.linspace(0, 1, sqrt_n, device=device)
            y_vals = torch.linspace(0, 1, sqrt_n, device=device)
            X, Y = torch.meshgrid(x_vals, y_vals, indexing='ij')
            return torch.stack([X.flatten(), Y.flatten()], dim=1)
        else:
            return torch.rand(n, 2, device=device)
    
    def evaluate(self, x):
        """Evaluate the 2D vector function at x."""
        y1 = (torch.exp(torch.sin(torch.pi * x[:, [0]]) + x[:, [1]]**2) - 1) / 7
        y2 = (1/4)*x[:, [1]] + (1/5)*x[:, [1]]**2 - x[:, [0]]**3 + (1/5)*torch.sin(7*x[:, [0]])
        return torch.cat([y1, y2], dim=1)


class Toy100D(Dataset):
    """100D function: exp of mean squared sine (KAN Section 3)."""
    
    @property
    def input_dim(self):
        return 100
    
    @property
    def output_dim(self):
        return 1
    
    def evaluate(self, x):
        """Evaluate the 100D function at x."""
        # f(x_1,...,x_100) = exp(1/100 * sum(sin^2(pi*x_i/2)))
        return torch.exp(torch.mean(torch.sin(np.pi * x / 2)**2, dim=1, keepdim=True))


class SpecialBessel(Dataset):
    """Special function: Bessel function approximation."""
    
    @property
    def input_dim(self):
        return 2
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """Generate grid if n is perfect square, otherwise random."""
        sqrt_n = int(np.sqrt(n))
        if sqrt_n * sqrt_n == n:
            x = torch.linspace(0, 1, sqrt_n, device=device)
            y = torch.linspace(0, 1, sqrt_n, device=device)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            return torch.stack([X.flatten(), Y.flatten()], dim=1)
        else:
            return torch.rand(n, 2, device=device)
    
    def evaluate(self, x):
        """Evaluate the Bessel-like function at x."""
        r = torch.sqrt(x[:, [0]]**2 + x[:, [1]]**2)
        return torch.sin(10 * r) / (1 + 10 * r)


class FeynmanUV(Dataset):
    """Feynman dataset: UV radiation formula (Planck's law)."""
    
    @property
    def input_dim(self):
        return 2
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """Generate grid if n is perfect square, otherwise random."""
        sqrt_n = int(np.sqrt(n))
        if sqrt_n * sqrt_n == n:
            x = torch.linspace(0.1, 1, sqrt_n, device=device)  # Avoid zero
            y = torch.linspace(0.1, 1, sqrt_n, device=device)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            return torch.stack([X.flatten(), Y.flatten()], dim=1)
        else:
            return torch.rand(n, 2, device=device) * 0.9 + 0.1  # Avoid zero
    
    def evaluate(self, x):
        """Evaluate the Feynman UV formula at x."""
        T = x[:, [0]]  # Temperature parameter
        nu = x[:, [1]]  # Frequency parameter
        scaling_factor = 5.0
        y = (8 * torch.pi * nu**3) / (torch.exp(scaling_factor * nu / T) - 1)
        return y / (8 * torch.pi)  # Normalize


class Poisson2D(Dataset):
    """2D Poisson equation manufactured solution."""
    
    @property
    def input_dim(self):
        return 2
    
    @property
    def output_dim(self):
        return 1
    
    def generate_inputs(self, n, device='cpu'):
        """Generate grid if n is perfect square, otherwise random."""
        sqrt_n = int(np.sqrt(n))
        if sqrt_n * sqrt_n == n:
            x = torch.linspace(0, 1, sqrt_n, device=device)
            y = torch.linspace(0, 1, sqrt_n, device=device)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            return torch.stack([X.flatten(), Y.flatten()], dim=1)
        else:
            return torch.rand(n, 2, device=device)
    
    def evaluate(self, x):
        """Evaluate the Poisson solution at x."""
        return torch.sin(torch.pi * x[:, [0]]) * torch.sin(torch.pi * x[:, [1]])


class Toy4Dto5D(Dataset):
    """4D to 5D vector function with mixed operations."""
    
    @property
    def input_dim(self):
        return 4
    
    @property
    def output_dim(self):
        return 5
    
    def evaluate(self, x):
        """Evaluate the 4D to 5D function at x."""
        y1 = torch.sin(2 * np.pi * x[:, 0]) * torch.cos(np.pi * x[:, 1])
        y2 = torch.exp(-2 * (x[:, 0]**2 + x[:, 1]**2))
        y3 = x[:, 2]**3 - x[:, 3]**2 + 0.5 * torch.sin(5 * x[:, 2])
        y4 = torch.sigmoid(3 * (x[:, 0] + x[:, 1] - x[:, 2] + x[:, 3]))
        y5 = 0.5 * torch.sin(4 * np.pi * x[:, 0] * x[:, 3]) + 0.5 * torch.cos(3 * np.pi * x[:, 1] * x[:, 2])
        
        return torch.stack([y1, y2, y3, y4, y5], dim=1)


# Dataset registry
DATASETS = {
    "toy_1d_poly": Toy1DPoly(),
    "toy_1d_complex": Toy1DComplex(),
    "toy_2d": Toy2D(),
    "toy_2d_vector": Toy2DVector(),
    "toy_100d": Toy100D(),
    "special_bessel": SpecialBessel(),
    "feynman_uv": FeynmanUV(),
    "poisson": Poisson2D(),
    "toy_4d_to_5d": Toy4Dto5D(),
}


def get_dataset(name: str) -> Dataset:
    """Get dataset by name."""
    if name not in DATASETS:
        raise ValueError(f"Unknown dataset '{name}'. Available: {list(DATASETS.keys())}")
    return DATASETS[name]

----------
benchmarks/kan_sn_parity_bench.py:
----------

"""
KAN vs SN — Parameter‑Parity Benchmark (with BN recalc + eval progress)

This version **equalizes linear residual semantics**:
- For KAN with residual_type='linear':
    * If d_in == d_out  → use a **single scalar** residual α per layer (adds α·x once per layer).
    * If d_in != d_out  → use the **usual projection matrix** per edge (wb[i,j]) as before.
- For KAN with residual_type='silu' or 'none' behavior is unchanged.

SN already behaves as: scalar when d_in==d_out, projection when d_in!=d_out.

BN behavior at test:
- For **KAN**: when `--bn_eval_mode recalc_eval`, we recompute BN stats on training data and
  switch to eval() to use running stats for test.
- For **SN**: when `--bn_eval_mode recalc_eval`, we recompute BN stats on training data **but
  keep the model in train mode** for test so BN uses per-batch stats. This avoids
  badly-calibrated running stats that were causing inflated errors in eval-mode.

Usage:
  python -m benchmarks.kan_sn_parity_bench [FLAGS]

Flags
  General:
    --device {auto,cpu,cuda}            (default: auto)
    --seed INT                           (default: 45)
    --epochs INT                         (default: 4000)
    --dataset STR                        (default: toy_4d_to_5d)

  Test / Evaluation:
    --n_test INT                         (default: 20000)
    --bn_eval_mode {off,recalc_eval}     (default: recalc_eval)
    --bn_recalc_passes INT               (default: 10)
    --eval_batch_size INT                (default: 8192)

  Fairness / Parity:
    --equalize_params                    Match KAN params to SN param count
    --prefer_leq                         When equalizing, prefer <= target params

  SN (Sprecher Network):
    --sn_arch STR                        e.g., "15,15"
    --sn_phi_knots INT
    --sn_Phi_knots INT
    --sn_norm_type {none,batch,layer}    (default: batch)
    --sn_norm_position {before,after}    (default: after)
    --sn_norm_skip_first                 (default: True)
    --sn_norm_first                      Include first norm layer (overrides skip_first)
    --sn_no_residual                     Disable residual path
    --sn_residual_style {node,linear,standard,matrix}  (default: CONFIG)
    --sn_no_lateral                      Disable lateral mixing
    --sn_freeze_domains_after INT        Warm‑up epochs with domain updates, then freeze (0 = never)
    --sn_domain_margin FLOAT             Safety margin on computed domains during warm‑up (e.g., 0.01)

  KAN:
    --kan_arch STR                       e.g., "4,4"
    --kan_degree {2,3}                   (default: 3)
    --kan_K INT                          Basis count (ignored if --equalize_params)
    --kan_bn_type {none,batch}           (default: batch)
    --kan_bn_position {before,after}     (default: after)
    --kan_bn_skip_first
    --kan_outside {linear,clamp}         (default: linear)
    --kan_residual_type {silu,linear,none}  (default: silu)
    --kan_lr FLOAT                       (default: 1e-3)
    --kan_wd FLOAT                       (default: 1e-6)
    --kan_impl {fast,slow}               Implementation switch used in this script

  Output:
    --outdir PATH                        (default: benchmarks/results)

Examples
  # CPU, parameter parity, BN recalc, fast KAN, large eval batches, with *linear* residuals on both:
  python -m benchmarks.kan_sn_parity_bench \
    --dataset toy_4d_to_5d --epochs 4000 --device cpu --n_test 20000 \
    --seed 0 \
    --sn_arch 15,15 --sn_phi_knots 60 --sn_Phi_knots 60 \
    --sn_norm_type batch --sn_norm_position after --sn_norm_skip_first \
    --sn_residual_style linear --sn_no_lateral \
    --sn_freeze_domains_after 1500 --sn_domain_margin 0.01 \
    --kan_arch 4,4 --kan_degree 3 \
    --kan_bn_type batch --kan_bn_position after --kan_bn_skip_first \
    --kan_residual_type linear --kan_outside linear \
    --equalize_params --prefer_leq \
    --bn_eval_mode recalc_eval --bn_recalc_passes 10 \
    --eval_batch_size 8192 --kan_impl fast
"""

import argparse, math, time, os, json
from dataclasses import dataclass
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm

# --- Use the SN code directly ---
from sn_core import (
    get_dataset, train_network, CONFIG, has_batchnorm
)

# -------------------------------
# Utilities
# -------------------------------

def set_global_seeds(seed: int):
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

def count_params(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def rmse_per_head(y_true, y_pred):
    mse = torch.mean((y_true - y_pred) ** 2, dim=0)
    per_head = torch.sqrt(mse)
    mean_rmse = torch.sqrt(torch.mean((y_true - y_pred) ** 2))
    return per_head.cpu().tolist(), float(mean_rmse.cpu())

def corr_frobenius(y_true, y_pred, eps=1e-8):
    yt = y_true - y_true.mean(dim=0, keepdim=True)
    yp = y_pred - y_pred.mean(dim=0, keepdim=True)
    std_t = yt.std(dim=0, keepdim=True)
    std_p = yp.std(dim=0, keepdim=True)
    use = ((std_t > eps) & (std_p > eps)).squeeze(0)

    if use.sum() >= 2:
        yt = yt[:, use] / (std_t[:, use] + eps)
        yp = yp[:, use] / (std_p[:, use] + eps)
        Ct = yt.t().mm(yt) / (yt.shape[0] - 1)
        Cp = yp.t().mm(yp) / (yp.shape[0] - 1)
        fr = float(torch.norm(Ct - Cp, p='fro').cpu())
        heads_used = int(use.sum().item())
    else:
        fr = float('nan')
        heads_used = int(use.sum().item())

    return fr, heads_used

@dataclass
class RunResult:
    model_name: str
    params: int
    train_mse: float
    test_rmse_mean: float
    test_rmse_per_head: list
    corr_Frob: float
    corr_heads_used: int
    seconds: float
    notes: str = ""


# -------------------------------
# Cubic / Quadratic B-spline machinery (KAN)
# -------------------------------

def make_clamped_uniform_knots(in_min, in_max, n_basis, degree):
    """Open-uniform (clamped) knot vector: length = n_basis + degree + 1."""
    in_min = float(in_min); in_max = float(in_max)
    assert in_max > in_min
    n_interior = n_basis - degree - 1
    if n_interior < 0:
        n_basis = degree + 1
        n_interior = 0
    if n_interior == 0:
        interior = []
    else:
        interior = list(np.linspace(in_min, in_max, n_interior + 2)[1:-1])
    knots = [in_min] * (degree + 1) + interior + [in_max] * (degree + 1)
    return torch.tensor(knots, dtype=torch.float32)

def bspline_basis(x, knots, degree):
    """
    Cox–de Boor recursion for B-spline basis.
    x: [B] tensor, knots: [M] (M = n_basis + degree + 1)
    returns: [B, n_basis]
    """
    device = x.device; dtype = x.dtype
    knots = knots.to(device=device, dtype=dtype)
    n_basis = knots.numel() - degree - 1
    # Degree 0
    B = []
    for i in range(n_basis):
        t_i, t_ip1 = knots[i], knots[i+1]
        if i == n_basis - 1:
            B0 = ((x >= t_i) & (x <= t_ip1)).to(dtype)
        else:
            B0 = ((x >= t_i) & (x <  t_ip1)).to(dtype)
        B.append(B0)
    B = torch.stack(B, dim=1)  # [B, n_basis]

    for p in range(1, degree + 1):
        Bp = torch.zeros_like(B)
        for i in range(n_basis):
            denom1 = knots[i+p] - knots[i]
            if denom1.abs() > 1e-12:
                left = (x - knots[i]) / denom1 * (B[:, i] if i < n_basis else 0.0)
            else:
                left = 0.0
            denom2 = knots[i+p+1] - knots[i+1] if (i+1) < knots.numel() else torch.tensor(0., device=device, dtype=dtype)
            if (i+1) < n_basis and denom2.abs() > 1e-12:
                right = (knots[i+p+1] - x) / denom2 * B[:, i+1]
            else:
                right = 0.0
            Bp[:, i] = left + right
        B = Bp
    return B  # [B, n_basis]

# --- Original (slow) KAN ---------------------------------------------

class BSpline1D(nn.Module):
    def __init__(self, n_basis=10, degree=3, in_min=0.0, in_max=1.0, outside="linear"):
        super().__init__()
        assert degree in (2, 3)
        self.n_basis = int(n_basis)
        self.degree = int(degree)
        self.in_min = float(in_min); self.in_max = float(in_max)

        # FIX: ensure at least degree+1 basis functions
        if self.n_basis < self.degree + 1:
            self.n_basis = self.degree + 1

        self.register_buffer('knots', make_clamped_uniform_knots(self.in_min, self.in_max, self.n_basis, self.degree))
        self.coeffs = nn.Parameter(torch.zeros(self.n_basis))
        self.outside = outside

    def _eval_inside(self, x):
        B = bspline_basis(x, self.knots, self.degree)  # [B, n_basis]
        return (B * self.coeffs.view(1, -1)).sum(dim=1)

    def forward(self, x):
        x = x.view(-1)
        m_lo = x < self.in_min
        m_hi = x > self.in_max
        m_in = (~m_lo) & (~m_hi)
        y = torch.empty_like(x)
        if m_in.any():
            y[m_in] = self._eval_inside(x[m_in])
        if m_lo.any() or m_hi.any():
            if self.outside == "clamp":
                if m_lo.any():
                    y[m_lo] = self._eval_inside(torch.full_like(x[m_lo], self.in_min))
                if m_hi.any():
                    y[m_hi] = self._eval_inside(torch.full_like(x[m_hi], self.in_max))
            else:
                eps = 1e-3 * (self.in_max - self.in_min)
                if m_lo.any():
                    x0 = torch.full_like(x[m_lo], self.in_min)
                    y0 = self._eval_inside(x0)
                    x1 = torch.full_like(x[m_lo], min(self.in_min + eps, self.in_max))
                    y1 = self._eval_inside(x1)
                    slope = (y1 - y0) / (x1 - x0 + 1e-12)
                    y[m_lo] = y0 + slope * (x[m_lo] - self.in_min)
                if m_hi.any():
                    x0 = torch.full_like(x[m_hi], self.in_max)
                    y0 = self._eval_inside(x0)
                    x1 = torch.full_like(x[m_hi], max(self.in_max - eps, self.in_min))
                    y1 = self._eval_inside(x1)
                    slope = (y0 - y1) / (x0 - x1 + 1e-12)
                    y[m_hi] = y0 + slope * (x[m_hi] - self.in_max)
        return y.view(-1, 1)  # column

class KANUnivariate(nn.Module):
    def __init__(self, n_basis=10, degree=3, in_min=0.0, in_max=1.0, outside="linear", residual_type="silu"):
        super().__init__()
        self.spline = BSpline1D(n_basis=n_basis, degree=degree, in_min=in_min, in_max=in_max, outside=outside)
        # Per-edge scales
        self.ws = nn.Parameter(torch.tensor(1.0))  # spline scale
        self.residual_type = residual_type
        if residual_type in ("silu", "linear"):
            self.wb = nn.Parameter(torch.tensor(1.0))  # bypass scale (per edge)
        else:
            self.wb = None  # no bypass

    def forward(self, x):
        s = self.spline(x).view(-1)  # [B]
        if self.residual_type == "silu":
            return self.wb * F.silu(x) + self.ws * s
        elif self.residual_type == "linear":
            return self.wb * x + self.ws * s
        else:
            return self.ws * s

class KANLayerSlow(nn.Module):
    def __init__(self, d_in, d_out, n_basis=10, degree=3, in_min=0.0, in_max=1.0,
                 outside="linear", residual_type="silu"):
        super().__init__()
        self.d_in = d_in; self.d_out = d_out
        # Equalized linear residual: scalar α if dims match, else edge-wise wb
        self.equalized_linear_scalar = (residual_type == "linear" and d_in == d_out)
        edge_residual_type = residual_type if not self.equalized_linear_scalar else "none"

        self.phi = nn.ModuleList(
            [KANUnivariate(n_basis=n_basis, degree=degree, in_min=in_min, in_max=in_max,
                           outside=outside, residual_type=edge_residual_type)
             for _ in range(d_in * d_out)]
        )

        if self.equalized_linear_scalar:
            self.residual_alpha = nn.Parameter(torch.tensor(0.1))
        else:
            self.register_parameter("residual_alpha", None)

    def forward(self, x):
        B = x.shape[0]
        out = x.new_zeros(B, self.d_out)
        idx = 0
        for j in range(self.d_out):
            s = 0.0
            for i in range(self.d_in):
                s = s + self.phi[idx](x[:, i]).view(-1)
                idx += 1
            out[:, j] = s

        # Add α·x once per layer when d_in == d_out and residual_type == 'linear'
        if self.residual_alpha is not None:
            out = out + self.residual_alpha * x
        return out

# --- FAST vectorized KAN ---------------------------------------------

class FastKANLayer(nn.Module):
    """
    Vectorized KAN layer mapping d_in -> d_out:
      y[:, j] = sum_i [ residual_term(x[:,i]) * wb[i,j]  +  ws[i,j] * S_i,j(x[:,i]) ],
    where residual_term is SiLU(x) if residual_type='silu', x if 'linear', or 0 if 'none'.
    S_i,j is a degree-d clamped-uniform B-spline with n_basis coefficients.
    Outside behavior 'linear' or 'clamp' matches the slow implementation.

    Equalized linear residuals:
      - If residual_type == 'linear' and d_in == d_out, we DO NOT use per-edge wb.
        Instead we add a single scalar α per layer and add α·x after the spline sum.
      - If residual_type == 'linear' and d_in != d_out, behavior is unchanged (wb matrix).
    """
    def __init__(self, d_in, d_out, n_basis=10, degree=3, in_min=0.0, in_max=1.0,
                 outside="linear", residual_type="silu"):
        super().__init__()
        assert degree in (2, 3)
        self.d_in = int(d_in)
        self.d_out = int(d_out)
        self.degree = int(degree)
        self.n_basis = int(n_basis)
        self.in_min = float(in_min)
        self.in_max = float(in_max)
        self.outside = outside
        self.residual_type = residual_type
        self.equalized_linear_scalar = (residual_type == "linear" and self.d_in == self.d_out)

        # FIX: ensure at least degree+1 basis functions
        if self.n_basis < self.degree + 1:
            self.n_basis = self.degree + 1

        # Shared knot vector
        self.register_buffer('knots', make_clamped_uniform_knots(self.in_min, self.in_max, self.n_basis, self.degree))
        # Parameters: per-edge coefficients and scales
        self.coeffs = nn.Parameter(torch.zeros(self.d_in, self.d_out, self.n_basis))  # [d_in, d_out, K]
        self.ws     = nn.Parameter(torch.ones(self.d_in, self.d_out))                # spline scale

        # Residual parameters:
        #   silu: per-edge wb
        #   linear & d_in!=d_out: per-edge wb
        #   linear & d_in==d_out: single scalar residual_alpha
        if self.residual_type == "silu":
            self.wb = nn.Parameter(torch.ones(self.d_in, self.d_out))
            self.register_parameter("residual_alpha", None)
        elif self.residual_type == "linear":
            if self.equalized_linear_scalar:
                self.register_parameter("wb", None)
                self.residual_alpha = nn.Parameter(torch.tensor(0.1))
            else:
                self.wb = nn.Parameter(torch.ones(self.d_in, self.d_out))
                self.register_parameter("residual_alpha", None)
        else:
            self.register_parameter("wb", None)
            self.register_parameter("residual_alpha", None)

        # Precompute boundary bases
        with torch.no_grad():
            self._B_min_cpu = bspline_basis(torch.tensor([self.in_min]), self.knots.cpu(), self.degree)  # [1,K]
            self._B_max_cpu = bspline_basis(torch.tensor([self.in_max]), self.knots.cpu(), self.degree)
            eps = max(1e-3 * (self.in_max - self.in_min), 1e-6)
            self._B_min_eps_cpu = bspline_basis(torch.tensor([min(self.in_min + eps, self.in_max)]),
                                                self.knots.cpu(), self.degree)
            self._B_max_eps_cpu = bspline_basis(torch.tensor([max(self.in_max - eps, self.in_min)]),
                                                self.knots.cpu(), self.degree)
            self._eps = eps

    def _eval_inside_all_outputs(self, B_i, coeffs_i):
        # B_i: [B, K], coeffs_i: [d_out, K] -> [B, d_out]
        return B_i @ coeffs_i.t()

    def forward(self, x):
        # x: [B, d_in]
        device = x.device
        Bsize = x.shape[0]
        out = x.new_zeros(Bsize, self.d_out)

        Bmin  = self._B_min_cpu.to(device=device, dtype=x.dtype)
        Bmax  = self._B_max_cpu.to(device=device, dtype=x.dtype)
        BminE = self._B_min_eps_cpu.to(device=device, dtype=x.dtype)
        BmaxE = self._B_max_eps_cpu.to(device=device, dtype=x.dtype)
        eps = self._eps

        for i in range(self.d_in):
            xi = x[:, i]  # [B]

            # residual / projection term
            if self.wb is not None:
                if self.residual_type == "silu":
                    bypass = F.silu(xi).unsqueeze(1) * self.wb[i]  # [B, d_out]
                else:  # "linear" with d_in != d_out
                    bypass = xi.unsqueeze(1) * self.wb[i]          # [B, d_out]
            else:
                bypass = 0.0

            # Spline term
            m_lo = xi < self.in_min
            m_hi = xi > self.in_max
            m_in = (~m_lo) & (~m_hi)

            if m_in.any():
                Bi = bspline_basis(xi[m_in], self.knots.to(device=device, dtype=x.dtype), self.degree)  # [B_in,K]
                Si = self._eval_inside_all_outputs(Bi, self.coeffs[i])  # [B_in, d_out]
                spline_term_in = torch.zeros(Bsize, self.d_out, device=device, dtype=x.dtype)
                spline_term_in[m_in] = Si
            else:
                spline_term_in = torch.zeros(Bsize, self.d_out, device=device, dtype=x.dtype)

            if m_lo.any() or m_hi.any():
                if self.outside == "clamp":
                    if m_lo.any():
                        y0 = (Bmin @ self.coeffs[i].t()).squeeze(0)
                        spline_term_in[m_lo] = y0
                    if m_hi.any():
                        y1 = (Bmax @ self.coeffs[i].t()).squeeze(0)
                        spline_term_in[m_hi] = y1
                else:
                    if m_lo.any():
                        y0 = (Bmin  @ self.coeffs[i].t()).squeeze(0)
                        y1 = (BminE @ self.coeffs[i].t()).squeeze(0)
                        slope = (y1 - y0) / (eps + 1e-12)
                        dx = (xi[m_lo] - self.in_min).unsqueeze(1)
                        spline_term_in[m_lo] = y0 + dx * slope
                    if m_hi.any():
                        y0 = (Bmax  @ self.coeffs[i].t()).squeeze(0)
                        y1 = (BmaxE @ self.coeffs[i].t()).squeeze(0)
                        slope = (y0 - y1) / (eps + 1e-12)
                        dx = (xi[m_hi] - self.in_max).unsqueeze(1)
                        spline_term_in[m_hi] = y0 + dx * slope

            out = out + bypass + (spline_term_in * self.ws[i])

        # Add α·x once per layer for linear residual with matching dims
        if self.residual_alpha is not None:
            out = out + self.residual_alpha * x

        return out

class FastKANNet(nn.Module):
    """
    Multi-layer KAN with optional BN before/after each layer and per-head affine at the end.
    Uses FastKANLayer internally.
    """
    def __init__(self, input_dim, architecture, final_dim, n_basis=10, degree=3,
                 bn_type="batch", bn_position="after", bn_skip_first=True, outside="linear",
                 residual_type="silu"):
        super().__init__()
        self.deg = int(degree)
        self.n_basis = int(n_basis)
        self.bn_type = bn_type
        self.bn_position = bn_position
        self.bn_skip_first = bn_skip_first
        self.outside = outside
        self.residual_type = residual_type

        dims = [input_dim] + list(architecture) + [final_dim]
        self.layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()

        for li, (a, b) in enumerate(zip(dims[:-1], dims[1:])):
            if bn_type == "batch" and bn_position == "before" and not (bn_skip_first and li == 0):
                self.bn_layers.append(nn.BatchNorm1d(a))
            else:
                self.bn_layers.append(nn.Identity())

            self.layers.append(FastKANLayer(a, b, n_basis=self.n_basis, degree=self.deg,
                                            outside=self.outside, residual_type=self.residual_type))

            if bn_type == "batch" and bn_position == "after" and not (bn_skip_first and li == 0):
                self.bn_layers.append(nn.BatchNorm1d(b))
            else:
                self.bn_layers.append(nn.Identity())

        self.out_scale = nn.Parameter(torch.ones(final_dim))
        self.out_bias  = nn.Parameter(torch.zeros(final_dim))

    def forward(self, x):
        h = x
        for li, layer in enumerate(self.layers):
            bn_before = self.bn_layers[2*li + 0]
            bn_after  = self.bn_layers[2*li + 1]
            h = bn_before(h)
            h = layer(h)
            h = bn_after(h)
        return h * self.out_scale + self.out_bias

# --- Backward-compatible (slow) KANNet -------------------------------

class KANNetSlow(nn.Module):
    def __init__(self, input_dim, architecture, final_dim, n_basis=10, degree=3,
                 bn_type="batch", bn_position="after", bn_skip_first=True, outside="linear",
                 residual_type="silu"):
        super().__init__()
        self.deg = int(degree)
        self.n_basis = int(n_basis)
        self.bn_type = bn_type
        self.bn_position = bn_position
        self.bn_skip_first = bn_skip_first
        self.outside = outside
        self.residual_type = residual_type

        dims = [input_dim] + list(architecture) + [final_dim]
        self.layers = nn.ModuleList()
        self.bn_layers = nn.ModuleList()

        for li, (a, b) in enumerate(zip(dims[:-1], dims[1:])):
            if bn_type == "batch" and bn_position == "before" and not (bn_skip_first and li == 0):
                self.bn_layers.append(nn.BatchNorm1d(a))
            else:
                self.bn_layers.append(nn.Identity())

            self.layers.append(KANLayerSlow(a, b, n_basis=self.n_basis, degree=self.deg,
                                            outside=self.outside, residual_type=self.residual_type))

            if bn_type == "batch" and bn_position == "after" and not (bn_skip_first and li == 0):
                self.bn_layers.append(nn.BatchNorm1d(b))
            else:
                self.bn_layers.append(nn.Identity())

        self.out_scale = nn.Parameter(torch.ones(final_dim))
        self.out_bias  = nn.Parameter(torch.zeros(final_dim))

    def forward(self, x):
        h = x
        for li, layer in enumerate(self.layers):
            bn_before = self.bn_layers[2*li + 0]
            bn_after  = self.bn_layers[2*li + 1]
            h = bn_before(h)
            h = layer(h)
            h = bn_after(h)
            return h * self.out_scale + self.out_bias


def kan_param_count(arch, input_dim, final_dim, n_basis, degree,
                    bn_type, bn_position, bn_skip_first, residual_type):
    """
    Count parameters for KAN under the chosen residual_type.
      - per-edge: n_basis coeffs + ws (+ wb if residual_type!='none')
      - BUT for residual_type='linear' with a==b, **omit wb per-edge** and add **1 scalar** per layer.
      - BN affine params
      - output affine
    """
    dims = [input_dim] + list(arch) + [final_dim]
    total = 0
    eff_basis = max(int(n_basis), int(degree) + 1)  # FIX: enforce minimum basis

    for a, b in zip(dims[:-1], dims[1:]):
        use_layer_scalar = (residual_type == "linear" and a == b)
        layer_add_wb = (residual_type != "none") and not use_layer_scalar
        per_edge = eff_basis + 1 + (1 if layer_add_wb else 0)  # coeffs + ws + (wb?)
        total += per_edge * a * b
        if use_layer_scalar:
            total += 1  # α scalar for this layer
    if bn_type == "batch":
        for li, (a, b) in enumerate(zip(dims[:-1], dims[1:])):
            if bn_position == "before" and not (bn_skip_first and li == 0):
                total += 2 * a
            if bn_position == "after"  and not (bn_skip_first and li == 0):
                total += 2 * b
    total += (final_dim * 2)
    return total


def choose_kan_basis_for_parity(target_params, arch, input_dim, final_dim,
                                degree, bn_type, bn_position, bn_skip_first,
                                residual_type, prefer_leq=True):
    min_K = int(degree) + 1  # FIX: search only valid K
    best_n = None; best_diff = float('inf'); best_count = None
    for n in range(min_K, 200):
        cnt = kan_param_count(arch, input_dim, final_dim, n, degree,
                              bn_type, bn_position, bn_skip_first, residual_type)
        diff = abs(cnt - target_params)
        if prefer_leq:
            if cnt <= target_params and diff < best_diff:
                best_diff = diff; best_n = n; best_count = cnt
        else:
            if diff < best_diff:
                best_diff = diff; best_n = n; best_count = cnt
    if best_n is None:
        for n in range(min_K, 200):
            cnt = kan_param_count(arch, input_dim, final_dim, n, degree,
                                  bn_type, bn_position, bn_skip_first, residual_type)
            if cnt >= target_params:
                return n, cnt
        return 199, kan_param_count(arch, input_dim, final_dim, 199, degree,
                                    bn_type, bn_position, bn_skip_first, residual_type)
    return best_n, best_count


# -------------------------------
# KAN BN recalc (with progress)
# -------------------------------

def recalc_bn_for_kan_progress(model: nn.Module, x_train: torch.Tensor, passes: int = 10, desc="KAN BN re-calc"):
    """Recompute KAN BatchNorm running stats using training data (with tqdm)."""
    was_training = model.training
    for m in model.modules():
        if isinstance(m, nn.BatchNorm1d):
            m.reset_running_stats()
            m.momentum = 0.1
    model.train()
    with torch.no_grad():
        for p in tqdm(range(passes), desc=desc):
            _ = model(x_train)
            if p >= passes - 3:
                for m in model.modules():
                    if isinstance(m, nn.BatchNorm1d):
                        m.momentum = 0.01
    if not was_training:
        model.eval()


def recalc_bn_for_sn_progress(model: nn.Module, x_train: torch.Tensor, passes: int = 10, desc="SN BN re-calc"):
    """SN BN recalc with progress bars."""
    was_training = model.training
    for module in model.modules():
        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            module.reset_running_stats()
            module.momentum = 0.1
    model.train()
    with torch.no_grad():
        for p in tqdm(range(passes), desc=desc):
            _ = model(x_train)
            if p >= passes - 3:
                for module in model.modules():
                    if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                        module.momentum = 0.01
    if not was_training:
        model.eval()


# -------------------------------
# KAN training (fast/slow)
# -------------------------------

def build_kan(input_dim, final_dim, arch, n_basis, degree, bn_type, bn_position, bn_skip_first,
              outside, residual_type, impl="fast"):
    if impl == "fast":
        return FastKANNet(
            input_dim=input_dim, architecture=arch, final_dim=final_dim,
            n_basis=n_basis, degree=degree,
            bn_type=bn_type, bn_position=bn_position, bn_skip_first=bn_skip_first,
            outside=outside, residual_type=residual_type
        )
    else:
        return KANNetSlow(
            input_dim=input_dim, architecture=arch, final_dim=final_dim,
            n_basis=n_basis, degree=degree,
            bn_type=bn_type, bn_position=bn_position, bn_skip_first=bn_skip_first,
            outside=outside, residual_type=residual_type
        )

def train_kan(x_train, y_train, input_dim, final_dim, arch, n_basis, degree, device, epochs=4000,
              lr=1e-3, wd=1e-6, seed=0, bn_type="batch", bn_position="after", bn_skip_first=True,
              outside="linear", residual_type="silu", impl="fast"):
    set_global_seeds(seed)
    model = build_kan(
        input_dim=input_dim, final_dim=final_dim, arch=arch, n_basis=n_basis, degree=degree,
        bn_type=bn_type, bn_position=bn_position, bn_skip_first=bn_skip_first,
        outside=outside, residual_type=residual_type, impl=impl
    ).to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)
    loss_fn = nn.MSELoss()
    x_train = x_train.to(device); y_train = y_train.to(device)

    t0 = time.time()
    for _ in tqdm(range(epochs), desc=f"Training KAN ({impl})"):
        opt.zero_grad(set_to_none=True)
        yhat = model(x_train)
        loss = loss_fn(yhat, y_train)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        opt.step()
    secs = time.time() - t0

    with torch.no_grad():
        train_mse = float(loss_fn(model(x_train), y_train).cpu())
    return model, train_mse, secs


# -------------------------------
# SN warm-up then freeze helper
# -------------------------------

def continue_train_sn_no_domain_updates(model, x_train, y_train, epochs, device, seed,
                                        lr_other=3e-4, lr_codomain=1e-3, wd=1e-7, clip=1.0):
    set_global_seeds(seed)
    model = model.to(device)
    model.train()
    params = []
    if CONFIG.get('train_phi_codomain', True):
        params.append({"params": [p for n, p in model.named_parameters() if "phi_codomain_params" in n], "lr": lr_codomain})
    if CONFIG.get('use_lateral_mixing', False):
        params.append({"params": [p for n, p in model.named_parameters() if "lateral" in n], "lr": 5e-4})
    excluded = []
    if CONFIG.get('train_phi_codomain', True): excluded.append("phi_codomain_params")
    if CONFIG.get('use_lateral_mixing', False): excluded.append("lateral")
    params.append({"params": [p for n, p in model.named_parameters() if not any(e in n for e in excluded)], "lr": lr_other})
    opt = torch.optim.Adam(params, weight_decay=wd)

    loss_fn = nn.MSELoss()
    x_train = x_train.to(device); y_train = y_train.to(device)
    t0 = time.time()
    for _ in tqdm(range(max(0, int(epochs))), desc="Continue SN (domains frozen)"):
        opt.zero_grad(set_to_none=True)
        yhat = model(x_train)
        loss = loss_fn(yhat, y_train)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        opt.step()
    secs = time.time() - t0

    with torch.no_grad():
        train_mse = float(loss_fn(model(x_train), y_train).cpu())
    return model, train_mse, secs


# -------------------------------
# Batched evaluation with PROGRESS
# -------------------------------

def _is_sn_model(model: nn.Module) -> bool:
    """
    Heuristic: SN models in this codebase have `layers` AND `norm_layers` attributes.
    KAN nets have `layers` and `bn_layers` but no `norm_layers`.
    """
    return hasattr(model, "layers") and hasattr(model, "norm_layers")

def evaluate_with_progress(model, x_test, y_test, *,
                           bn_mode="off", bn_recalc_passes=10, x_train_for_bn=None,
                           name="Model", eval_batch_size=8192):
    """
    - Optionally re-calc BN stats on training batch, then evaluate on x_test in batches with tqdm.
    - **SN fix**: for Sprecher Networks, even under `recalc_eval`, keep BN in **train** mode at test.
      This avoids reliance on running stats that proved poorly calibrated for SN.
    """
    if bn_mode == "recalc_eval" and has_batchnorm(model):
        if _is_sn_model(model):
            # Recompute stats for completeness, but keep train-mode semantics for test
            recalc_bn_for_sn_progress(model, x_train_for_bn, passes=bn_recalc_passes, desc=f"{name} BN re-calc")
            model.train()  # <-- critical: use batch stats for SN at test
        else:
            recalc_bn_for_kan_progress(model, x_train_for_bn, passes=bn_recalc_passes, desc=f"{name} BN re-calc")
            model.eval()   # KAN uses running stats in eval mode

    N = x_test.shape[0]
    out_dim = y_test.shape[1]
    y_pred_cpu = torch.empty(N, out_dim, dtype=y_test.dtype)

    bs = max(1, int(eval_batch_size))
    device = next((p.device for p in model.parameters() if p.requires_grad), x_test.device)

    with torch.no_grad():
        rng = range(0, N, bs)
        for start in tqdm(rng, desc=f"Evaluating {name}", total=(N + bs - 1) // bs):
            end = min(N, start + bs)
            yb = model(x_test[start:end].to(device))
            y_pred_cpu[start:end] = yb.detach().cpu()

    per_head, mean_rmse = rmse_per_head(y_test.cpu(), y_pred_cpu)
    corrF, used = corr_frobenius(y_test.cpu(), y_pred_cpu)
    return per_head, mean_rmse, corrF, used


# -------------------------------
# Main
# -------------------------------

def main():
    p = argparse.ArgumentParser()
    p.add_argument("--device", type=str, default="auto", choices=["auto", "cpu", "cuda"])
    p.add_argument("--seed", type=int, default=45)
    p.add_argument("--epochs", type=int, default=4000)

    # Dataset
    p.add_argument("--dataset", type=str, default="toy_4d_to_5d")

    # --- SN config ---
    p.add_argument("--sn_arch", type=str, default="15,15")
    p.add_argument("--sn_phi_knots", type=int, default=60)
    p.add_argument("--sn_Phi_knots", type=int, default=60)
    p.add_argument("--sn_norm_type", type=str, default="batch", choices=["none", "batch", "layer"])
    p.add_argument("--sn_norm_position", type=str, default="after", choices=["before", "after"])
    p.add_argument("--sn_norm_skip_first", action="store_true", default=True)
    p.add_argument("--sn_norm_first", action="store_true")
    p.add_argument("--sn_no_residual", action="store_true")
    p.add_argument("--sn_residual_style", type=str, default=None,
                   choices=["node", "linear", "standard", "matrix"])
    p.add_argument("--sn_no_lateral", action="store_true")
    # Domain policy
    p.add_argument("--sn_freeze_domains_after", type=int, default=0)
    p.add_argument("--sn_domain_margin", type=float, default=0.0)

    # --- KAN config ---
    p.add_argument("--kan_arch", type=str, default="4,4")
    p.add_argument("--kan_degree", type=int, default=3, choices=[2, 3])
    p.add_argument("--kan_K", type=int, default=60, help="Initial #basis; ignored if --equalize_params is used.")
    p.add_argument("--kan_bn_type", type=str, default="batch", choices=["none", "batch"])
    p.add_argument("--kan_bn_position", type=str, default="after", choices=["before", "after"])
    p.add_argument("--kan_bn_skip_first", action="store_true", default=True)
    p.add_argument("--kan_outside", type=str, default="linear", choices=["linear", "clamp"])
    p.add_argument("--kan_residual_type", type=str, default="silu", choices=["silu", "linear", "none"])
    p.add_argument("--kan_lr", type=float, default=1e-3)
    p.add_argument("--kan_wd", type=float, default=1e-6)
    p.add_argument("--kan_impl", type=str, default="fast", choices=["fast", "slow"],
                   help="Use fast vectorized KAN or the original per-edge ModuleList version.")

    # Global fairness / parity
    p.add_argument("--equalize_params", action="store_true")
    p.add_argument("--prefer_leq", action="store_true")

    # BN at test
    p.add_argument("--bn_eval_mode", type=str, default="recalc_eval", choices=["off", "recalc_eval"])
    p.add_argument("--bn_recalc_passes", type=int, default=10)

    # Test set size + eval batching
    p.add_argument("--n_test", type=int, default=20000)
    p.add_argument("--eval_batch_size", type=int, default=8192)

    # Output
    p.add_argument("--outdir", type=str, default="benchmarks/results")

    args = p.parse_args()
    device = ("cuda" if (args.device == "auto" and torch.cuda.is_available()) else
              ("cpu" if args.device == "auto" else args.device))
    set_global_seeds(args.seed)

    # dataset & dims
    dataset = get_dataset(args.dataset)
    input_dim = dataset.input_dim
    final_dim = dataset.output_dim

    # fixed test set
    torch.manual_seed(args.seed + 2025)
    with torch.no_grad():
        x_test, y_test = dataset.sample(args.n_test, device=device)

    # ------------------------------
    # Configure SN fairness knobs
    # ------------------------------
    if args.sn_no_residual:
        CONFIG['use_residual_weights'] = False
    else:
        # residual style override if provided
        if args.sn_residual_style is not None:
            style = args.sn_residual_style.lower()
            if style in ("standard", "matrix"):
                style = "linear"
            CONFIG['residual_style'] = style  # used when constructing SN blocks

    if args.sn_no_lateral:
        CONFIG['use_lateral_mixing'] = False

    sn_norm_skip = False if args.sn_norm_first else args.sn_norm_skip_first
    CONFIG['domain_safety_margin'] = float(args.sn_domain_margin)

    sn_arch = [int(x) for x in args.sn_arch.split(",")] if args.sn_arch.strip() else []

    # ------------------------------
    # SN training: warm-up then freeze (optional)
    # ------------------------------
    total_epochs = int(args.epochs)
    warmup_epochs = max(0, min(args.sn_freeze_domains_after, total_epochs))
    rest_epochs = total_epochs - warmup_epochs

    # Phase 1: warm-up WITH domain updates (standard trainer)
    t0_sn = time.time()
    # residual style to pass explicitly (if residuals enabled)
    residual_style_override = None if args.sn_no_residual else CONFIG.get('residual_style', 'node')
    plotting_snapshot, _ = train_network(
        dataset=dataset,
        architecture=sn_arch,
        total_epochs=warmup_epochs if warmup_epochs > 0 else total_epochs,
        print_every=max(1, (warmup_epochs if warmup_epochs > 0 else total_epochs) // 10),
        device=device,
        phi_knots=args.sn_phi_knots,
        Phi_knots=args.sn_Phi_knots,
        seed=args.seed,
        norm_type=args.sn_norm_type,
        norm_position=args.sn_norm_position,
        norm_skip_first=sn_norm_skip,
        no_load_best=False,
        bn_recalc_on_load=False,
        residual_style=residual_style_override
    )
    sn_secs = time.time() - t0_sn

    sn_model = plotting_snapshot["model"].to(device)
    x_train  = plotting_snapshot["x_train"].to(device)
    y_train  = plotting_snapshot["y_train"].to(device)

    if rest_epochs > 0:
        t1 = time.time()
        sn_model, sn_train_mse, secs2 = continue_train_sn_no_domain_updates(
            model=sn_model, x_train=x_train, y_train=y_train, epochs=rest_epochs,
            device=device, seed=args.seed
        )
        sn_secs += (time.time() - t1)
    else:
        with torch.no_grad():
            sn_train_mse = float(torch.mean((sn_model(x_train) - y_train) ** 2).cpu())

    sn_params = count_params(sn_model)

    # ------------------------------
    # Choose K for KAN if equalizing
    # ------------------------------
    kan_arch = [int(x) for x in args.kan_arch.split(",")] if args.kan_arch.strip() else []
    if args.equalize_params:
        chosen_K, est_cnt = choose_kan_basis_for_parity(
            target_params=sn_params, arch=kan_arch, input_dim=input_dim, final_dim=final_dim,
            degree=args.kan_degree, bn_type=args.kan_bn_type, bn_position=args.kan_bn_position,
            bn_skip_first=args.kan_bn_skip_first, residual_type=args.kan_residual_type,
            prefer_leq=args.prefer_leq
        )
        kan_K = chosen_K
        parity_note = f"[ParamMatch] Target SN params = {sn_params}. Chosen K for KAN = {kan_K} (est. {est_cnt} params)."
        print(parity_note)
    else:
        kan_K = args.kan_K
        parity_note = ""

    # ------------------------------
    # Train KAN on EXACT same (x_train, y_train)
    # ------------------------------
    t0_kan = time.time()
    kan_model, kan_train_mse, kan_secs = train_kan(
        x_train=x_train, y_train=y_train,
        input_dim=input_dim, final_dim=final_dim,
        arch=kan_arch, n_basis=kan_K, degree=args.kan_degree,
        device=device, epochs=args.epochs, seed=args.seed,
        lr=args.kan_lr, wd=args.kan_wd,
        bn_type=args.kan_bn_type, bn_position=args.kan_bn_position, bn_skip_first=args.kan_bn_skip_first,
        outside=args.kan_outside, residual_type=args.kan_residual_type, impl=args.kan_impl
    )
    kan_params = count_params(kan_model)

    # ------------------------------
    # Test evaluation with PROGRESS
    # ------------------------------
    sn_per_head, sn_rmse_mean, sn_corrF, sn_corr_used = evaluate_with_progress(
        sn_model, x_test, y_test,
        bn_mode=args.bn_eval_mode, bn_recalc_passes=args.bn_recalc_passes, x_train_for_bn=x_train,
        name="SN", eval_batch_size=args.eval_batch_size
    )
    kan_per_head, kan_rmse_mean, kan_corrF, kan_corr_used = evaluate_with_progress(
        kan_model, x_test, y_test,
        bn_mode=args.bn_eval_mode, bn_recalc_passes=args.bn_recalc_passes, x_train_for_bn=x_train,
        name=f"KAN-{args.kan_impl}", eval_batch_size=args.eval_batch_size
    )

    # ------------------------------
    # Print & save results
    # ------------------------------
    def pretty(title, params, train_mse, mean_rmse, per_head, corrF, used, secs, notes=""):
        out = {
            "model": title,
            "params": int(params),
            "train_mse": float(train_mse),
            "test_rmse_mean": float(mean_rmse),
            "test_rmse_per_head": [float(x) for x in per_head],
            "corr_Frob_error": (None if (corrF is None or not math.isfinite(corrF)) else float(corrF)),
            "corr_Frob_heads_used": int(used),
            "train_seconds": float(secs),
        }
        if notes:
            out["notes"] = notes
        return out

    sn_title = (
        f"SN(arch={sn_arch}, phi_knots={args.sn_phi_knots}, Phi_knots={args.sn_Phi_knots}, "
        f"norm={args.sn_norm_type}/{args.sn_norm_position}/"
        f"{'skip_first' if sn_norm_skip else 'include_first'}, "
        f"residuals={'off' if args.sn_no_residual else 'on(style='+CONFIG.get('residual_style','node')+')'}, "
        f"lateral={'off' if args.sn_no_lateral else 'on'}, "
        f"domains={'warmup+freeze' if args.sn_freeze_domains_after>0 else 'updated'})"
    )
    kan_title = (
        f"KAN[{args.kan_impl}](arch={kan_arch}, K={kan_K}, degree={args.kan_degree}, "
        f"BN={args.kan_bn_type}/{args.kan_bn_position}/"
        f"{'skip_first' if args.kan_bn_skip_first else 'include_first'}, "
        f"outside={args.kan_outside}, residual={args.kan_residual_type})"
    )

    sn_result = pretty(
        sn_title, sn_params, sn_train_mse, sn_rmse_mean, sn_per_head, sn_corrF, sn_corr_used, sn_secs,
        notes="SN timing includes warm-up + optional freeze phase. " + parity_note
    )
    kan_result = pretty(
        kan_title, kan_params, kan_train_mse, kan_rmse_mean, kan_per_head, kan_corrF, kan_corr_used, kan_secs,
        notes=("BN standardized at test" if args.bn_eval_mode == "recalc_eval" else "") + ("; " + parity_note if parity_note else "")
    )

    print(f"\n=== Head-to-Head Results ({args.dataset}) ===")
    for r in (sn_result, kan_result):
        print(json.dumps(r, indent=2))

    os.makedirs(args.outdir, exist_ok=True)
    out_path = os.path.join(args.outdir, f"kan_sn_parity_{args.dataset}_seed{args.seed}.json")
    with open(out_path, "w") as f:
        json.dump([sn_result, kan_result], f, indent=2)
    print(f"\nSaved: {out_path}")


if __name__ == "__main__":
    main()

----------
requirements.txt:
----------

torch>=1.7.1
torchvision>=0.8.2
numpy>=1.19.2
matplotlib>=3.3.4
tqdm>=4.56.0
Pillow>=8.0.0
rich>=10.0.0